{
  "source_url": "https://github.com/Nihalmannath/Grounded-one_click_structural_solution/blob/94373d5c3a0ea210040d9de1195b23acb6fba0c2/Experiments/2/TERM%203/GLB%20to%20JSON/structural_data/jsonexportsimple.py",
  "repo": "Nihalmannath/Grounded-one_click_structural_solution",
  "repo_stars": 0,
  "repo_description": "Research studio/term_2",
  "license": "unknown",
  "filepath": "Experiments/2/TERM 3/GLB to JSON/structural_data/jsonexportsimple.py",
  "instruction": "-------------------------------------------------------------- GLB TO 3DM CONVERTER --------------------------------------------------------------",
  "code": "\n## --------------------------------------------------------------\n# GLB TO 3DM CONVERTER\n# --------------------------------------------------------------\n\nimport pygltflib\nfrom pygltflib.validator import validate, summary\nimport rhino3dm\nimport os\nimport numpy as np\nfrom shapely.geometry import Polygon, Point, MultiPolygon, LineString\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom collections import OrderedDict\nimport json\nfrom supabase import create_client, Client\nimport requests\nimport tempfile\nimport plotly.graph_objects as go\n\n# Fixed scale factor for all GLB files\nscale_factor = 1\nprint(f\"Using fixed scale factor: {scale_factor}\")\nunityAxisFormat = False\n\n\ndef convert_glb_to_3dm(glb_data):\n    \"\"\"\n    Loads GLB data from memory, extracts its mesh data, and returns a 3dm model object.\n    Only mesh geometry will be converted. Materials, animations, etc., are not translated.\n    \"\"\"\n    global unityAxisFormat\n\n    print(f\"Loading GLB data from memory\")\n    \n    # Write GLB data to a temporary file for pygltflib to read\n    import tempfile\n    with tempfile.NamedTemporaryFile(suffix='.glb', delete=False) as temp_file:\n        temp_file.write(glb_data)\n        temp_glb_path = temp_file.name\n    \n    try:\n        gltf = pygltflib.GLTF2().load(temp_glb_path)\n        validate(gltf)  # will throw an error depending on the problem\n        summary(gltf)  # will pretty print human readable summary of errors\n    finally:\n        # Clean up the temporary file\n        os.unlink(temp_glb_path)\n\n    model_3dm = rhino3dm.File3dm()\n\n    # Iterate through GLTF meshes and add them to the 3dm model\n    for mesh_idx, gltf_mesh in enumerate(gltf.meshes):\n        print(f\"Processing mesh: {gltf_mesh.name if gltf_mesh.name else f'Mesh_{mesh_idx}'}\")\n\n        for primitive in gltf_mesh.primitives:\n            # Get vertex positions\n            accessor_pos = gltf.accessors[primitive.attributes.POSITION]\n            buffer_view_pos = gltf.bufferViews[accessor_pos.bufferView]\n            buffer_pos = gltf.buffers[buffer_view_pos.buffer]\n\n            # Extract vertices\n            vertices_bytes = gltf.get_data_from_buffer_uri(buffer_pos.uri)[\n                buffer_view_pos.byteOffset : buffer_view_pos.byteOffset + buffer_view_pos.byteLength\n            ]\n            vertices = np.frombuffer(vertices_bytes, dtype=np.float32).reshape(-1, 3)\n\n            print(f\"Using fixed scale factor: {scale_factor}\")\n\n            rhino_mesh = rhino3dm.Mesh()\n\n            for v in vertices:\n                if not unityAxisFormat:\n                    # Unity's Y-up format: (x, y, z) -> (x, z, y)\n                    rhino_mesh.Vertices.Add(v[0] * scale_factor, v[1] * scale_factor, v[2] * scale_factor)\n                else:\n                    rhino_mesh.Vertices.Add(v[0] * scale_factor, v[2] * scale_factor, v[1] * scale_factor)\n\n            # Get indices (faces)\n            if primitive.indices is not None:\n                accessor_indices = gltf.accessors[primitive.indices]\n                buffer_view_indices = gltf.bufferViews[accessor_indices.bufferView]\n                buffer_indices = gltf.buffers[buffer_view_indices.buffer]\n\n                indices_bytes = gltf.get_data_from_buffer_uri(buffer_indices.uri)[\n                    buffer_view_indices.byteOffset : buffer_view_indices.byteOffset + buffer_view_indices.byteLength\n                ]\n\n                # Determine dtype for indices (UINT8, UINT16, UINT32)\n                if accessor_indices.componentType == pygltflib.UNSIGNED_BYTE:\n                    indices_dtype = np.uint8\n                elif accessor_indices.componentType == pygltflib.UNSIGNED_SHORT:\n                    indices_dtype = np.uint16\n                elif accessor_indices.componentType == pygltflib.UNSIGNED_INT:\n                    indices_dtype = np.uint32\n                else:\n                    print(f\"Warning: Unsupported index component type: {accessor_indices.componentType}. Skipping faces for this primitive.\")\n                    continue\n\n                indices = np.frombuffer(indices_bytes, dtype=indices_dtype)\n\n                # glTF uses flat arrays for indices, assuming triangles (mode 4)\n                if primitive.mode == pygltflib.TRIANGLES:\n                    for i in range(0, len(indices), 3):\n                        rhino_mesh.Faces.AddFace(int(indices[i]), int(indices[i+1]), int(indices[i+2]))\n                else:\n                    print(f\"Warning: Skipping primitive with unsupported mode: {primitive.mode}. Only triangles (mode 4) are fully supported for faces.\")\n                    continue\n            else:\n                print(f\"Warning: Primitive has no indices. Assuming sequential triangles, but this might not be correct for complex GLBs.\")\n                for i in range(0, len(vertices) - 2, 3):\n                    rhino_mesh.Faces.AddFace(i, i+1, i+2)\n\n            # Optional: Calculate normals\n            rhino_mesh.Normals.ComputeNormals()\n            rhino_mesh.Compact()\n\n            model_3dm.Objects.AddMesh(rhino_mesh)\n\n    print(\"Conversion complete!\")\n    print(\"Sample vertices:\", vertices[:5])\n    \n    return model_3dm\n\n\n# --- Define paths ---\n# Prompt the user for the GLB file path\n\n# Initialize Supabase client\nSUPABASE_URL = \"https://apdbfbjnlsxjfubqahtl.supabase.co\"\nSUPABASE_KEY = \"REDACTED_SUPABASE_KEY\"\nsupabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n\n# Storage parameters\nbucket_name = \"models\"\nfolder_path = \"79edaed4-a719-4390-a485-519b68fa68ea\"\n\n# List all files in the folder\nfiles = supabase.storage.from_(bucket_name).list(folder_path)\nif not files:\n    raise RuntimeError(f\"No files found in folder '{folder_path}'\")\n\n# Pick the latest file by created_at (or updated_at) timestamp\nlatest_file = sorted(\n    files,\n    key=lambda f: f.get(\"created_at\", f.get(\"updated_at\", \"\")),\n    reverse=True\n)[0][\"name\"]\n\n# Generate a signed URL for the latest file\nstorage_path = f\"{folder_path}/{latest_file}\"\nsigned = supabase.storage.from_(bucket_name).create_signed_url(storage_path, 60)\ndownload_url = signed.get(\"signedURL\")\n\n# Get GLB data directly into memory\nresp = requests.get(download_url)\nresp.raise_for_status()\nglb_data = resp.content\n\nprint(f\"âœ… Loaded latest GLB '{latest_file}' into memory from Supabase\")\n\n# Set Unity Y-up format to False by default (no user prompt)\nunityAxisFormat = False \n\n# Extract filename for later use\nfile_name_without_ext = os.path.splitext(latest_file)[0]\n\n# --- Run the conversion ---\ntry:\n    model = convert_glb_to_3dm(glb_data)\n    print(f\"\\nConversion successful! The 3DM model is ready in memory.\")\n\nexcept Exception as e:\n    print(f\"An error occurred during conversion: {e}\")\n\n\n\n# --------------------------------------------------------------\n# MASHALLA FOR FORCED COLUMNS - Now uses the in-memory 3DM model\n# --------------------------------------------------------------\n\n# Use the in-memory model directly\nprint(f\"Using in-memory 3DM model for structural analysis\")\n\n# Extract geometries and their bounding boxes\nbuilding_floor_footprints = []\nall_mesh_bboxes = []\nroof_meshes_info = []\nmax_z = 0.0\n\nZ_FLATNESS_TOLERANCE = 3\n\nfor obj in model.Objects:\n    geom = obj.Geometry\n    if geom.ObjectType == rhino3dm.ObjectType.Mesh:\n        bbox = geom.GetBoundingBox()\n        print(bbox.Min, bbox.Max)\n        \n        bbox_x_dim = bbox.Max.X - bbox.Min.X\n        bbox_y_dim = bbox.Max.Y - bbox.Min.Y\n        bbox_z_dim = bbox.Max.Z - bbox.Min.Z\n        print (bbox_x_dim, bbox_y_dim, bbox_z_dim)\n\n        if bbox_z_dim < Z_FLATNESS_TOLERANCE and bbox_x_dim > 0.1 and bbox_y_dim > 0.1:\n            base_pts = [\n                [bbox.Min.X, bbox.Min.Y],\n                [bbox.Max.X, bbox.Min.Y],\n                [bbox.Max.X, bbox.Max.Y],\n                [bbox.Min.X, bbox.Max.Y],\n                [bbox.Min.X, bbox.Min.Y]\n            ]\n            poly = Polygon(base_pts)\n            if poly.area > 1e-3:\n                building_floor_footprints.append(poly)\n                roof_meshes_info.append((obj.Attributes.Id, bbox, poly))\n\n        all_mesh_bboxes.append(bbox)\n        \n        max_z = max(max_z, bbox.Max.Z)\n\nif not building_floor_footprints:\n    raise RuntimeError(\"No meaningful building floor footprints (meshes flat in Z with area) found in the model.\")\n\nprint(f\"Detected {len(building_floor_footprints)} building floor footprints. Max Z height: {max_z:.2f}m\")\nprint(f\"Total meshes contributing to height calculation: {len(all_mesh_bboxes)}\")\nprint(f\"Detected {len(roof_meshes_info)} potential roof meshes for comparison.\")\n\n\n# ===== AUTOMATIC FLOOR CALCULATION =====\nfloor_height = 2.5  # meters per floor\nnum_floors = max(1, int(round(max_z / floor_height)))\nprint(f\"Automatically calculated number of floors: {num_floors} (based on max height {max_z:.2f}m and {floor_height}m per floor)\")\n\n# # --- Then continues with wall thickness input ---\n# while True:\n#     try:\n#         wall_thickness = float(input(\"Enter desired wall thickness for the perimeter (e.g., 0.3): \"))\n#         if wall_thickness <= 0:\n#             raise ValueError\n#         break\n#     except ValueError:\n#         print(\"Please enter a valid positive number for wall thickness.\")\n\n\n# Find and print roofs that are peaks (taller than all directly touching roofs)\nprint(\"\\n--- Analyzing Roof Heights ---\")\n\nINTERSECTION_BUFFER_ROOF = 0.1\n\ndominant_roofs_identified = []\n\nfor i, (roof1_id, roof1_bbox, roof1_poly_2d) in enumerate(roof_meshes_info):\n    roof1_max_z = roof1_bbox.Max.Z\n    is_dominant_roof = True\n    touching_lower_neighbors = []\n    touching_equal_or_higher_neighbors = []\n\n    for j, (roof2_id, roof2_bbox, roof2_poly_2d) in enumerate(roof_meshes_info):\n        if i == j:\n            continue\n\n        roof2_max_z = roof2_bbox.Max.Z\n\n        intersection_geometry = roof1_poly_2d.buffer(INTERSECTION_BUFFER_ROOF).intersection(roof2_poly_2d.buffer(INTERSECTION_BUFFER_ROOF))\n        \n        is_touching = not intersection_geometry.is_empty and \\\n                      intersection_geometry.geom_type in ['LineString', 'MultiLineString', 'Polygon', 'MultiPolygon']\n        \n        if is_touching:\n            if roof1_max_z <= roof2_max_z + 1e-6:\n                is_dominant_roof = False\n                touching_equal_or_higher_neighbors.append({\"Id\": str(roof2_id), \"Height\": round(roof2_max_z, 3)})\n            else:\n                touching_lower_neighbors.append({\"Id\": str(roof2_id), \"Height\": round(roof2_max_z, 3)})\n        \n    if is_dominant_roof and (touching_lower_neighbors or touching_equal_or_higher_neighbors):\n        dominant_roofs_identified.append({\n            \"RhinoObjectId\": str(roof1_id),\n            \"Height\": round(roof1_max_z, 3),\n            \"Location_Min_X\": round(roof1_bbox.Min.X, 3),\n            \"Location_Min_Y\": round(roof1_bbox.Min.Y, 3),\n            \"Polygon\": roof1_poly_2d,\n            \"TouchingLowerNeighbors\": touching_lower_neighbors,\n            \"TouchingEqualOrHigherNeighbors\": touching_equal_or_higher_neighbors\n        })\n\nif dominant_roofs_identified:\n    print(\"\\nRoofs identified as strictly taller than all their directly touching neighbors:\")\n    for roof_info in dominant_roofs_identified:\n        if not roof_info['TouchingEqualOrHigherNeighbors']:\n            print(f\"   Roof ID: {roof_info['RhinoObjectId']} (Height: {roof_info['Height']}m)\")\n            print(f\"     Location (Min XY): ({roof_info['Location_Min_X']}, {roof_info['Location_Min_Y']})\")\n            if roof_info['TouchingLowerNeighbors']:\n                neighbor_details = \", \".join([f\"ID: {n['Id']} (H: {n['Height']}m)\" for n in roof_info['TouchingLowerNeighbors']])\n                print(f\"     Touching Lower Neighbors: {neighbor_details}\")\n            else:\n                print(f\"     No directly touching lower neighbors found (might be isolated or higher than implied).\")\nelse:\n    print(\"\\nNo roofs found that are strictly taller than all their directly touching neighbors.\")\n\n\n# # Ask for number of floors\n# while True:\n#     try:\n#         num_floors = int(input(\"How many floors does the building have? (e.g., 2 for ground + 1 middle + roof): \"))\n#         if num_floors < 1:\n#             raise ValueError\n#         break\n#     except ValueError:\n#         print(\"Please enter a valid positive integer for the number of floors.\")\n\n# --- Section for perimeter line and wall thickness ---\n\nwall_thickness = 0.3  # meters (30 cm)\nprint(f\"Using default wall thickness: {wall_thickness} m\")\n\n\ncombined_building_polygon = MultiPolygon(building_floor_footprints)\n\ntry:\n    exterior_perimeter = combined_building_polygon.buffer(wall_thickness, join_style=1)\nexcept Exception as e:\n    print(f\"Could not buffer the building outline. Error: {e}\")\n    exterior_perimeter = None\n\nif exterior_perimeter and exterior_perimeter.geom_type == 'MultiPolygon':\n    exterior_perimeter = max(exterior_perimeter.geoms, key=lambda p: p.area)\n\nperimeter_line_coords = []\nif exterior_perimeter:\n    if exterior_perimeter.geom_type == 'Polygon':\n        perimeter_line_coords = list(exterior_perimeter.exterior.coords)\n    elif exterior_perimeter.geom_type == 'MultiPolygon':\n        perimeter_line_coords = list(exterior_perimeter.geoms[0].exterior.coords)\n    else:\n        print(\"Warning: The buffered perimeter is not a Polygon or MultiPolygon. Cannot extract line coordinates.\")\n\n# --- End perimeter section ---\n\ndetected_rooms = sorted([(poly, poly.area) for poly in building_floor_footprints], key=lambda x: -x[1])\nif not detected_rooms:\n    raise RuntimeError(\"No valid rooms detected after filtering by area. Check your Rhino model geometry.\")\n\nimport rhino3dm\nimport os\nfrom shapely.geometry import Polygon, MultiPolygon\nfrom shapely.ops import unary_union # For combining polygons on a floor\n\n# Assume floor_height and Z_FLATNESS_TOLERANCE are defined\n\n# --- Step 1: Categorize Floor Footprints by Height/Floor Level ---\nfloor_footprints_by_level = {}\nfloor_z_levels = set() # To store the z-coordinates of each detected floor\n\nfor obj in model.Objects:\n    geom = obj.Geometry\n    if geom.ObjectType == rhino3dm.ObjectType.Mesh:\n        bbox = geom.GetBoundingBox()\n        \n        bbox_x_dim = bbox.Max.X - bbox.Min.X\n        bbox_y_dim = bbox.Max.Y - bbox.Min.Y\n        bbox_z_dim = bbox.Max.Z - bbox.Min.Z\n\n        if bbox_z_dim < Z_FLATNESS_TOLERANCE and bbox_x_dim > 0.1 and bbox_y_dim > 0.1:\n            base_pts = [\n                [bbox.Min.X, bbox.Min.Y],\n                [bbox.Max.X, bbox.Min.Y],\n                [bbox.Max.X, bbox.Max.Y],\n                [bbox.Min.X, bbox.Max.Y],\n                [bbox.Min.X, bbox.Min.Y]\n            ]\n            poly = Polygon(base_pts)\n            if poly.area > 1e-3:\n                # Estimate floor level based on min Z of the bbox\n                # You might need to refine this to snap to discrete floor levels\n                \n                # A more robust way might be to cluster Z values\n                # For simplicity, let's just use the rounded Z for grouping for now\n                approx_z = round(bbox.Max.Z / floor_height) * floor_height # Snap to nearest floor_height multiple\n                floor_z_levels.add(approx_z)\n                \n                if approx_z not in floor_footprints_by_level:\n                    floor_footprints_by_level[approx_z] = []\n                floor_footprints_by_level[approx_z].append(poly)\n\n# Sort the Z levels to process floors in order\nsorted_floor_z_levels = sorted(list(floor_z_levels))\nprint(f\"Detected Z levels for floors: {sorted_floor_z_levels}\")\n\n# --- REVISED CANTILEVER DETECTION LOGIC ---\nCANTILEVER_AREA_THRESHOLD = 0.5 # m^2, adjust as needed\n\nprint(\"\\n--- REVISING Cantilever Analysis (Ground Floor vs. First Floor) ---\")\n\ndetected_cantilevers = []\n\n# Store regions where columns should NOT be placed from ground to first floor\ncantilever_no_column_zones_ground_to_first = [] \n\n# Define a small buffer for spatial checks to account for floating point inaccuracies\nCANTILEVER_CHECK_BUFFER = 0.05 # meters, a small buffer for point-in-polygon checks\n\n# Initialize a set to store the (x,y) coordinates of columns that should *not* extend from ground to first floor.\n# These columns will start from the first floor level instead of the ground.\ncolumns_to_skip_ground_to_first_span = set()\n\n# Ensure we have at least a ground floor and a \"first floor\" above it\nif len(sorted_floor_z_levels) >= 2:\n    ground_floor_z = sorted_floor_z_levels[0] # Assuming the lowest Z is the ground floor\n    first_floor_above_ground_z = sorted_floor_z_levels[1] # This is the \"first floor\" to check for cantilevers\n\n    ground_floor_polygons = floor_footprints_by_level.get(ground_floor_z, [])\n    first_floor_polygons = floor_footprints_by_level.get(first_floor_above_ground_z, [])\n\n    if not ground_floor_polygons:\n        print(f\"No ground floor polygons found at Z level {ground_floor_z:.2f}. Cannot check for first floor cantilevers.\")\n    elif not first_floor_polygons:\n        print(f\"No first floor polygons found at Z level {first_floor_above_ground_z:.2f}. No cantilevers to detect.\")\n    else:\n        # Create a combined footprint for the ground floor\n        combined_ground_floor_footprint = unary_union(ground_floor_polygons)\n        \n        print(f\"\\nChecking First Floor (Z={first_floor_above_ground_z:.2f}m) against Ground Floor (Z={ground_floor_z:.2f}m) for cantilevers.\")\n\n        for first_floor_poly in first_floor_polygons:\n            # Calculate the part of the first floor polygon that extends beyond the ground floor\n            cantilever_part = first_floor_poly.difference(combined_ground_floor_footprint)\n\n            if not cantilever_part.is_empty and cantilever_part.geom_type in ['Polygon', 'MultiPolygon']:\n                cantilever_area = 0\n                if cantilever_part.geom_type == 'Polygon':\n                    cantilever_area = cantilever_part.area\n                else: # MultiPolygon\n                    for geom in cantilever_part.geoms:\n                        if geom.geom_type == 'Polygon':\n                            cantilever_area += geom.area\n                \n                if cantilever_area > CANTILEVER_AREA_THRESHOLD:\n                    detected_cantilevers.append({\n                        \"UpperFloorZ\": round(first_floor_above_ground_z, 2),\n                        \"LowerFloorZ\": round(ground_floor_z, 2),\n                        \"CantileverArea\": round(cantilever_area, 2),\n                        \"CantileverGeometry\": cantilever_part,\n                        \"UpperFloorPolygonCenter\": [round(first_floor_poly.centroid.x, 2), round(first_floor_poly.centroid.y, 2)]\n                    })\n                    print(f\"  Detected cantilever at First Floor Z={first_floor_above_ground_z:.2f}m with area: {cantilever_area:.2f} mÂ²\")\n                    print(f\"    Originating polygon center: X={first_floor_poly.centroid.x:.2f}, Y={first_floor_poly.centroid.y:.2f}\")\nelse:\n    print(\"\\nNot enough floor levels (less than 2) to check for first floor cantilevers.\")\n\nif detected_cantilevers:\n    print(\"\\n--- Summary of Detected First Floor Cantilevers ---\")\n    for cantilever in detected_cantilevers:\n        print(f\"  - Upper Floor Z: {cantilever['UpperFloorZ']}m, Lower Floor Z: {cantilever['LowerFloorZ']}m, Cantilever Area: {cantilever['CantileverArea']} mÂ²\")\nelse:\n    print(\"\\nNo significant first floor cantilevers detected based on the defined thresholds.\")\n\n# ... (your existing REVISED Cantilever Detection Logic ends here) ...\n\n# --- Identify \"No-Column\" Zones for Cantilevers (Ground to First Floor) ---\nif detected_cantilevers and len(sorted_floor_z_levels) >= 2:\n    ground_floor_z = sorted_floor_z_levels[0]\n    combined_ground_floor_footprint = unary_union(floor_footprints_by_level.get(ground_floor_z, []))\n\n    if not combined_ground_floor_footprint.is_empty:\n        print(\"\\n--- Identifying Cantilever 'No-Column' Zones ---\")\n        for cantilever_info in detected_cantilevers:\n            cantilever_geom = cantilever_info['CantileverGeometry']\n            \n            # The perimeter of the cantilever geometry\n            cantilever_perimeter = cantilever_geom.exterior if cantilever_geom.geom_type == 'Polygon' else unary_union([g.exterior for g in cantilever_geom.geoms if g.geom_type == 'Polygon'])\n\n            if cantilever_perimeter.is_empty:\n                continue\n\n            # The part of the cantilever perimeter that does NOT touch the ground floor footprint\n            exposed_perimeter_candidate = cantilever_perimeter.difference(combined_ground_floor_footprint.buffer(CANTILEVER_CHECK_BUFFER))\n\n            # Filter out small artifacts and keep only LineString or MultiLineString components\n            no_column_lines = []\n            if exposed_perimeter_candidate.geom_type == 'LineString':\n                if exposed_perimeter_candidate.length > 0.1: # Only consider significant lengths\n                    no_column_lines.append(exposed_perimeter_candidate)\n            elif exposed_perimeter_candidate.geom_type == 'MultiLineString':\n                for line in exposed_perimeter_candidate.geoms:\n                    if line.length > 0.1:\n                        no_column_lines.append(line)\n            elif exposed_perimeter_candidate.geom_type == 'Polygon': # Should ideally not be a polygon unless a very thin part\n                if exposed_perimeter_candidate.area > 0.01: # Small area threshold\n                    no_column_lines.append(exposed_perimeter_candidate.exterior) # Use its exterior as the line\n\n            for line in no_column_lines:\n                # Create a thin rectangular zone along this line\n                try:\n                    no_column_zone = line.buffer(CANTILEVER_CHECK_BUFFER, cap_style=3) # cap_style=3 for square ends\n                    cantilever_no_column_zones_ground_to_first.append(no_column_zone)\n                except Exception as e:\n                    print(f\"Warning: Could not create no-column zone from line. Error: {e}\")\n        \n        # Combine all no-column zones into a single MultiPolygon for efficient checking\n        if cantilever_no_column_zones_ground_to_first:\n            cantilever_no_column_zones_ground_to_first_combined = unary_union(cantilever_no_column_zones_ground_to_first)\n            print(f\"  Combined {len(cantilever_no_column_zones_ground_to_first)} no-column zones.\")\n        else:\n            cantilever_no_column_zones_ground_to_first_combined = None\n            print(\"  No significant cantilever no-column zones identified.\")\n    else:\n        cantilever_no_column_zones_ground_to_first_combined = None\n        print(\"  Ground floor footprint is empty, cannot define cantilever no-column zones.\")\nelse:\n    cantilever_no_column_zones_ground_to_first_combined = None\n    print(\"  No detected cantilevers or not enough floor levels to define no-column zones.\")\n\n\n# Structural logic\nMaxS = 6.0\nMinS = 3.0\n\ncolumns_2d_points = [] # Store raw (x,y) points for columns\nbeams_2d_lines = []    # Store raw ((x1,y1),(x2,y2)) for beams for 2D plot\n\nadded_column_xy = set()\ncolumns_to_skip_ground_to_first_span = set()\n\n# Force columns at the corners of dominant roof footprints (this part is fine, columns go to ground)\n# ... (your existing code for forced columns) ...\n\n# Iterate through detected rooms/floor footprints\nfor approx_z, floor_polygons in floor_footprints_by_level.items():\n    current_floor_z = approx_z # Get the Z-level for the current set of polygons\n\n    for room_poly in floor_polygons: # Iterate through individual room polygons on this floor\n        minx, miny, maxx, maxy = room_poly.bounds\n        width, height = maxx - minx, maxy - miny\n        \n        divisions_x = max(1, int(np.ceil(width / MaxS)))\n        divisions_y = max(1, int(np.ceil(height / MaxS)))\n        \n        x_points_grid = np.linspace(minx, maxx, divisions_x + 1)\n        y_points_grid = np.linspace(miny, maxy, divisions_y + 1)\n        \n        # Add interior grid columns (this part is fine, columns go to ground unless skipped)\n        for x in x_points_grid:\n            for y in y_points_grid:\n                col_pt = Point(x, y)\n                rounded_x = round(x, 5)\n                rounded_y = round(y, 5)\n                \n                if room_poly.contains(col_pt) or room_poly.buffer(1e-6).contains(col_pt):\n                    if (rounded_x, rounded_y) not in added_column_xy:\n                        if all(np.linalg.norm(np.array((rounded_x, rounded_y)) - np.array(exist_col_xy)) >= MinS for exist_col_xy in added_column_xy):\n                            columns_2d_points.append((rounded_x, rounded_y))\n                            added_column_xy.add((rounded_x, rounded_y))\n\n                    # Check for cantilever no-column zones for the vertical span\n                    if cantilever_no_column_zones_ground_to_first_combined:\n                        if cantilever_no_column_zones_ground_to_first_combined.intersects(col_pt.buffer(CANTILEVER_CHECK_BUFFER)):\n                            columns_to_skip_ground_to_first_span.add((rounded_x, rounded_y))\n\n        # Add columns at corners of the room polygon (this part is fine)\n        for corner_x, corner_y in room_poly.exterior.coords:\n            corner_pt_shapely = Point(corner_x, corner_y)\n            rounded_corner_x = round(corner_x, 5)\n            rounded_corner_y = round(corner_y, 5)\n            \n            if (rounded_corner_x, rounded_corner_y) not in added_column_xy:\n                if all(np.linalg.norm(np.array((corner_x, corner_y)) - np.array(exist_col_xy)) >= MinS * 0.5 for exist_col_xy in added_column_xy):\n                    columns_2d_points.append((rounded_corner_x, rounded_corner_y))\n                    added_column_xy.add((rounded_corner_x, rounded_corner_y))\n            \n            # Check for cantilever no-column zones for the vertical span\n            if cantilever_no_column_zones_ground_to_first_combined:\n                if cantilever_no_column_zones_ground_to_first_combined.intersects(corner_pt_shapely.buffer(CANTILEVER_CHECK_BUFFER)):\n                    columns_to_skip_ground_to_first_span.add((rounded_corner_x, rounded_corner_y))\n\n        # IMPORTANT: NEW BEAM GENERATION LOGIC - APPLY CONDITION HERE\n        if abs(current_floor_z - ground_floor_z) > 1e-4: # Or 1e-5, depending on precision needed            # Horizontal beams\n            for y_fixed in y_points_grid:\n                points_on_line = []\n                for x_coord in x_points_grid:\n                    p = Point(x_coord, y_fixed)\n                    if room_poly.buffer(1e-6).contains(p):\n                        points_on_line.append((x_coord, y_fixed))\n                \n                if len(points_on_line) > 1:\n                    for i in range(len(points_on_line) - 1):\n                        beams_2d_lines.append((points_on_line[i], points_on_line[i+1]))\n\n            # Vertical beams\n            for x_fixed in x_points_grid:\n                points_on_line = []\n                for y_coord in y_points_grid:\n                    p = Point(x_fixed, y_coord)\n                    if room_poly.buffer(1e-6).contains(p):\n                        points_on_line.append((x_fixed, y_coord))\n                \n                if len(points_on_line) > 1:\n                    for i in range(len(points_on_line) - 1):\n                        beams_2d_lines.append((points_on_line[i], points_on_line[i+1]))\n\n\n# Remove duplicate 2D beam lines\nunique_beams_2d = set()\nfor p1, p2 in beams_2d_lines:\n    # Ensure consistent order for tuple comparison\n    ordered_segment = tuple(sorted((tuple(np.round(p1,5)), tuple(np.round(p2,5)))))\n    unique_beams_2d.add(ordered_segment)\nbeams_2d_lines = [ (np.array(p1), np.array(p2)) for p1,p2 in unique_beams_2d]\n\n\n# Combine all base columns\nall_base_columns = list(added_column_xy)\n\n# --- COLUMN GRID NUMBERING LOGIC ---\ngrid_xs = sorted(list(set([col[0] for col in all_base_columns])))\ngrid_ys = sorted(list(set([col[1] for col in all_base_columns])))\n\nx_grid_labels = {x: chr(65 + i) for i, x in enumerate(grid_xs)}\ny_grid_labels = {y: i + 1 for i, y in enumerate(grid_ys)}\n\ncol_min_x_extent = min(col[0] for col in all_base_columns) if all_base_columns else 0\ncol_max_x_extent = max(col[0] for col in all_base_columns) if all_base_columns else 0\ncol_min_y_extent = min(col[1] for col in all_base_columns) if all_base_columns else 0\ncol_max_y_extent = max(col[1] for col in all_base_columns) if all_base_columns else 0\n\ngrid_extent_buffer = 1.0\ncol_min_x_extent -= grid_extent_buffer\ncol_max_x_extent += grid_extent_buffer\ncol_min_y_extent -= grid_extent_buffer\ncol_max_y_extent += grid_extent_buffer\n\nmin_x_plot = min(col_min_x_extent, min([coord[0] for coord in perimeter_line_coords] + [col[0] for col in all_base_columns])) - 3.0 if perimeter_line_coords else col_min_x_extent - 3.0\nmax_x_plot = max(col_max_x_extent, max([coord[0] for coord in perimeter_line_coords] + [col[0] for col in all_base_columns])) + 1.0 if perimeter_line_coords else col_max_x_extent + 1.0\nmin_y_plot = min(col_min_y_extent, min([coord[1] for coord in perimeter_line_coords] + [col[1] for col in all_base_columns])) - 1.0 if perimeter_line_coords else col_min_y_extent - 1.0\nmax_y_plot = max(col_max_y_extent, max([coord[1] for coord in perimeter_line_coords] + [col[1] for col in all_base_columns])) + 2.0 if perimeter_line_coords else col_max_y_extent + 2.0\n\n\n# --- Utility function for wall height ---\ndef get_wall_height(x, y, mesh_bboxes, global_max_z):\n    pt = Point(x, y)\n    relevant_bboxes = []\n    for bbox in mesh_bboxes:\n        bbox_poly = Polygon([\n            [bbox.Min.X, bbox.Min.Y],\n            [bbox.Max.X, bbox.Min.Y],\n            [bbox.Max.X, bbox.Max.Y],\n            [bbox.Min.X, bbox.Max.Y],\n            [bbox.Min.X, bbox.Min.Y]\n        ])\n        if bbox_poly.buffer(1e-4).contains(pt):\n            relevant_bboxes.append(bbox)\n\n    if not relevant_bboxes:\n        return global_max_z \n\n    max_relevant_z = 0.0\n    for bbox in relevant_bboxes:\n        max_relevant_z = max(max_relevant_z, bbox.Max.Z)\n        \n    return max_relevant_z if max_relevant_z > 0 else global_max_z\n\n\n# --- DATA GENERATION AND COMBINED JSON EXPORT ---\nprint(\"--- Generating data and creating combined JSON ---\")\n\nEXPORT_SAVE_PATH = os.path.join(os.getcwd(), \"structural_data\") # Use current working directory\n\nos.makedirs(EXPORT_SAVE_PATH, exist_ok=True)\nprint(f\"Ensuring directory exists: {os.path.abspath(EXPORT_SAVE_PATH)}\")\n\nnode_coords = []\nnode_dict = OrderedDict()\n\ndef add_node(pt):\n    key = tuple(np.round(pt, 5))\n    if key not in node_dict:\n        node_id = f\"N{len(node_dict)}\"\n        node_dict[key] = node_id\n        node_coords.append([node_id] + list(key))\n    return node_dict[key]\n\ncolumn_lines = []\nfor x, y in all_base_columns:\n    current_column_max_z = get_wall_height(x, y, all_mesh_bboxes, max_z)\n    \n    # Determine the actual floor Z-levels that this specific column should span\n    # We need to map the generic 'num_floors' based Z calculation to the actual detected floor Zs.\n\n    # Find the actual ground floor Z and first floor above ground Z\n    # Ensure sorted_floor_z_levels is populated and contains these\n    local_ground_floor_z = sorted_floor_z_levels[0] if sorted_floor_z_levels else 0.0\n    local_first_floor_z = sorted_floor_z_levels[1] if len(sorted_floor_z_levels) >= 2 else local_ground_floor_z # Fallback\n    \n    # Check if this column needs to skip the ground-to-first-floor span\n    skip_ground_to_first = (round(x, 5), round(y, 5)) in columns_to_skip_ground_to_first_span\n\n    # Define the actual Z-levels for this column's segments\n    # Start with the local_ground_floor_z\n    column_z_levels_for_this_column = [local_ground_floor_z] \n\n    # Add all other detected floor Z-levels\n    # Skip adding local_ground_floor_z if it's already there\n    for z in sorted_floor_z_levels:\n        if z > local_ground_floor_z + 1e-6: # Add levels above ground, avoiding duplicates\n            column_z_levels_for_this_column.append(z)\n    \n    # Filter out Z-levels that are above the local building's height at this column location\n    column_z_levels_for_this_column = [z for z in column_z_levels_for_this_column if z <= current_column_max_z + 1e-6]\n    \n    # IMPORTANT: If skipping ground-to-first, make sure the first Z level is first_floor_above_ground_z\n    # and remove any floor Zs below that (like the actual ground floor).\n    if skip_ground_to_first and local_first_floor_z > local_ground_floor_z + 1e-6: # Ensure there IS a first floor\n        # Remove any Z-levels below the first floor if skipping\n        # This handles cases where local_first_floor_z might be slightly different from column_z_levels_for_this_column[0]\n        column_z_levels_for_this_column = [z for z in column_z_levels_for_this_column if z >= local_first_floor_z - 1e-6]\n        # Ensure the list is not empty, if it becomes empty, this column might not exist at all,\n        # or it should just start at its lowest valid Z.\n        if not column_z_levels_for_this_column:\n            column_z_levels_for_this_column = [local_first_floor_z] # Make sure there's at least one start point\n        elif column_z_levels_for_this_column[0] > local_first_floor_z + 1e-6:\n             # If the lowest valid Z is higher than expected, prepend the actual first_floor_z\n             column_z_levels_for_this_column.insert(0, local_first_floor_z)\n\n    # Sort and remove duplicates in case of slight floating point differences\n    column_z_levels_for_this_column = sorted(list(set(np.round(column_z_levels_for_this_column, 5))))\n    \n    # Now, generate column segments based on these adjusted Z levels\n    for i in range(len(column_z_levels_for_this_column) - 1):\n        start_z = column_z_levels_for_this_column[i]\n        end_z = column_z_levels_for_this_column[i+1]\n        \n        # Only create a segment if it has a non-zero height\n        if end_z > start_z + 1e-6:\n            id_start_node = add_node((x, y, start_z))\n            id_end_node = add_node((x, y, end_z))\n            column_lines.append((id_start_node, id_end_node))\n\n# Note: The 'num_floors' variable as used in your original loop was implicitly\n# assuming evenly spaced floors up to `current_column_max_z`. By using `sorted_floor_z_levels`,\n# we are now explicitly using the detected floor heights, which is more robust\n# and directly aligns with your floor detection.\n\n# ... (rest of your beam_lines generation, which doesn't need modification for this specific rule) ...\nbeam_lines = []\nunique_beam_tuples_3d = set() # Use a new set for 3D beam uniqueness\n\nfor (x1, y1), (x2, y2) in beams_2d_lines:\n    mid_x, mid_y = (x1 + x2) / 2, (y1 + y2) / 2\n    beam_local_max_z = get_wall_height(mid_x, mid_y, all_mesh_bboxes, max_z)\n\n    for floor_z_level_actual in sorted_floor_z_levels:\n        if floor_z_level_actual > beam_local_max_z + 1e-6:\n            continue\n\n        # ðŸ›‘ Skip beams at ground level (Z â‰ˆ 0)\n        if abs(floor_z_level_actual) < 1e-4:\n            continue\n\n        floor_z_level_rounded = round(floor_z_level_actual, 5)\n\n        id1 = add_node((x1, y1, floor_z_level_rounded))\n        id2 = add_node((x2, y2, floor_z_level_rounded))\n\n        ordered_nodes = tuple(sorted((id1, id2)))\n        if ordered_nodes not in unique_beam_tuples_3d:\n            unique_beam_tuples_3d.add(ordered_nodes)\n            beam_lines.append((id1, id2))\n\n\n# --- Create combined structural data ---\nprint(\"--- Creating combined structural data ---\")\n\n# Create DataFrames\ndf_nodes = pd.DataFrame(node_coords, columns=[\"ID\", \"X\", \"Y\", \"Z\"])\n\n# Prepare nodes data\nnodes_data = []\nfor node in node_coords:\n    nodes_data.append({\n        \"ID\": node[0],\n        \"X\": node[1],\n        \"Y\": node[2],\n        \"Z\": node[3]\n    })\n\n# Prepare columns data  \ncolumns_data = []\nfor i, (i_node_id, j_node_id) in enumerate(column_lines):\n    p1_coords = df_nodes[df_nodes[\"ID\"] == i_node_id][[\"X\", \"Y\", \"Z\"]].values[0]\n    p2_coords = df_nodes[df_nodes[\"ID\"] == j_node_id][[\"X\", \"Y\", \"Z\"]].values[0]\n    length = np.linalg.norm(p2_coords - p1_coords)\n    columns_data.append({\n        \"ID\": f\"C{i}\",\n        \"i_node\": i_node_id,\n        \"j_node\": j_node_id,\n        \"length\": round(length, 3)\n    })\n\n# Prepare beams data\nbeams_data = []\nfor i, (i_node_id, j_node_id) in enumerate(beam_lines):\n    p1_coords = df_nodes[df_nodes[\"ID\"] == i_node_id][[\"X\", \"Y\", \"Z\"]].values[0]\n    p2_coords = df_nodes[df_nodes[\"ID\"] == j_node_id][[\"X\", \"Y\", \"Z\"]].values[0]\n    length = np.linalg.norm(p2_coords - p1_coords)\n    beams_data.append({\n        \"ID\": f\"B{i}\",\n        \"i_node\": i_node_id,\n        \"j_node\": j_node_id,\n        \"length\": round(length, 3)\n    })\n\n# Create DataFrames for later processing\ndf_columns = pd.DataFrame([(f\"C{i}\", i_node_id, j_node_id, round(np.linalg.norm(df_nodes[df_nodes[\"ID\"] == j_node_id][[\"X\", \"Y\", \"Z\"]].values[0] - df_nodes[df_nodes[\"ID\"] == i_node_id][[\"X\", \"Y\", \"Z\"]].values[0]), 3)) for i, (i_node_id, j_node_id) in enumerate(column_lines)], columns=[\"ID\", \"i_node\", \"j_node\", \"length\"])\ndf_beams = pd.DataFrame([(f\"B{i}\", i_node_id, j_node_id, round(np.linalg.norm(df_nodes[df_nodes[\"ID\"] == j_node_id][[\"X\", \"Y\", \"Z\"]].values[0] - df_nodes[df_nodes[\"ID\"] == i_node_id][[\"X\", \"Y\", \"Z\"]].values[0]), 3)) for i, (i_node_id, j_node_id) in enumerate(beam_lines)], columns=[\"ID\", \"i_node\", \"j_node\", \"length\"])\n\n\n# ---------------------------------------\n# Data Processing and Analysis Section\n# ---------------------------------------\n\n# Re-load data and re-run identification logic to ensure all variables are defined\n# This part processes the structural data to identify and analyze beam patterns\n\nprint(f\"Using in-memory structural data from previous processing\")\nprint(f\"Using in-memory 3DM model for analysis\")\n\n# Use the DataFrames from the previous section (already available in memory)\ndf_nodes_loaded = df_nodes.copy()\ndf_columns_loaded = df_columns.copy()\ndf_beams_loaded = df_beams.copy()\nprint(\"âœ… Structural data ready for update process.\")\n\n# Use the in-memory model (already loaded above)\n\nbuilding_floor_footprints = []\nall_mesh_bboxes = []\nroof_meshes_info = []\nmax_z_overall_model = 0.0\nZ_FLATNESS_TOLERANCE = 0.1\n\nfor obj in model.Objects:\n    geom = obj.Geometry\n    if geom.ObjectType == rhino3dm.ObjectType.Mesh:\n        bbox = geom.GetBoundingBox()\n        bbox_z_dim = bbox.Max.Z - bbox.Min.Z\n        if bbox_z_dim < Z_FLATNESS_TOLERANCE and (bbox.Max.X - bbox.Min.X) > 0.1 and (bbox.Max.Y - bbox.Min.Y) > 0.1:\n            base_pts = [[bbox.Min.X, bbox.Min.Y], [bbox.Max.X, bbox.Min.Y], [bbox.Max.X, bbox.Max.Y], [bbox.Min.X, bbox.Max.Y], [bbox.Min.X, bbox.Min.Y]]\n            poly = Polygon(base_pts)\n            if poly.area > 1e-3:\n                building_floor_footprints.append(poly)\n                roof_meshes_info.append((obj.Attributes.Id, bbox, poly))\n        all_mesh_bboxes.append(bbox)\n        max_z_overall_model = max(max_z_overall_model, bbox.Max.Z)\n\ndef get_wall_height(x, y, mesh_bboxes, global_max_z):\n    pt = Point(x, y)\n    relevant_bboxes = []\n    for bbox in mesh_bboxes:\n        bbox_poly = Polygon([[bbox.Min.X, bbox.Min.Y], [bbox.Max.X, bbox.Min.Y], [bbox.Max.X, bbox.Max.Y], [bbox.Min.X, bbox.Max.Y], [bbox.Min.X, bbox.Min.Y]])\n        if bbox_poly.buffer(1e-4).contains(pt):\n            relevant_bboxes.append(bbox)\n    if not relevant_bboxes: return global_max_z\n    max_relevant_z = 0.0\n    for bbox in relevant_bboxes:\n        max_relevant_z = max(max_relevant_z, bbox.Max.Z)\n    return max_relevant_z if max_relevant_z > 0 else global_max_z\n\n# Re-identify dominant roofs\nINTERSECTION_BUFFER_ROOF = 0.1\ndominant_roofs_identified = []\nfor i, (roof1_id, roof1_bbox, roof1_poly_2d) in enumerate(roof_meshes_info):\n    roof1_max_z = roof1_bbox.Max.Z\n    is_dominant_roof = True\n    for j, (roof2_id, roof2_bbox, roof2_poly_2d) in enumerate(roof_meshes_info):\n        if i == j: continue\n        roof2_max_z = roof2_bbox.Max.Z\n        intersection_geometry = roof1_poly_2d.buffer(INTERSECTION_BUFFER_ROOF).intersection(roof2_poly_2d.buffer(INTERSECTION_BUFFER_ROOF))\n        is_touching = not intersection_geometry.is_empty and intersection_geometry.geom_type in ['LineString', 'MultiLineString', 'Polygon', 'MultiPolygon']\n        if is_touching:\n            if roof1_max_z <= roof2_max_z + 1e-6:\n                is_dominant_roof = False\n                break\n    if is_dominant_roof and (len(roof_meshes_info) > 1 or len(dominant_roofs_identified) == 0):\n        if not is_dominant_roof: continue\n        dominant_roofs_identified.append({\"RhinoObjectId\": str(roof1_id), \"Height\": round(roof1_max_z, 3), \"Polygon_2D\": roof1_poly_2d})\n\n# Re-identify peak roof beams\npeak_roof_beam_ids = set()\nZ_HEIGHT_TOLERANCE = 0.5\nnode_id_to_coords = df_nodes_loaded.set_index('ID')[['X', 'Y', 'Z']].T.to_dict('list')\n\nfor index, row in df_beams_loaded.iterrows():\n    beam_id = row['ID']\n    n1_id = row['i_node']\n    n2_id = row['j_node']\n    try:\n        p1_coords = np.array(node_id_to_coords[n1_id])\n        p2_coords = np.array(node_id_to_coords[n2_id])\n    except KeyError:\n        continue\n    mid_x, mid_y = (p1_coords[0] + p2_coords[0]) / 2, (p1_coords[1] + p2_coords[1]) / 2\n    avg_z = (p1_coords[2] + p2_coords[2]) / 2\n\n    # Get the \"true\" max Z for the area covered by this beam (using its midpoint)\n    local_max_z_at_beam_location = get_wall_height(mid_x, mid_y, all_mesh_bboxes, max_z_overall_model)\n\n    # If the beam's average Z is within tolerance of the local max Z, consider it a peak roof beam\n    if abs(local_max_z_at_beam_location - avg_z) < Z_HEIGHT_TOLERANCE:\n        # Also check if this beam's 2D projection falls within any dominant roof polygon\n        beam_line_2d = LineString([(p1_coords[0], p1_coords[1]), (p2_coords[0], p2_coords[1])])\n        for roof_info in dominant_roofs_identified:\n            roof_poly_2d = roof_info[\"Polygon_2D\"]\n            if roof_poly_2d.buffer(0.01).intersects(beam_line_2d): # Small buffer for intersection\n                peak_roof_beam_ids.add(beam_id)\n                break # Found a matching dominant roof, no need to check other roofs for this beam\n\n\n# Generate 3D visualization\nimport plotly.graph_objects as go # Added this import\n\nfig_3d = go.Figure()\n\n# Add columns\nfor index, row in df_columns_loaded.iterrows():\n    n1_coords = df_nodes_loaded[df_nodes_loaded['ID'] == row['i_node']][['X', 'Y', 'Z']].values[0]\n    n2_coords = df_nodes_loaded[df_nodes_loaded['ID'] == row['j_node']][['X', 'Y', 'Z']].values[0]\n    fig_3d.add_trace(go.Scatter3d(\n        x=[n1_coords[0], n2_coords[0]],\n        y=[n1_coords[1], n2_coords[1]],\n        z=[n1_coords[2], n2_coords[2]],\n        mode='lines',\n        line=dict(color='blue', width=5),\n        name=f'Column {row[\"ID\"]}',\n        hoverinfo='text',\n        text=f'Column: {row[\"ID\"]}<br>Length: {row[\"length\"]:.2f}m'\n    ))\n\n# Add beams\nfor index, row in df_beams_loaded.iterrows():\n    n1_coords = df_nodes_loaded[df_nodes_loaded['ID'] == row['i_node']][['X', 'Y', 'Z']].values[0]\n    n2_coords = df_nodes_loaded[df_nodes_loaded['ID'] == row['j_node']][['X', 'Y', 'Z']].values[0]\n    \n    line_color = 'green'\n    line_width = 3\n    if row['ID'] in peak_roof_beam_ids:\n        line_color = 'red' # Highlight peak roof beams\n        line_width = 6\n\n    fig_3d.add_trace(go.Scatter3d(\n        x=[n1_coords[0], n2_coords[0]],\n        y=[n1_coords[1], n2_coords[1]],\n        z=[n1_coords[2], n2_coords[2]],\n        mode='lines',\n        line=dict(color=line_color, width=line_width),\n        name=f'Beam {row[\"ID\"]}',\n        hoverinfo='text',\n        text=f'Beam: {row[\"ID\"]}<br>Length: {row[\"length\"]:.2f}m'\n    ))\n\n# Add nodes as markers\nfig_3d.add_trace(go.Scatter3d(\n    x=df_nodes_loaded['X'],\n    y=df_nodes_loaded['Y'],\n    z=df_nodes_loaded['Z'],\n    mode='markers',\n    marker=dict(size=4, color='black'),\n    name='Nodes',\n    hoverinfo='text',\n    text=df_nodes_loaded['ID']\n))\n\n# Update layout for better visualization\nfig_3d.update_layout(\n    title='3D Structural Model with Columns (Blue), Beams (Green), and Peak Roof Beams (Red)',\n    scene=dict(\n        xaxis_title='X',\n        yaxis_title='Y',\n        zaxis_title='Z',\n        aspectmode='data' # Ensure uniform scaling\n    ),\n    height=800\n)\n\nfig_3d.show()\n\nprint(f\"\\nTotal peak roof beams identified: {len(peak_roof_beam_ids)}\")\nif peak_roof_beam_ids:\n    print(\"IDs of peak roof beams:\", \", \".join(sorted(list(peak_roof_beam_ids))))\n\n\n\n#--------------------------------------\n# Extended Analysis and Data Processing\n#-----------------------\n\nimport pandas as pd\nimport plotly.graph_objects as go\nimport os\nimport numpy as np\nimport rhino3dm\nfrom shapely.geometry import Polygon, Point, MultiPolygon, LineString\nfrom collections import OrderedDict # Re-import for OrderedDict if not already global\n\n# --- Re-load data and re-run identification logic to ensure all variables are defined ---\n# This part is a copy of the previous cell's data loading and identification logic\n# to ensure this cell can run independently if the previous one wasn't executed immediately before.\n\n# base_github_path = r\"C:\\Users\\papad\\Documents\\GitHub\\Octopusie\"\nimport os\n\nscript_dir = os.path.dirname(os.path.abspath(__file__))\nbase_github_path = os.path.abspath(os.path.join(script_dir, \"..\"))\n\nCSV_SAVE_PATH = os.path.join(base_github_path, \"eleftheriaexperiment\", \"structural_data\")\nos.makedirs(CSV_SAVE_PATH, exist_ok=True)  # <â€” ensure it exists!\n\nprint(f\"Using in-memory structural data from previous processing\")\nprint(f\"Using in-memory 3DM model for analysis\")\n\n# Use the DataFrames from the previous section (already available in memory)\ndf_nodes_loaded = df_nodes.copy()\ndf_columns_loaded = df_columns.copy()\ndf_beams_loaded = df_beams.copy()\nprint(\"âœ… Structural data ready for update process.\")\n\n# Use the in-memory model (already loaded above)\n\nbuilding_floor_footprints = []\nall_mesh_bboxes = []\nroof_meshes_info = []\nmax_z_overall_model = 0.0\nZ_FLATNESS_TOLERANCE = 0.1\n\nfor obj in model.Objects:\n    geom = obj.Geometry\n    if geom.ObjectType == rhino3dm.ObjectType.Mesh:\n        bbox = geom.GetBoundingBox()\n        bbox_z_dim = bbox.Max.Z - bbox.Min.Z\n        if bbox_z_dim < Z_FLATNESS_TOLERANCE and (bbox.Max.X - bbox.Min.X) > 0.1 and (bbox.Max.Y - bbox.Min.Y) > 0.1:\n            base_pts = [[bbox.Min.X, bbox.Min.Y], [bbox.Max.X, bbox.Min.Y], [bbox.Max.X, bbox.Max.Y], [bbox.Min.X, bbox.Max.Y], [bbox.Min.X, bbox.Min.Y]]\n            poly = Polygon(base_pts)\n            if poly.area > 1e-3:\n                building_floor_footprints.append(poly)\n                roof_meshes_info.append((obj.Attributes.Id, bbox, poly))\n        all_mesh_bboxes.append(bbox)\n        max_z_overall_model = max(max_z_overall_model, bbox.Max.Z)\n\ndef get_wall_height(x, y, mesh_bboxes, global_max_z):\n    pt = Point(x, y)\n    relevant_bboxes = []\n    for bbox in mesh_bboxes:\n        bbox_poly = Polygon([[bbox.Min.X, bbox.Min.Y], [bbox.Max.X, bbox.Min.Y], [bbox.Max.X, bbox.Max.Y], [bbox.Min.X, bbox.Max.Y], [bbox.Min.X, bbox.Min.Y]])\n        if bbox_poly.buffer(1e-4).contains(pt):\n            relevant_bboxes.append(bbox)\n    if not relevant_bboxes: return global_max_z\n    max_relevant_z = 0.0\n    for bbox in relevant_bboxes:\n        max_relevant_z = max(max_relevant_z, bbox.Max.Z)\n    return max_relevant_z if max_relevant_z > 0 else global_max_z\n\n# Re-identify dominant roofs\nINTERSECTION_BUFFER_ROOF = 0.1\ndominant_roofs_identified = []\nfor i, (roof1_id, roof1_bbox, roof1_poly_2d) in enumerate(roof_meshes_info):\n    roof1_max_z = roof1_bbox.Max.Z\n    is_dominant_roof = True\n    for j, (roof2_id, roof2_bbox, roof2_poly_2d) in enumerate(roof_meshes_info):\n        if i == j: continue\n        roof2_max_z = roof2_bbox.Max.Z\n        intersection_geometry = roof1_poly_2d.buffer(INTERSECTION_BUFFER_ROOF).intersection(roof2_poly_2d.buffer(INTERSECTION_BUFFER_ROOF))\n        is_touching = not intersection_geometry.is_empty and intersection_geometry.geom_type in ['LineString', 'MultiLineString', 'Polygon', 'MultiPolygon']\n        if is_touching:\n            if roof1_max_z <= roof2_max_z + 1e-6:\n                is_dominant_roof = False\n                break\n    if is_dominant_roof and (len(roof_meshes_info) > 1 or len(dominant_roofs_identified) == 0):\n        if not is_dominant_roof: continue\n        dominant_roofs_identified.append({\"RhinoObjectId\": str(roof1_id), \"Height\": round(roof1_max_z, 3), \"Polygon_2D\": roof1_poly_2d})\n\n# Re-identify peak roof beams\npeak_roof_beam_ids = set()\nZ_HEIGHT_TOLERANCE = 0.5\nnode_id_to_coords = df_nodes_loaded.set_index('ID')[['X', 'Y', 'Z']].T.to_dict('list')\n\nfor index, row in df_beams_loaded.iterrows():\n    beam_id = row['ID']\n    n1_id = row['i_node']\n    n2_id = row['j_node']\n    try:\n        p1_coords = np.array(node_id_to_coords[n1_id])\n        p2_coords = np.array(node_id_to_coords[n2_id])\n    except KeyError:\n        continue\n    mid_x, mid_y = (p1_coords[0] + p2_coords[0]) / 2, (p1_coords[1] + p2_coords[1]) / 2\n    avg_z = (p1_coords[2] + p2_coords[2]) / 2\n    beam_midpoint_2d = Point(mid_x, mid_y)\n    for roof_info in dominant_roofs_identified:\n        roof_polygon_2d = roof_info['Polygon_2D']\n        roof_peak_height = roof_info['Height']\n        if roof_polygon_2d.buffer(1e-3).contains(beam_midpoint_2d) and abs(avg_z - roof_peak_height) < Z_HEIGHT_TOLERANCE:\n            peak_roof_beam_ids.add(beam_id)\n            break\n\n# Re-identify low-connectivity linear beams on peak roofs\nbeam_adj_list = {node_id: [] for node_id in df_nodes_loaded['ID']}\nfor index, row in df_beams_loaded.iterrows():\n    beam_adj_list[row['i_node']].append(row['ID'])\n    beam_adj_list[row['j_node']].append(row['ID'])\n\ndef angle_between_vectors(v1, v2):\n    v1, v2 = np.array(v1), np.array(v2)\n    unit_vector_1 = v1 / (np.linalg.norm(v1) + 1e-9)\n    unit_vector_2 = v2 / (np.linalg.norm(v2) + 1e-9)\n    dot_product = np.dot(unit_vector_1, unit_vector_2)\n    dot_product = np.clip(dot_product, -1.0, 1.0)\n    angle_rad = np.arccos(dot_product)\n    return np.degrees(angle_rad)\n\nCOLLINEARITY_TOLERANCE_DEG = 10 \nhighlighted_peak_roof_beams = set()\n\nif not df_beams_loaded.empty:\n    for index, row in df_beams_loaded.iterrows():\n        beam_id = row['ID']\n        if beam_id not in peak_roof_beam_ids:\n            continue\n\n        n1_id, n2_id = row['i_node'], row['j_node']\n        is_low_connectivity_linear = False\n\n        if len(beam_adj_list[n1_id]) == 2:\n            other_beam_ids_at_n1 = [b_id for b_id in beam_adj_list[n1_id] if b_id != beam_id]\n            if other_beam_ids_at_n1:\n                other_beam_id_at_n1 = other_beam_ids_at_n1[0]\n                current_beam_other_node = n2_id\n                other_beam_row = df_beams_loaded[df_beams_loaded['ID'] == other_beam_id_at_n1].iloc[0]\n                other_beam_other_node = other_beam_row['i_node'] if other_beam_row['j_node'] == n1_id else other_beam_row['j_node']\n                coords_n1 = np.array(node_id_to_coords.get(n1_id, [0,0,0]))\n                coords_current_other = np.array(node_id_to_coords.get(current_beam_other_node, [0,0,0]))\n                coords_other_beam_other = np.array(node_id_to_coords.get(other_beam_other_node, [0,0,0]))\n                vector_current_beam = coords_current_other - coords_n1\n                vector_other_beam = coords_other_beam_other - coords_n1\n                angle = angle_between_vectors(vector_current_beam, vector_other_beam)\n                if abs(angle) < COLLINEARITY_TOLERANCE_DEG or abs(angle - 180) < COLLINEARITY_TOLERANCE_DEG:\n                    is_low_connectivity_linear = True\n\n        if not is_low_connectivity_linear and len(beam_adj_list[n2_id]) == 2:\n            other_beam_ids_at_n2 = [b_id for b_id in beam_adj_list[n2_id] if b_id != beam_id]\n            if other_beam_ids_at_n2:\n                other_beam_id_at_n2 = other_beam_ids_at_n2[0]\n                current_beam_other_node = n1_id\n                other_beam_row = df_beams_loaded[df_beams_loaded['ID'] == other_beam_id_at_n2].iloc[0]\n                other_beam_other_node = other_beam_row['i_node'] if other_beam_row['j_node'] == n2_id else other_beam_row['j_node']\n                coords_n2 = np.array(node_id_to_coords.get(n2_id, [0,0,0]))\n                coords_current_other = np.array(node_id_to_coords.get(current_beam_other_node, [0,0,0]))\n                coords_other_beam_other = np.array(node_id_to_coords.get(other_beam_other_node, [0,0,0]))\n                vector_current_beam = coords_current_other - coords_n2\n                vector_other_beam = coords_other_beam_other - coords_n2\n                angle = angle_between_vectors(vector_current_beam, vector_other_beam)\n                if abs(angle) < COLLINEARITY_TOLERANCE_DEG or abs(angle - 180) < COLLINEARITY_TOLERANCE_DEG:\n                    is_low_connectivity_linear = True\n\n        if not is_low_connectivity_linear:\n            if len(beam_adj_list[n1_id]) == 1 or len(beam_adj_list[n2_id]) == 1:\n                is_low_connectivity_linear = True\n        \n        if is_low_connectivity_linear:\n            highlighted_peak_roof_beams.add(beam_id)\n\nprint(f\"\\nIdentified {len(highlighted_peak_roof_beams)} beam segments that are low-connectivity linear ends AND on peak roofs for deletion.\")\nprint(\"IDs of beams to be deleted:\")\nfor beam_id in sorted(list(highlighted_peak_roof_beams)):\n    print(f\"- {beam_id}\")\n\n# --- END Re-run identification logic ---\n\n\n# --- DELETE BEAMS AND UPDATE DATA ---\nprint(\"\\n--- Deleting identified beams and updating structural data ---\")\n\n# Filter df_beams_loaded to remove the identified beams\ndf_beams_updated = df_beams_loaded[~df_beams_loaded['ID'].isin(highlighted_peak_roof_beams)].copy()\n\n# Now, re-prune nodes based on the remaining beams and all columns\nconnected_nodes_after_deletion = set()\n\n# Add nodes from columns\nif not df_columns_loaded.empty:\n    connected_nodes_after_deletion.update(df_columns_loaded[\"i_node\"].tolist())\n    connected_nodes_after_deletion.update(df_columns_loaded[\"j_node\"].tolist())\n\n# Add nodes from the updated beams\nif not df_beams_updated.empty:\n    connected_nodes_after_deletion.update(df_beams_updated[\"i_node\"].tolist())\n    connected_nodes_after_deletion.update(df_beams_updated[\"j_node\"].tolist())\n\nprint(f\"Initial nodes count: {len(df_nodes_loaded)}\")\nprint(f\"Nodes connected after beam deletion: {len(connected_nodes_after_deletion)}\")\n\n# Filter nodes and re-map IDs\ndf_nodes_updated = df_nodes_loaded[df_nodes_loaded['ID'].isin(connected_nodes_after_deletion)].copy()\nnew_node_id_mapping = {old_id: f\"N{i}\" for i, old_id in enumerate(df_nodes_updated['ID'])}\ndf_nodes_updated['ID'] = df_nodes_updated['ID'].map(new_node_id_mapping)\n\n# Rebuild node_lookup_loaded and node_id_to_coords with updated nodes\nnode_lookup_loaded = df_nodes_updated.set_index('ID')\nnode_id_to_coords = df_nodes_updated.set_index('ID')[['X', 'Y', 'Z']].T.to_dict('list')\n\n# Re-map node IDs in columns DataFrame\ndf_columns_updated = df_columns_loaded.copy()\ndf_columns_updated['i_node'] = df_columns_updated['i_node'].map(new_node_id_mapping)\ndf_columns_updated['j_node'] = df_columns_updated['j_node'].map(new_node_id_mapping)\n# Filter out columns whose nodes might have been deleted (map result will be NaN for deleted nodes)\ndf_columns_updated.dropna(subset=['i_node', 'j_node'], inplace=True)\ndf_columns_updated['ID'] = [f\"C{i}\" for i in range(len(df_columns_updated))] # Re-index column IDs\n\n\n# Re-map node IDs in the updated beams DataFrame\ndf_beams_updated['i_node'] = df_beams_updated['i_node'].map(new_node_id_mapping)\ndf_beams_updated['j_node'] = df_beams_updated['j_node'].map(new_node_id_mapping)\n# Filter out beams whose nodes might have been deleted\ndf_beams_updated.dropna(subset=['i_node', 'j_node'], inplace=True)\ndf_beams_updated['ID'] = [f\"B{i}\" for i in range(len(df_beams_updated))] # Re-index beam IDs\n\n\nprint(f\"Final nodes count: {len(df_nodes_updated)}\")\nprint(f\"Final columns count: {len(df_columns_updated)}\")\nprint(f\"Final beams count: {len(df_beams_updated)}\")\n\n\n# --- FINAL DATA PROCESSING AND UPLOAD ---\nprint(\"\\n--- Processing final structural data ---\")\n\n# --- CREATE AND UPLOAD COMBINED JSON TO SUPABASE ---\nprint(\"\\n--- Creating and uploading combined structural data to Supabase ---\")\n\n# Create combined structural data\ncombined_structural_data = {\n    \"metadata\": {\n        \"filename\": file_name_without_ext,\n        \"scale_factor\": scale_factor,\n        \"num_floors\": num_floors,\n        \"wall_thickness\": wall_thickness,\n        \"unity_axis_format\": unityAxisFormat,\n        \"generation_timestamp\": pd.Timestamp.now().isoformat()\n    },\n    \"nodes\": df_nodes_updated.to_dict(orient='records'),\n    \"columns\": df_columns_updated.to_dict(orient='records'),\n    \"beams\": df_beams_updated.to_dict(orient='records')\n}\n\n# Convert to JSON string\ncombined_json_data = json.dumps(combined_structural_data, indent=4)\n\n# Check bucket permissions and list existing JSON files\ntry:\n    print(\"\\n--- Checking bucket access and listing existing JSON files ---\")\n    \n    # First, try to list buckets to check general access\n    try:\n        buckets = supabase.storage.list_buckets()\n        print(f\"âœ… Found {len(buckets)} accessible buckets\")\n        bucket_names = [bucket.name for bucket in buckets]\n        print(f\"Available buckets: {bucket_names}\")\n    except Exception as bucket_err:\n        print(f\"âš ï¸ Warning: Cannot list buckets: {bucket_err}\")\n    \n    # Try to list files in the analysis-results bucket\n    existing_files = supabase.storage.from_(\"analysis-results\").list(\"JSON\")\n    if existing_files:\n        print(f\"Found {len(existing_files)} existing JSON files:\")\n        for file_info in existing_files:\n            print(f\"  - {file_info.get('name', 'Unknown')} (Size: {file_info.get('metadata', {}).get('size', 'Unknown')} bytes)\")\n    else:\n        print(\"No existing JSON files found in the JSON folder.\")\nexcept Exception as e:\n    print(f\"âŒ Error accessing bucket or listing files: {e}\")\n    print(f\"This might indicate RLS policy restrictions or bucket access issues.\")\n\n# Upload to Supabase with enhanced error handling\nprint(\"\\n--- Attempting upload to Supabase ---\")\nupload_successful = False\n\ntry:\n    # Try uploading to the analysis-results bucket in the JSON folder\n    json_filename = f\"JSON/{file_name_without_ext}_structural_data.json\"\n    print(f\"Attempting to upload: {json_filename}\")\n    \n    upload_response = supabase.storage.from_(\"analysis-results\").upload(\n        json_filename, \n        combined_json_data.encode('utf-8'),\n        {\"content-type\": \"application/json\"}\n    )\n    \n    if upload_response:\n        print(f\"âœ… Combined structural data uploaded to Supabase as '{json_filename}'\")\n        upload_successful = True\n        \n        # Generate a public URL for the uploaded file\n        try:\n            public_url = supabase.storage.from_(\"analysis-results\").get_public_url(json_filename)\n            print(f\"ðŸ“„ Public URL: {public_url}\")\n        except Exception as url_err:\n            print(f\"âš ï¸ Warning: Could not generate public URL: {url_err}\")\n        \n        # List updated JSON files after upload\n        try:\n            print(\"\\n--- Updated JSON files list after upload ---\")\n            updated_files = supabase.storage.from_(\"analysis-results\").list(\"JSON\")\n            if updated_files:\n                print(f\"Total JSON files now: {len(updated_files)}\")\n                for file_info in updated_files:\n                    print(f\"  - {file_info.get('name', 'Unknown')} (Size: {file_info.get('metadata', {}).get('size', 'Unknown')} bytes)\")\n        except Exception as list_err:\n            print(f\"âš ï¸ Warning: Could not list files after upload: {list_err}\")\n    else:\n        print(\"âŒ Upload response was empty or failed\")\n        \nexcept Exception as e:\n    print(f\"âŒ Error uploading to analysis-results bucket: {e}\")\n    \n    # Try alternative: Upload to the models bucket (which we know works)\n    if \"403\" in str(e) or \"Unauthorized\" in str(e) or \"row-level security policy\" in str(e):\n        print(\"\\n--- RLS Policy Error: Trying alternative upload to models bucket ---\")\n        try:\n            # Try uploading to the models bucket instead\n            alt_json_filename = f\"{folder_path}/structural_analysis/{file_name_without_ext}_structural_data.json\"\n            print(f\"Attempting alternative upload: {alt_json_filename}\")\n            \n            alt_upload_response = supabase.storage.from_(\"models\").upload(\n                alt_json_filename, \n                combined_json_data.encode('utf-8'),\n                {\"content-type\": \"application/json\"}\n            )\n            \n            if alt_upload_response:\n                print(f\"âœ… Successfully uploaded to models bucket as '{alt_json_filename}'\")\n                upload_successful = True\n                \n                # Generate a public URL for the uploaded file\n                try:\n                    alt_public_url = supabase.storage.from_(\"models\").get_public_url(alt_json_filename)\n                    print(f\"ðŸ“„ Public URL: {alt_public_url}\")\n                except Exception as alt_url_err:\n                    print(f\"âš ï¸ Warning: Could not generate public URL for alternative upload: {alt_url_err}\")\n            else:\n                print(\"âŒ Alternative upload also failed\")\n                \n        except Exception as alt_err:\n            print(f\"âŒ Alternative upload to models bucket also failed: {alt_err}\")\n\n# Fallback: save locally if upload failed\nif not upload_successful:\n    print(\"\\n--- Saving locally as fallback ---\")\n    local_json_path = os.path.join(CSV_SAVE_PATH, f\"{file_name_without_ext}_structural_data.json\")\n    try:\n        with open(local_json_path, 'w') as f:\n            f.write(combined_json_data)\n        print(f\"ðŸ’¾ Saved combined structural data locally: {local_json_path}\")\n    except Exception as local_err:\n        print(f\"âŒ Error saving locally: {local_err}\")\n        \n        # Ultimate fallback: save in the current directory\n        try:\n            current_dir_path = os.path.join(os.getcwd(), f\"{file_name_without_ext}_structural_data.json\")\n            with open(current_dir_path, 'w') as f:\n                f.write(combined_json_data)\n            print(f\"ðŸ’¾ Saved in current directory as ultimate fallback: {current_dir_path}\")\n        except Exception as current_dir_err:\n            print(f\"âŒ Critical error: Cannot save file anywhere: {current_dir_err}\")\nelse:\n    print(\"âœ… Upload completed successfully!\")\n\nprint(\"\\nðŸŽ‰ Processing complete!\")\n\n",
  "language": "python",
  "imports": [
    "rhino3dm"
  ],
  "has_docstring": true
}