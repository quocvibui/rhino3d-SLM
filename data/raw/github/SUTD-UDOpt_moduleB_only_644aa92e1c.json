{
  "source_url": "https://github.com/SUTD-UDOpt/moduleB_only/blob/7b53aad95ea9931b83e1615102e0f07f465bd5c0/static/py/Utility.py",
  "repo": "SUTD-UDOpt/moduleB_only",
  "repo_stars": 0,
  "repo_description": null,
  "license": "unknown",
  "filepath": "static/py/Utility.py",
  "instruction": "Utility",
  "code": "import sys\nsys.path.append(\".\")\nimport os\nimport copy\nimport rhino3dm\nimport datetime\nimport pymoo\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport pickle\nimport matplotlib as mpl\nimport base64\nimport compute_rhino3d.Util\nimport requests\nimport json\nimport psutil\nfrom collections.abc import MutableMapping\n\nfrom pymoo.algorithms.moo.nsga2 import NSGA2\nfrom pymoo.core.problem import ElementwiseProblem\nfrom pymoo.optimize import minimize\nfrom pymoo.factory import get_sampling, get_crossover, get_mutation, get_termination, get_problem, get_performance_indicator\nfrom pymoo.util.misc import stack\nfrom pymoo.indicators.hv import Hypervolume\nfrom pymoo.util.running_metric import RunningMetric\n\ndef Parameterbounds(X,P):\n    lower = [round(float(x*(1-(P/2))),3) for x in X]\n    upper = [round(float(x*(1+(P/2))),3) for x in X]\n\n    if upper[3]  > 1.0:\n        lower[3] -= upper[3] - 1.0\n        upper[3] = 1.0\n\n    if upper[4] > 0.5:\n        lower[4] -= upper[4] - 0.5\n        upper[4] = 0.5\n\n    return lower,upper\n\ndef Remap(OldValue, OldMin, OldMax):\n    OldRange = (OldMax - OldMin)  \n    NewRange = 1 \n    return (((OldValue - OldMin) * NewRange) / OldRange)\n\ndef func_pf(flatten=True, **kwargs):\n        f1_a = np.linspace(0.1**2, 0.4**2, 100)\n        f2_a = (np.sqrt(f1_a) - 1)**2\n\n        f1_b = np.linspace(0.6**2, 0.9**2, 100)\n        f2_b = (np.sqrt(f1_b) - 1)**2\n\n        a, b = np.column_stack([f1_a, f2_a]), np.column_stack([f1_b, f2_b])\n        return stack(a, b, flatten=flatten)\n\ndef func_ps(flatten=True, **kwargs):\n        x1_a = np.linspace(0.1, 0.4, 50)\n        x1_b = np.linspace(0.6, 0.9, 50)\n        x2 = np.zeros(50)\n\n        a, b = np.column_stack([x1_a, x2]), np.column_stack([x1_b, x2])\n        return stack(a,b, flatten=flatten)\n\ndef PrintResult(res, export = False):\n\n    try:\n        dfs = []\n        for generation,data in enumerate(res.history):\n\n            if len(np.array(data.result().X).shape) == 1:\n                arr = np.concatenate(([data.result().X],[data.result().F]),axis=1)\n            else:\n                arr = np.concatenate((data.result().X,data.result().F),axis=1)\n\n            labels = ['Length/mm','Depth/mm','Height/mm','Span/%','Leg/%','Gap/%','CapMat','LegMat','Waste/%','VolRatio/%','TotalDisp/cm','EmbCarbon/KgCO2e']\n            df = pd.DataFrame(arr, columns = labels)\n            df.reset_index(level=0, inplace=True)\n            df.insert(0, 'Gen', [generation]*df.shape[0])\n            dfs.append(df)\n        df = pd.concat(dfs,ignore_index=True)\n\n        time = str(datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")) \n        with pd.option_context('display.max_rows',10):\n            print('\\n\\n')\n            pd.set_option('display.max_columns', None)\n            print(df)\n        if export:\n            pd.DataFrame(df).to_csv(time + \".csv\")\n\n    except Exception as e: \n        print('error: {0}'.format(e))\n    return\n\ndef ExtractHistory(res):\n    hist = res.history\n    n_evals = []             # corresponding number of function evaluations\\\n    hist_F = []              # the objective space values in each generation\n    hist_cv = []             # constraint violation in each generation\n    hist_cv_avg = []         # average constraint violation in the whole population\n\n    for algo in hist:\n\n        # store the number of function evaluations\n        n_evals.append(algo.evaluator.n_eval)\n\n        # retrieve the optimum from the algorithm\n        opt = algo.opt\n\n        # store the least contraint violation and the average in each population\n        hist_cv.append(opt.get(\"CV\").min())\n        hist_cv_avg.append(algo.pop.get(\"CV\").mean())\n\n        # filter out only the feasible and append and objective space values\n        feas = np.where(opt.get(\"feasible\"))[0]\n        hist_F.append(opt.get(\"F\")[feas])\n    return n_evals,hist_F,hist_cv,hist_cv_avg\n\ndef PlotHypervolume(res, prefix):\n    n_evals,hist_F,hist_cv,hist_cv_avg = ExtractHistory(res)\n    X, F = res.opt.get(\"X\", \"F\")\n    approx_ideal = F.min(axis=0)\n    approx_nadir = F.max(axis=0)\n\n    metric = Hypervolume(ref_point= np.array([10]*len(F[0])),\n                     norm_ref_point=False,\n                     zero_to_one=True,\n                     ideal=approx_ideal,\n                     nadir=approx_nadir)\n    hv = [metric.do(_F) for _F in hist_F]\n\n    plt.figure(figsize=(7, 5))\n    plt.plot(n_evals, hv,  color='black', lw=0.7, label=\"Avg. CV of Pop\")\n    plt.scatter(n_evals, hv,  facecolor=\"none\", edgecolor='black', marker=\"p\")\n    plt.title(\"Convergence\")\n    plt.xlabel(\"Function Evaluations\")\n    plt.ylabel(\"Hypervolume\")\n    plt.savefig(prefix + ' Hypervolume convergence.png')\n    return\n\ndef SaveResult(res, filename = None):\n    if filename:\n        fn = filename + '.pickle'\n    else:\n        fn = '{0}.pickle',format(str(datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")))\n    try:\n        with open(fn,'wb') as f:\n            pickle.dump(res,f,protocol=pickle.HIGHEST_PROTOCOL)\n    except Exception as ex:\n        print(\"Error during pickling object (Possibly unsupported):\", ex)\n \ndef LoadResult(filename):\n\n    try:\n        with open(filename + '.pickle', \"rb\") as f:\n            return pickle.load(f)\n    except Exception as ex:\n        print(\"Error during unpickling object (Possibly unsupported):\", ex)\n\ndef ReadResults(res):\n    arr = []\n    for generation, population in enumerate(res.history):\n        for index, individual in enumerate(res.history[generation].pop):\n            arr.append([generation, index] + list(individual.X) + list(individual.F) + list(individual.G))\n    df = pd.DataFrame(arr)\n    return df\n\ndef PlotPairwise(df, basemap, fig_size, label_size, tick_size, marker_size):\n    variable_count = len(df.columns)\n    fig, axs = plt.subplots(variable_count, variable_count, figsize=(fig_size,fig_size))\n\n    fig.tight_layout()\n    plt.subplots_adjust(left = 0.1, right = 0.9, top = 0.9, bottom = 0.1)\n\n    offdiagonal_indicies = [[i,j] for i in range(variable_count-1) for j in range(variable_count-1-i)]\n    gen_count = int(max(list(basemap)) + 1)\n    norm = mpl.colors.Normalize(vmin = 0, vmax = gen_count)\n\n    for i in range(variable_count):\n        for j in range(variable_count):\n            ax_y = variable_count-j-1\n            ax_x = i\n            ax = axs[ax_y,ax_x]\n            ax.set_xlabel(df.columns[ax_x], fontsize = label_size)\n            ax.set_ylabel(df.columns[ax_y], fontsize = label_size)\n\n\n            if [i,j] in offdiagonal_indicies:\n\n                ax.scatter(x = df.iloc[:,ax_x], y = df.iloc[:,ax_y], c = basemap , cmap = 'Blues', s=marker_size)\n                \n                ax.tick_params(axis='both', labelsize = tick_size)\n\n\n                if ax_x != 0:\n                    ax.sharey(axs[0,ax_y])\n                    ax.set_ylabel(None)\n                if ax_y != variable_count-1:\n                    ax.sharex(axs[ax_x,variable_count-1])\n                    ax.set_xlabel(None)\n            \n            elif ax_x==ax_y:\n                \n                \n                hist_arr = [[] for i in range(gen_count)]\n                for row in range(df.shape[0]):\n                    hist_arr[int(basemap.iloc[row])].append(df.iloc[row,ax_x])\n                \n                \n                colors = [mpl.cm.Blues(norm(x)) for x in range(gen_count)]\n                \n                ax.hist(hist_arr, color = colors, histtype = 'barstacked')\n                ax.tick_params(axis='both', labelsize = tick_size)\n                ax.set_ylabel(None)\n                if ax_y != variable_count-1:\n                    ax.set_xlabel(None)\n\n                if ax_y == variable_count-1:\n                    ax.set_xlabel(df.columns[ax_x])\n            \n            else:\n                ax.axis('off')\n\n    plt.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap='Blues'), label = 'Generation', ax = axs[0,variable_count-1], orientation = 'horizontal')\n    return fig,axs\n\ndef PlotSingleObj(df):\n    fig, ax = plt.subplots(1,1)\n    ax.scatter(y = df['WeightedObj'], x = df['Gen'], s = 10)\n    ax.set_xlabel('Generation')\n    ax.set_ylabel('WeightedObj')\n    return fig, ax\n\n\ndef PlotRunningMetric(res, prefix):\n    filename = \"{0}\\{1} Running Metric.png\".format(os.getcwd(),prefix)\n    g_count = len(res.history)\n    running = RunningMetric(delta_gen=int(g_count/10),\n                            n_plots=10,\n                            only_if_n_plots=True,\n                            key_press=False,\n                            do_show=False,\n                            filename = filename\n                            )\n\n    for algorithm in res.history:\n        running.notify(algorithm)\n\ndef NormalizeDF(df):    \n    arr = df.to_numpy().transpose().copy()\n    #print(\"Normalizing DF using ranges:\")\n    for i,row in enumerate(arr):\n        #print(row.min(), row.max())\n        arr[i] = np.interp(row, (row.min(), row.max()), (0,1))\n    return pd.DataFrame(arr.transpose(),columns = df.columns)\n\n\ndef DeNormalizeDF(df,refdf):\n    arr = df.to_numpy().transpose()\n    refarr = refdf.to_numpy().transpose()\n    print(\"DeNormalizing DF using ranges:\")\n    for i in range(len(arr)):\n        arr[i] = np.interp(arr[i], (0,1), (refarr[i].min(),refarr[i].max()))\n        print(refarr[i].min(),refarr[i].max())\n    return pd.DataFrame(arr.transpose(),columns = df.columns)\n\ndef EvaluateGrasshopper(filename,X,XKeys,XTypes,CloudCompute = False,ComputeURL=None,ComputeKey=None,Authtoken=None):\n    if CloudCompute:\n        compute_rhino3d.Util.url = ComputeURL\n        compute_rhino3d.Util.apiKey = ComputeKey\n\n    else:\n        compute_rhino3d.Util.authToken = Authtoken\n        compute_rhino3d.Util.url = \"http://localhost:8081/\"\n    \n    gh_data = open(filename, mode=\"r\", encoding=\"utf-8-sig\").read()\n    decoded = base64.b64encode(gh_data.encode(\"utf-8\")).decode(\"utf-8\")\n    myjson = {\n        \"algo\": decoded,\n        \"pointer\": None,\n        \"values\":[]\n        }\n    for x,xkey,xtype in zip(X,XKeys,XTypes):\n        myjson['values'].append({\n            \"ParamName\": \"RH_IN:\" + xkey,\n                \"InnerTree\": {\n                    \"{ 0; }\": [\n                        {\n                            \"type\": xtype,\n                            \"data\": x\n                        }\n                    ]\n                }\n        })\n\n    response = requests.post(\n        url=compute_rhino3d.Util.url + \"grasshopper\", \n        headers={\"RhinoComputeKey\" : ComputeKey},\n        json=myjson\n        )\n    return json.loads(response.content.decode(\"utf-8\"))\n\ndef is_pareto_efficient(costs):\n    \"\"\"\n    Find the pareto-efficient points\n    :param costs: An (n_points, n_costs) array\n    :return: A (n_points, ) boolean array, indicating whether each point is Pareto efficient\n    \"\"\"\n    is_efficient = np.ones(costs.shape[0], dtype = bool)\n    for i, c in enumerate(costs):\n        if is_efficient[i]:\n            is_efficient[is_efficient] = np.any(costs[is_efficient]<c, axis=1)  # Keep any point with a lower cost\n            is_efficient[i] = True  # And keep self\n    return is_efficient\n\ndef GetParetoDF(df, FKeys, MinParetoSolutions):\n    PD = 0\n    pdf = pd.DataFrame([])\n    copydf = df.copy()\n    while pdf.shape[0] < MinParetoSolutions:\n        boollist = is_pareto_efficient(copydf[FKeys].to_numpy())\n        ndf = copydf.loc[boollist,:]\n        ndf['PD'] = PD\n        pdf = pd.concat([pdf,ndf],axis=0)\n        copydf = copydf.loc[[not elem for elem in boollist],:]\n        PD += 1\n    return pdf \n\nclass MemoryStore():\n    def __init__(self):\n        self.info = {'deltaTime':[]}\n        self.time = datetime.datetime.now()\n\n    def SaveState(self):\n        currTime = datetime.datetime.now()\n        deltatime = currTime - self.time\n        self.info['deltaTime'].append(deltatime.seconds + deltatime.microseconds/1000000)\n        self.time = currTime\n        for key,value in  zip(['total','available','percent','used','free'],list(psutil.virtual_memory())):\n            if key not in self.info:\n                self.info[key] = []\n            self.info[key].append(value)\n        for key,value in  zip(['rss','vms','num_page_faults','peak_wset','wset','peak_paged_pool','paged_pool','peak_nonpaged_pool','nonpaged_pool','pagefile','peak_pagefile','private'],list(psutil.Process(os.getpid()).memory_info())):\n            if key not in self.info:\n                self.info[key] = []\n            self.info[key].append(value)\n    \n    def plot(self):\n        n = int(len(self.info)**0.5)\n        fig,axs = plt.subplots(nrows = n, ncols = n + 1, figsize = ((n+1)*3,n*3))\n        axs = np.array(axs).flatten()\n\n        for ax,(key,values) in zip(axs,self.info.items()):\n            ax.plot(\n                list(range(len(values))),\n                values\n            )\n            ax.set_title(key)\n        plt.tight_layout()\n        return fig,axs\n\ndef GetArchiveKey(X):\n    return '-'.join(['%.5f' % x for x in X])\n",
  "language": "python",
  "imports": [
    "rhino3dm"
  ],
  "has_docstring": false
}