{
  "source_url": "https://github.com/adapt-ai-team/ADAPT_AI/blob/5de8b42c4f4d4ad35cbd8e79588fc6e869398bc4/spz_analysis2/osm_fetch_convert_to_3dm.py",
  "repo": "adapt-ai-team/ADAPT_AI",
  "repo_stars": 0,
  "repo_description": null,
  "license": "unknown",
  "filepath": "spz_analysis2/osm_fetch_convert_to_3dm.py",
  "instruction": "Osm fetch convert to 3dm",
  "code": "import requests\nimport pyproj\nimport shapely.geometry as sg\nimport trimesh\nimport rhino3dm\nimport numpy as np\nimport os\nfrom flask import Flask, request, jsonify\nimport sys\nimport tempfile\nfrom dotenv import load_dotenv\nfrom supabase import create_client, Client\n\n# Load environment variables\nload_dotenv()\nSUPABASE_URL = os.environ[\"SUPABASE_URL\"]\nSUPABASE_SERVICE_ROLE_KEY = os.environ[\"SUPABASE_SERVICE_ROLE_KEY\"]\nsupabase: Client = create_client(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY)\n\n# Bucket names\nLATLON_BUCKET = \"location\"\nINPUT_BUCKET = \"2d-to-3d\"\nMERGED_BUCKET = \"context-merged\"  # Use the existing bucket name\n\n# üìç Constants\nRADIUS = 250  # Max area in meters for OSM data fetch\n\n# Initialize Flask app\napp = Flask(__name__)\n\n@app.route('/save_latlon', methods=['POST'])\ndef save_latlon():\n    try:\n        data = request.json\n        user_id = data.get(\"user_id\")\n        project_id = data.get(\"project_id\")\n        latitude = data.get(\"latitude\")\n        longitude = data.get(\"longitude\")\n\n        if not all([user_id, project_id, latitude, longitude]):\n            return jsonify({\"status\": \"error\", \"message\": \"Missing required parameters\"}), 400\n\n        path = f\"{user_id}/{project_id}/latlon.txt\"\n        content = f\"{latitude},{longitude}\"\n        \n        try:\n            # Remove existing file if present\n            supabase.storage.from_(LATLON_BUCKET).remove([path])\n        except Exception:\n            pass  # Safe to ignore if file doesn't exist\n            \n        # Upload new coordinates\n        supabase.storage.from_(LATLON_BUCKET).upload(\n            path,\n            content.encode(),\n            file_options={\"content-type\": \"text/plain\"}\n        )\n        \n        return jsonify({\"status\": \"success\", \"message\": \"Lat/lon saved successfully\"}), 200\n    except Exception as e:\n        return jsonify({\"status\": \"error\", \"message\": str(e)}), 500\n\ndef fetch_latlon_from_supabase(user_id: str, project_id: str):\n    path = f\"{user_id}/{project_id}/latlon.txt\"\n    try:\n        data = supabase.storage.from_(LATLON_BUCKET).download(path)\n        lat, lon = map(float, data.decode(\"utf-8\").strip().split(\",\"))\n        print(f\"üìç Fetched lat/lon: {lat}, {lon}\")\n        return lat, lon\n    except Exception as e:\n        raise Exception(f\"‚ùå Failed to fetch latlon.txt: {e}\")\n\ndef latlon_to_utm(lat, lon):\n    proj = pyproj.Proj(proj=\"utm\", zone=int((lon + 180) / 6) + 1, ellps=\"WGS84\")\n    x, y = proj(lon, lat)\n    return x, y\n\ndef compute_bottom_center(bounds, up_axis=2):\n    \"\"\"\n    Compute the bottom center point of the bounding box based on the up axis.\n    \n    For up_axis = 2 (Z up): returns \n        [ (min_x + max_x)/2, (min_y + max_y)/2, min_z ]\n    \n    For up_axis = 1 (Y up): returns \n        [ (min_x + max_x)/2, min_y, (min_z + max_z)/2 ]\n    \n    For up_axis = 0 (X up, rarely used): returns \n        [ min_x, (min_y + max_y)/2, (min_z + max_z)/2 ]\n    \"\"\"\n    bmin = bounds[0]\n    bmax = bounds[1]\n    pivot = np.empty(3)\n    for i in range(3):\n        if i == up_axis:\n            pivot[i] = bmin[i]\n        else:\n            pivot[i] = (bmin[i] + bmax[i]) / 2\n    return pivot\n\ndef fetch_model_from_supabase(user_id: str, project_id: str):\n    path = f\"{user_id}/{project_id}/model.glb\"\n    url = supabase.storage.from_(INPUT_BUCKET).get_public_url(path)\n    response = requests.get(url)\n    if response.status_code != 200:\n        raise Exception(f\"‚ùå Failed to download model.glb from: {url}\")\n    with tempfile.NamedTemporaryFile(delete=False, suffix=\".glb\") as tmp:\n        tmp.write(response.content)\n        tmp.flush()\n        return trimesh.load(tmp.name)\n\ndef upload_fixed_model(scene: trimesh.Scene, user_id: str, project_id: str):\n    path = f\"{user_id}/{project_id}/model_fixed.glb\"\n    with tempfile.NamedTemporaryFile(delete=False, suffix=\".glb\") as tmp_file:\n        scene.export(tmp_file.name)\n        tmp_file.flush()\n        try:\n            # Remove existing file if present\n            supabase.storage.from_(INPUT_BUCKET).remove([path])\n        except Exception:\n            pass  # Safe to ignore if file doesn't exist\n\n        # Upload the new model\n        supabase.storage.from_(INPUT_BUCKET).upload(\n            path,\n            tmp_file.name,\n            file_options={\"content-type\": \"model/gltf-binary\"}\n        )\n        print(f\"‚úÖ Uploaded model_fixed.glb to {INPUT_BUCKET}/{path}\")\n\ndef upload_merged_model(scene: trimesh.Scene, user_id: str, project_id: str):\n    path = f\"{user_id}/{project_id}/merged_model.glb\"\n    with tempfile.NamedTemporaryFile(delete=False, suffix=\".glb\") as tmp_file:\n        scene.export(tmp_file.name)\n        tmp_file.flush()\n        \n        try:\n            # Remove existing file if present\n            supabase.storage.from_(MERGED_BUCKET).remove([path])\n        except Exception:\n            pass  # Safe to ignore if file doesn't exist\n        \n        # Upload the new model\n        supabase.storage.from_(MERGED_BUCKET).upload(\n            path,\n            tmp_file.name,\n            file_options={\"content-type\": \"model/gltf-binary\"}\n        )\n        print(f\"‚úÖ Uploaded merged_model.glb to {MERGED_BUCKET}/{path}\")\n\ndef export_scene_to_3dm_and_upload(scene, user_id: str, project_id: str):\n    print(f\"üîÑ Starting 3DM export process...\")\n    model = rhino3dm.File3dm()\n    \n    # Process each mesh with better error handling\n    for mesh_name, geometry in scene.geometry.items():\n        try:\n            if not isinstance(geometry, trimesh.Trimesh):\n                print(f\"‚ö†Ô∏è Skipping non-mesh object: {mesh_name}\")\n                continue\n                \n            print(f\"üìä Processing mesh '{mesh_name}': {len(geometry.vertices)} vertices, {len(geometry.faces)} faces\")\n            \n            # Skip excessively large meshes\n            if len(geometry.vertices) > 1000000:  # 1 million vertex limit\n                print(f\"‚ö†Ô∏è Mesh too large, skipping: {len(geometry.vertices)} vertices\")\n                continue\n                \n            # Create Rhino mesh\n            rhino_mesh = rhino3dm.Mesh()\n            \n            # Add vertices\n            for v in geometry.vertices:\n                rhino_mesh.Vertices.Add(float(v[0]), float(v[1]), float(v[2]))\n            \n            # Add faces with bounds checking\n            face_count = 0\n            for face in geometry.faces:\n                max_index = max(face)\n                if max_index >= len(geometry.vertices):\n                    continue  # Skip faces with out-of-bounds indices\n                \n                if len(face) == 3:\n                    rhino_mesh.Faces.AddFace(int(face[0]), int(face[1]), int(face[2]))\n                    face_count += 1\n                elif len(face) == 4:\n                    rhino_mesh.Faces.AddFace(int(face[0]), int(face[1]), int(face[2]), int(face[3]))\n                    face_count += 1\n            \n            print(f\"‚úÖ Added {face_count} faces to Rhino mesh\")\n            \n            # Compute normals and add mesh to model\n            rhino_mesh.Normals.ComputeNormals()\n            rhino_mesh.Compact()\n            model.Objects.AddMesh(rhino_mesh)\n            \n        except Exception as e:\n            print(f\"‚ùå Error processing mesh '{mesh_name}': {e}\")\n    \n    # Use a more reliable approach for temporary file\n    try:\n        temp_dir = tempfile.mkdtemp()\n        temp_path = os.path.join(temp_dir, \"merged_model.3dm\")\n        \n        print(f\"üìù Writing 3DM file to: {temp_path}\")\n        model.Write(temp_path, 5)\n        \n        # Verify file was created and has content\n        if not os.path.exists(temp_path):\n            raise Exception(\"File was not created\")\n            \n        file_size = os.path.getsize(temp_path)\n        print(f\"üìã 3DM file size: {file_size / 1024:.1f} KB\")\n        \n        if file_size == 0:\n            raise Exception(\"File is empty (0 bytes)\")\n        \n        # Upload with explicit file open\n        path = f\"{user_id}/{project_id}/merged_model.3dm\"\n        print(f\"üì§ Uploading to solar-radiation/{path}\")\n        \n        try:\n            # Remove existing file if present\n            supabase.storage.from_(\"solar-radiation\").remove([path])\n            print(f\"üóëÔ∏è Removed existing file (if any)\")\n        except Exception as e:\n            print(f\"‚ö†Ô∏è Note: Could not remove existing file: {e}\")\n        \n        # Upload with explicit file opening\n        with open(temp_path, \"rb\") as f:\n            result = supabase.storage.from_(\"solar-radiation\").upload(\n                path,\n                f,\n                file_options={\"content-type\": \"application/octet-stream\"}\n            )\n            print(f\"‚úÖ Upload successful: {result}\")\n        \n        # Get public URL to verify\n        url = supabase.storage.from_(\"solar-radiation\").get_public_url(path)\n        print(f\"üåê Public URL: {url}\")\n        \n    except Exception as e:\n        print(f\"‚ùå Error during 3DM export or upload: {str(e)}\")\n        raise\n    \n    finally:\n        # Clean up temporary directory\n        try:\n            import shutil\n            shutil.rmtree(temp_dir, ignore_errors=True)\n        except:\n            pass  # At least we tried\n\ndef process_example_image(user_id: str, project_id: str, osm_scene: trimesh.Scene, up_axis=2):\n    \"\"\"Process example_image.glb to match OSM model position and scale.\"\"\"\n    scene = fetch_model_from_supabase(user_id, project_id)\n    new_scene = trimesh.Scene()\n    \n    # Get OSM bounds and center\n    osm_bounds = osm_scene.bounds\n    osm_center = np.mean(osm_bounds, axis=0)\n    osm_lowest_z = osm_bounds[0][2]\n    print(f\"üìç OSM Model Bounds: {osm_bounds}\")\n    print(f\"üìç OSM Center Point: {osm_center}\")\n    print(f\"üìç OSM Lowest Z: {osm_lowest_z}\")\n\n    for mesh_name, geometry in scene.geometry.items():\n        print(f\"Processing mesh: {mesh_name}\")\n\n        # 1. Get original bounds and center\n        bounds = geometry.bounds\n        current_center = np.mean(bounds, axis=0)\n        current_lowest_z = bounds[0][2]\n        print(f\"üìè Original Bounds: {bounds}\")\n        print(f\"üìè Original Center: {current_center}\")\n        \n        # 2. Scale by 1000 uniformly from current position\n        scale_factor = 1000\n        \n        # Use bottom center as pivot point\n        pivot_point = np.array([\n            current_center[0],  # X center\n            bounds[0][1],      # Y bottom\n            current_center[2]   # Z center\n        ])\n        \n        # Create scaling transformation\n        to_origin = np.eye(4)\n        to_origin[:3, 3] = -pivot_point\n        \n        scale = np.eye(4)\n        scale[:3, :3] *= scale_factor\n        \n        from_origin = np.eye(4)\n        from_origin[:3, 3] = pivot_point\n        \n        # Apply scaling transformation\n        transform = from_origin @ scale @ to_origin\n        geometry.apply_transform(transform)\n        \n        # 3. Get updated bounds after scaling\n        updated_bounds = geometry.bounds\n        updated_center = np.mean(updated_bounds, axis=0)\n        \n        # 4. Calculate translation to match centers on XZ plane\n        translation = np.array([\n            osm_center[0] - updated_center[0],  # X alignment\n            0,                                  # Keep Y unchanged\n            osm_center[2] - updated_center[2]   # Z alignment\n        ])\n        \n        # 5. Apply translation\n        T_translate = np.eye(4)\n        T_translate[:3, 3] = translation\n        geometry.apply_transform(T_translate)\n        \n        # 6. Verify final position\n        final_bounds = geometry.bounds\n        final_center = np.mean(final_bounds, axis=0)\n        print(f\"üìè Final Bounds: {final_bounds}\")\n        print(f\"üìè Final Center: {final_center}\")\n        \n        # Verify center alignment on XZ plane\n        center_difference_xz = np.array([\n            abs(final_center[0] - osm_center[0]),\n            abs(final_center[2] - osm_center[2])\n        ])\n        print(f\"üìè XZ Center Difference: {center_difference_xz}\")\n        \n        if np.any(center_difference_xz > 0.001):\n            print(f\"‚ö†Ô∏è Warning: XZ center alignment offset detected: {center_difference_xz}\")\n\n        new_scene.add_geometry(geometry)\n\n    # Upload the fixed model\n    upload_fixed_model(new_scene, user_id, project_id)\n    return new_scene\n\ndef fetch_osm_data(lat, lon, radius):\n    query = f\"\"\"\n    [out:json];\n    (\n        way(around:{radius},{lat},{lon})[building];\n    );\n    out body;\n    >;\n    out skel qt;\n    \"\"\"\n    api_endpoints = [\n        \"https://overpass-api.de/api/interpreter\",\n        \"https://lz4.overpass-api.de/api/interpreter\",\n        \"https://z.overpass-api.de/api/interpreter\",\n        \"https://maps.mail.ru/osm/tools/overpass/api/interpreter\"\n    ]\n    for endpoint in api_endpoints:\n        try:\n            print(f\"üîç Trying to connect to Overpass API at: {endpoint}\")\n            response = requests.get(endpoint, params={\"data\": query}, timeout=30)\n            if response.status_code == 200:\n                print(f\"‚úÖ Successfully fetched OSM data from {endpoint}\")\n                return response.json()\n            else:\n                print(f\"‚ö†Ô∏è API returned status code {response.status_code}\")\n        except requests.exceptions.RequestException as e:\n            print(f\"‚ö†Ô∏è Connection error with {endpoint}: {e}\")\n    print(\"‚ùå All Overpass API endpoints failed.\")\n    return None\n\ndef parse_osm_data(osm_data):\n    buildings = []\n    nodes = {}\n    \n    # Collect all nodes\n    for element in osm_data[\"elements\"]:\n        if element[\"type\"] == \"node\":\n            x, y = latlon_to_utm(element[\"lat\"], element[\"lon\"])\n            nodes[element[\"id\"]] = (x, y)  # Store original UTM coordinates\n    \n    # Process buildings with original coordinates\n    for element in osm_data[\"elements\"]:\n        if element[\"type\"] == \"way\" and \"building\" in element.get(\"tags\", {}):\n            try:\n                height = float(element[\"tags\"].get(\"height\", 10))\n                footprint = []\n                for node_id in element[\"nodes\"]:\n                    if node_id in nodes:\n                        footprint.append(nodes[node_id])\n                if len(footprint) >= 3:\n                    buildings.append({\"footprint\": footprint, \"height\": height})\n            except Exception as e:\n                print(f\"Error processing building: {e}\")\n                continue\n                \n    return buildings\n\ndef create_3d_model(buildings, scale_factor=1.0):\n    scene = trimesh.Scene()\n    \n    # Create 3D models with centered coordinates\n    for building in buildings:\n        footprint = building[\"footprint\"]\n        height = building[\"height\"]\n        polygon = sg.Polygon(footprint)\n        try:\n            extruded = trimesh.creation.extrude_polygon(polygon, height, engine=\"triangle\")\n        except ValueError:\n            try:\n                extruded = trimesh.creation.extrude_polygon(polygon, height, engine=\"earcut\")\n            except ValueError:\n                continue\n                \n        # Apply transformations but maintain centering\n        rot_z = trimesh.transformations.rotation_matrix(-np.pi/2, [0, 0, 1])\n        extruded.apply_transform(rot_z)\n        rot_x = trimesh.transformations.rotation_matrix(-np.pi/2, [1, 0, 0])\n        extruded.apply_transform(rot_x)\n        extruded.apply_scale(scale_factor)\n        \n        scene.add_geometry(extruded)\n    \n    return scene\n\ndef run_osm_pipeline(user_id: str, project_id: str):\n    # Check if required buckets exist\n    try:\n        buckets = [bucket[\"name\"] for bucket in supabase.storage.list_buckets()]\n        required_buckets = [LATLON_BUCKET, INPUT_BUCKET, MERGED_BUCKET, \"solar-radiation\"]\n        \n        for bucket in required_buckets:\n            if bucket not in buckets:\n                print(f\"‚ö†Ô∏è Warning: Bucket '{bucket}' doesn't exist in Supabase. Some operations may fail.\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Warning: Couldn't verify buckets: {e}\")\n    \n    lat, lon = fetch_latlon_from_supabase(user_id, project_id)\n    print(f\"üîπ Using Lat/Lon: {lat}, {lon}\")\n    \n    REF_X, REF_Y = latlon_to_utm(lat, lon)\n    REF_Z = 0\n    print(f\"üîπ Reference UTM Coordinates: ({REF_X}, {REF_Y})\")\n    \n    osm_data = fetch_osm_data(lat, lon, RADIUS)\n    if osm_data:\n        print(f\"üìç Found {len(osm_data['elements'])} OSM elements\")\n        buildings = parse_osm_data(osm_data)\n        print(f\"üè¢ Parsed {len(buildings)} buildings\")\n        if not buildings:\n            print(\"‚ö†Ô∏è No buildings found. Try increasing RADIUS or verifying coordinates.\")\n            return\n            \n        scene_osm = create_3d_model(buildings, scale_factor=10)\n        if len(scene_osm.geometry) == 0:\n            print(\"‚ùå No valid geometry created from buildings\")\n            return\n            \n        # Process and align the input model\n        scene_fixed = process_example_image(user_id, project_id, scene_osm)\n        \n        # Create merged scene\n        merged_scene = trimesh.Scene()\n        for g in scene_osm.geometry.values():\n            merged_scene.add_geometry(g)\n        for g in scene_fixed.geometry.values():\n            merged_scene.add_geometry(g)\n            \n        # Upload merged model in GLB format\n        upload_merged_model(merged_scene, user_id, project_id)\n        \n        # Export and upload Rhino 3DM format\n        export_scene_to_3dm_and_upload(merged_scene, user_id, project_id)\n        \n    else:\n        print(\"‚ùå Failed to fetch OSM data\")\n        return\n\nif __name__ == \"__main__\":\n    # Run as a CLI tool or as a service\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--user_id\", required=True)\n    parser.add_argument(\"--project_id\", required=True)\n    args = parser.parse_args()\n    \n    # Start Flask in a background thread\n    from threading import Thread\n    flask_thread = Thread(target=lambda: app.run(port=5000, debug=False))\n    flask_thread.daemon = True\n    flask_thread.start()\n    \n    # Run the pipeline with arguments\n    run_osm_pipeline(args.user_id, args.project_id)",
  "language": "python",
  "imports": [
    "rhino3dm"
  ],
  "has_docstring": false
}