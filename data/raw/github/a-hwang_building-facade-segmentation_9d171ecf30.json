{
  "source_url": "https://github.com/a-hwang/building-facade-segmentation/blob/fdf532443d1c83547218bfbfce8fece3ab848c21/visualization.py",
  "repo": "a-hwang/building-facade-segmentation",
  "repo_stars": 0,
  "repo_description": null,
  "license": "unknown",
  "filepath": "visualization.py",
  "instruction": "Visualization",
  "code": "import os\nimport torch\nimport torchvision\nimport matplotlib.pyplot as plt\nimport cv2\nimport numpy as np\nimport rhino3dm\nimport json  # for exporting raw data\nfrom utils import get_unique_filepath  # reuse helper from utils.py\nfrom evaluation import evaluate_model_bbox, evaluate_model_masks  # update imports\n\ndef extract_edges_from_masks(masks):\n    \"\"\"\n    Extracts simplified edge contours from each mask using Douglas-Peucker algorithm.\n    \"\"\"\n    edges = []\n    for mask in masks:\n        mask = np.squeeze(mask)\n        mask = (mask > 0.5).astype(np.uint8) * 255\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        for contour in contours:\n            epsilon = 0.02 * cv2.arcLength(contour, True)\n            approx = cv2.approxPolyDP(contour, epsilon, True)\n            points = [(point[0][0], point[0][1]) for point in approx]\n            edges.append(points)\n    return edges\n\ndef export_to_rhino(masks, base_name):\n    \"\"\"\n    Exports detected edges as curves in a Rhino 3DM file.\n    \"\"\"\n    edges = extract_edges_from_masks(masks)\n    model3dm = rhino3dm.File3dm()\n    for edge_points in edges:\n        points3d = [rhino3dm.Point3d(x, y, 0) for x, y in edge_points]\n        if len(points3d) < 2:\n            continue\n        curve = rhino3dm.Curve.CreateControlPointCurve(points3d, 1)\n        if curve is not None and curve.IsValid:\n            model3dm.Objects.AddCurve(curve)\n    output_folder = \"output\"\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n    out_path = os.path.join(output_folder, f\"detected_edges_{base_name}.3dm\")\n    out_path = get_unique_filepath(out_path)\n    model3dm.Write(out_path, 6)\n    print(\"Exported Rhino file to\", out_path)\n\ndef visualize_predictions(image, prediction, categories, output_folder, base_name):\n    \"\"\"\n    Visualizes predictions by drawing bounding boxes and masks (if available) on the image.\n    \"\"\"\n    image_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n    preds = prediction[0]\n    boxes = preds['boxes'].cpu().numpy()\n    labels = preds['labels'].cpu().numpy()\n    scores = preds['scores'].cpu().numpy()\n    masks = preds['masks'].cpu().numpy() if 'masks' in preds else None\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n    bbox_image = image_cv.copy()\n    for box, label, score in zip(boxes, labels, scores):\n        if score < 0.5:\n            continue\n        x1, y1, x2, y2 = box.astype(int)\n        cv2.rectangle(bbox_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n        category_name = categories.get(int(label), \"unknown\")\n        cv2.putText(bbox_image, f\"{category_name} {score:.2f}\", (x1, max(y1-10,0)),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 2)\n    bbox_path = os.path.join(output_folder, f\"{base_name}_with_boxes.jpg\")\n    bbox_path = get_unique_filepath(bbox_path)\n    cv2.imwrite(bbox_path, bbox_image)\n    if masks is not None:\n        mask_image = image_cv.copy()\n        np.random.seed(42)\n        colors = np.random.randint(0,255, size=(len(categories), 3), dtype=np.uint8)\n        for mask, label, score in zip(masks, labels, scores):\n            if score < 0.5:\n                continue\n            color = colors[label].tolist()\n            binary_mask = (mask[0] > 0.5).astype(np.uint8)\n            colored_mask = np.zeros_like(image_cv)\n            colored_mask[binary_mask > 0] = color\n            cv2.addWeighted(colored_mask, 0.5, mask_image, 1, 0, mask_image)\n            moments = cv2.moments(binary_mask)\n            if moments[\"m00\"] != 0:\n                cx = int(moments[\"m10\"]/moments[\"m00\"])\n                cy = int(moments[\"m01\"]/moments[\"m00\"])\n                category_name = categories.get(int(label), \"unknown\")\n                cv2.putText(mask_image, f\"{category_name} {score:.2f}\", (cx,cy),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255),2)\n        mask_path = os.path.join(output_folder, f\"{base_name}_with_masks.jpg\")\n        mask_path = get_unique_filepath(mask_path)\n        cv2.imwrite(mask_path, mask_image)\n        print(f\"Saved visualizations to {output_folder}\")\n    else:\n        print(\"No masks predicted; skipping mask visualization.\")\n    return masks\n\ndef graph_model_performance(epochs, model, test_loader, device, checkpoints_folder, model_name, evaluation_mode, target_class, train_coco):\n    \"\"\"\n    Evaluate at each epoch using the specified evaluation mode (bbox or mask)\n    and graph performance. If target_class is provided:\n    - If target_class != \"all\": produce one graph (with 4 lines for metrics) for that class.\n    - If target_class == \"all\": for each metric, produce a graph with each class line and export 4 JSON files.\n    \"\"\"\n    if target_class and target_class != \"all\":\n        # Single class graph: one graph with all 4 metrics.\n        epochs_list, iou_list, precision_list, recall_list, f1_list = [], [], [], [], []\n        raw_data = {}\n        for epoch in sorted(epochs):\n            ckpt_path = os.path.join(checkpoints_folder, f\"model_checkpoint_epoch_{epoch}.pth\")\n            if not os.path.exists(ckpt_path):\n                print(f\"Epoch {epoch}: checkpoint missing, skipping.\")\n                continue\n            checkpoint = torch.load(ckpt_path, map_location=device)\n            model.load_state_dict(checkpoint)\n            if evaluation_mode == \"bbox\":\n                from evaluation import evaluate_model_bbox_by_class\n                metrics = evaluate_model_bbox_by_class(model, test_loader, device, target_class, train_coco=train_coco)\n            else:\n                from evaluation import evaluate_model_masks_by_class\n                metrics = evaluate_model_masks_by_class(model, test_loader, device, target_class, train_coco=train_coco)\n            if not metrics:\n                continue\n            epochs_list.append(epoch)\n            iou_list.append(metrics['IoU'])\n            precision_list.append(metrics['Precision'])\n            recall_list.append(metrics['Recall'])\n            f1_list.append(metrics['F1-Score'])\n            raw_data[epoch] = metrics\n        if not epochs_list:\n            print(\"No valid epochs found for class evaluation.\")\n            return\n        plt.figure(figsize=(10, 6))\n        plt.plot(epochs_list, iou_list, label='IoU', marker='o')\n        plt.plot(epochs_list, precision_list, label='Precision', marker='o')\n        plt.plot(epochs_list, recall_list, label='Recall', marker='o')\n        plt.plot(epochs_list, f1_list, label='F1-Score', marker='o')\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Metric Value\")\n        plt.title(f\"{model_name}_class_{target_class}_{evaluation_mode}_graph\")\n        plt.legend()\n        plt.grid(True)\n        graph_path = os.path.join(\"output\", f\"{model_name}_class_{target_class}_{evaluation_mode}_graph.png\")\n        plt.savefig(graph_path)\n        plt.close()\n        print(f\"Class-specific performance graph saved to {graph_path}\")\n        # Export raw data for this class.\n        raw_json_path = os.path.join(\"output\", f\"{model_name}_raw_metrics_class_{target_class}_{evaluation_mode}_graph.json\")\n        try:\n            from utils import get_unique_filepath\n            raw_json_path = get_unique_filepath(raw_json_path)\n        except ImportError:\n            pass\n        with open(raw_json_path, 'w') as f:\n            json.dump(raw_data, f, indent=4)\n        print(f\"Raw metrics for class saved to {raw_json_path}\")\n    elif target_class == \"all\":\n        # All classes: For each metric create separate graphs.\n        class_list = [\"facade\", \"balcony-fence\", \"car\", \"fence\", \"non-building-infrastructure\", \"shop\", \"street\", \"traffic-infrastructure\", \"vegetation\", \"window\"]\n        raw_data = {metric: {} for metric in [\"IoU\", \"Precision\", \"Recall\", \"F1-Score\"]}\n        data_by_class = {metric: {cls: ([], []) for cls in class_list} for metric in [\"IoU\", \"Precision\", \"Recall\", \"F1-Score\"]}\n        for epoch in sorted(epochs):\n            ckpt_path = os.path.join(checkpoints_folder, f\"model_checkpoint_epoch_{epoch}.pth\")\n            if not os.path.exists(ckpt_path):\n                continue\n            checkpoint = torch.load(ckpt_path, map_location=device)\n            model.load_state_dict(checkpoint)\n            for cls in class_list:\n                if evaluation_mode == \"bbox\":\n                    from evaluation import evaluate_model_bbox_by_class\n                    metrics = evaluate_model_bbox_by_class(model, test_loader, device, cls, train_coco=train_coco)\n                else:\n                    from evaluation import evaluate_model_masks_by_class\n                    metrics = evaluate_model_masks_by_class(model, test_loader, device, cls, train_coco=train_coco)\n                if not metrics:\n                    continue\n                for metric in [\"IoU\", \"Precision\", \"Recall\", \"F1-Score\"]:\n                    data_by_class[metric][cls][0].append(epoch)\n                    data_by_class[metric][cls][1].append(metrics[metric])\n                    raw_data[metric].setdefault(epoch, {})[cls] = metrics[metric]\n        for metric in [\"IoU\", \"Precision\", \"Recall\", \"F1-Score\"]:\n            plt.figure(figsize=(10, 6))\n            for cls in class_list:\n                epochs_list, values = data_by_class[metric][cls]\n                if epochs_list:\n                    plt.plot(epochs_list, values, label=cls, marker='o')\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(metric)\n            plt.title(f\"{model_name}_all_classes_{metric}_graph\")\n            plt.legend()\n            plt.grid(True)\n            graph_path = os.path.join(\"output\", f\"{model_name}_all_classes_{metric}_graph.png\")\n            plt.savefig(graph_path)\n            plt.close()\n            print(f\"All-classes {metric} graph saved to {graph_path}\")\n            raw_json_path = os.path.join(\"output\", f\"{model_name}_raw_metrics_all_classes_{metric}_graph.json\")\n            try:\n                from utils import get_unique_filepath\n                raw_json_path = get_unique_filepath(raw_json_path)\n            except ImportError:\n                pass\n            with open(raw_json_path, 'w') as f:\n                json.dump(raw_data[metric], f, indent=4)\n            print(f\"Raw metrics for {metric} saved to {raw_json_path}\")\n    else:\n        # No target_class specified: fallback to aggregated graph.\n        epochs_list = []\n        iou_list = []\n        precision_list = []\n        recall_list = []\n        f1_list = []\n        raw_data = {}  # for exporting raw metrics\n\n        for epoch in sorted(epochs):\n            ckpt_path = os.path.join(checkpoints_folder, f\"model_checkpoint_epoch_{epoch}.pth\")\n            if not os.path.exists(ckpt_path):\n                print(f\"Checkpoint for epoch {epoch} is missing. Skipping.\")\n                continue\n            checkpoint = torch.load(ckpt_path, map_location=device)\n            model.load_state_dict(checkpoint)\n            if evaluation_mode == \"bbox\":\n                metrics = evaluate_model_bbox(model, test_loader, device)\n            else:\n                metrics = evaluate_model_masks(model, test_loader, device)\n            print(f\"Epoch {epoch} - IoU: {metrics['IoU']:.4f}, Precision: {metrics['Precision']:.4f}, \"\n                  f\"Recall: {metrics['Recall']:.4f}, F1: {metrics['F1-Score']:.4f}\")\n            epochs_list.append(epoch)\n            iou_list.append(metrics['IoU'])\n            precision_list.append(metrics['Precision'])\n            recall_list.append(metrics['Recall'])\n            f1_list.append(metrics['F1-Score'])\n            raw_data[epoch] = metrics\n\n        if not epochs_list:\n            print(\"No valid checkpoints found. Exiting graph generation.\")\n            return\n\n        largest_epoch = max(epochs_list)\n        plt.figure(figsize=(10, 6))\n        plt.plot(epochs_list, iou_list, label='IoU', marker='o')\n        plt.plot(epochs_list, precision_list, label='Precision', marker='o')\n        plt.plot(epochs_list, recall_list, label='Recall', marker='o')\n        plt.plot(epochs_list, f1_list, label='F1-Score', marker='o')\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Metric Value\")\n        plt.title(f\"{model_name}_epoch_{largest_epoch}_{evaluation_mode}_graph\")\n        plt.legend()\n        plt.grid(True)\n        graph_path = os.path.join(\"output\", f\"{model_name}_epoch_{largest_epoch}_{evaluation_mode}_graph.png\")\n        plt.savefig(graph_path)\n        plt.close()\n        print(f\"Performance graph saved to {graph_path}\")\n        \n        # Export raw data as JSON.\n        raw_json_path = os.path.join(\"output\", f\"{model_name}_raw_metrics_{evaluation_mode}_graph.json\")\n        # Use get_unique_filepath helper if available\n        try:\n            from utils import get_unique_filepath\n            raw_json_path = get_unique_filepath(raw_json_path)\n        except ImportError:\n            pass\n        with open(raw_json_path, 'w') as f:\n            json.dump(raw_data, f, indent=4)\n        print(f\"Raw metrics data saved to {raw_json_path}\")",
  "language": "python",
  "imports": [
    "rhino3dm"
  ],
  "has_docstring": false
}