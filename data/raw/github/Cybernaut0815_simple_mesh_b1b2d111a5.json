{
  "source_url": "https://github.com/Cybernaut0815/simple_mesh/blob/e97ba57769cb9e19fbaeea11733ff711f6d489fe/RhinoScripts/back_project_mask.py",
  "repo": "Cybernaut0815/simple_mesh",
  "repo_stars": 0,
  "repo_description": null,
  "license": "Apache-2.0",
  "filepath": "RhinoScripts/back_project_mask.py",
  "instruction": "Back project mask",
  "code": "import rhinoscriptsyntax as rs\nimport scriptcontext as sc\nimport Rhino\nimport System\nimport os\nimport json\nimport numpy as np\nimport math\nfrom System.Drawing import Bitmap, Color, Rectangle\nfrom System.Drawing.Imaging import PixelFormat\nfrom System.Runtime.InteropServices import Marshal\n\ndef extract_mask_data(mask_bitmap):\n    \"\"\"Extract binary mask data\"\"\"\n    rect = Rectangle(0, 0, mask_bitmap.Width, mask_bitmap.Height)\n    bitmap_data = mask_bitmap.LockBits(rect, System.Drawing.Imaging.ImageLockMode.ReadOnly, \n                                      PixelFormat.Format32bppArgb)\n    \n    try:\n        # Get pixel data\n        byte_count = bitmap_data.Stride * bitmap_data.Height\n        byte_array = System.Array.CreateInstance(System.Byte, byte_count)\n        Marshal.Copy(bitmap_data.Scan0, byte_array, 0, byte_count)\n        \n        # Convert to numpy array\n        labels = np.zeros((mask_bitmap.Height, mask_bitmap.Width), dtype=np.uint8)\n        \n        # Process each pixel - just check if it's white (non-black)\n        for y in range(mask_bitmap.Height):\n            for x in range(mask_bitmap.Width):\n                offset = y * bitmap_data.Stride + x * 4\n                b = byte_array[offset]\n                g = byte_array[offset + 1]\n                r = byte_array[offset + 2]\n                \n                # If any channel has significant value, consider it white\n                if r > 127 or g > 127 or b > 127:\n                    labels[y, x] = 255\n    \n    finally:\n        mask_bitmap.UnlockBits(bitmap_data)\n    \n    # Print stats about the mask\n    nonzero = np.count_nonzero(labels)\n    total = labels.size\n    percent = (nonzero / total) * 100\n    print(f\"Mask stats: {nonzero} white pixels out of {total} ({percent:.2f}%)\")\n    \n    return labels\n\n\ndef back_project_using_rays(mask_filepath, depth_filepath, view_info, mesh_guid, \n                           debug_rays=False, fov_adjustment=1.0, max_sample_points=60,\n                           ray_length_multiplier=10.0):\n    \"\"\"Back-project mask points using ray casting with limited sample points\"\"\"\n    # Load mask image\n    mask_bitmap = Bitmap(mask_filepath)\n    mask_width = mask_bitmap.Width\n    mask_height = mask_bitmap.Height\n    \n    # Get camera information\n    camera_position = np.array(view_info[\"camera_position\"])\n    camera_target = np.array(view_info[\"target\"])\n    \n    # Calculate camera basis vectors\n    camera_forward = camera_target - camera_position\n    camera_distance = np.linalg.norm(camera_forward)\n    camera_forward = camera_forward / camera_distance  # Normalize\n    \n    # Create camera coordinate system\n    world_up = np.array([0, 0, 1])  # Z-up in Rhino\n    camera_right = np.cross(camera_forward, world_up)\n    if np.linalg.norm(camera_right) < 0.001:\n        world_up = np.array([0, 1, 0])  # Use Y as alternative\n        camera_right = np.cross(camera_forward, world_up)\n    camera_right = camera_right / np.linalg.norm(camera_right)  # Normalize\n    camera_up = np.cross(camera_right, camera_forward)\n    camera_up = camera_up / np.linalg.norm(camera_up)  # Normalize\n    \n    # Get viewport info\n    is_perspective = view_info[\"projection_info\"][\"is_perspective\"]\n    print(f\"Projection type: {'Perspective' if is_perspective else 'Orthographic'}\")\n    \n    # Extract mask data (white pixels only)\n    mask_data = extract_mask_data(mask_bitmap)\n    \n    # Get mesh for ray casting\n    mesh = rs.coercemesh(mesh_guid)\n    if not mesh:\n        print(\"Error: Could not coerce mesh for ray casting\")\n        return np.array([])\n    \n    # Get mesh bounding box for debugging\n    mesh_bbox = mesh.GetBoundingBox(True)\n    bbox_min = mesh_bbox.Min\n    bbox_max = mesh_bbox.Max\n    print(f\"Mesh bounding box: Min({bbox_min.X:.2f}, {bbox_min.Y:.2f}, {bbox_min.Z:.2f}), Max({bbox_max.X:.2f}, {bbox_max.Y:.2f}, {bbox_max.Z:.2f})\")\n    \n    # Create a layer for debug rays if needed\n    debug_layer = \"DebugRays\"\n    if debug_rays and not rs.IsLayer(debug_layer):\n        rs.AddLayer(debug_layer)\n        rs.LayerColor(debug_layer, rs.CreateColor(0, 255, 255))  # Cyan\n    \n    # Get FOV information from the enhanced data\n    if \"fov\" in view_info and isinstance(view_info[\"fov\"], dict):\n        v_fov = view_info[\"fov\"][\"vertical\"]\n        h_fov = view_info[\"fov\"][\"horizontal\"]\n    else:\n        # Fallback to estimated FOV\n        aspect_ratio = float(mask_width) / float(mask_height)\n        v_fov = 60.0  # Default if not available\n        h_fov = v_fov * aspect_ratio\n    \n    # Apply FOV adjustment\n    v_fov *= fov_adjustment\n    h_fov *= fov_adjustment\n    print(f\"Using FOV: vertical={v_fov:.2f}, horizontal={h_fov:.2f}\")\n    \n    # Calculate ray offsets based on adjusted FOV\n    h_tan = math.tan(math.radians(h_fov / 2))\n    v_tan = math.tan(math.radians(v_fov / 2))\n    \n    # Find white pixels for sampling\n    white_pixels = []\n    for y in range(mask_height):\n        for x in range(mask_width):\n            if mask_data[y, x] > 0:  # If white\n                white_pixels.append((x, y))\n    \n    # Limit to max_sample_points by selecting evenly distributed samples\n    sample_pixels = []\n    if len(white_pixels) > 0:\n        if len(white_pixels) <= max_sample_points:\n            sample_pixels = white_pixels\n        else:\n            # Select evenly spaced samples\n            step = len(white_pixels) // max_sample_points\n            sample_pixels = [white_pixels[i] for i in range(0, len(white_pixels), step)]\n            # Ensure we don't exceed max_sample_points\n            sample_pixels = sample_pixels[:max_sample_points]\n    \n    print(f\"Selected {len(sample_pixels)} sample points out of {len(white_pixels)} white pixels\")\n    \n    # Calculate max ray length for testing (longer rays)\n    mesh_size = max(\n        bbox_max.X - bbox_min.X,\n        bbox_max.Y - bbox_min.Y,\n        bbox_max.Z - bbox_min.Z\n    )\n    max_ray_length = max(camera_distance, mesh_size) * ray_length_multiplier\n    \n    # Keep track of previous layer\n    previous_layer = rs.CurrentLayer()\n    hits = 0\n    misses = 0\n    \n    # Array to store results\n    points_3d = []\n    \n    # Process only the selected sample points\n    for x, y in sample_pixels:\n        # Calculate normalized device coordinates (-1 to 1)\n        ndc_x = (x / float(mask_width - 1)) * 2.0 - 1.0\n        ndc_y = 1.0 - (y / float(mask_height - 1)) * 2.0  # Y flipped\n        \n        # Calculate ray direction based on projection type\n        if is_perspective:\n            # For perspective projection with adjusted FOV\n            ray_dir_x = ndc_x * h_tan\n            ray_dir_y = ndc_y * v_tan\n            ray_dir_z = 1.0  # Forward\n            \n            # Create normalized ray direction in view space\n            ray_dir_view = np.array([ray_dir_x, ray_dir_y, ray_dir_z])\n            ray_dir_view = ray_dir_view / np.linalg.norm(ray_dir_view)\n            \n            # Transform to world space using camera basis vectors\n            ray_dir_world = (\n                ray_dir_view[0] * camera_right + \n                ray_dir_view[1] * camera_up + \n                ray_dir_view[2] * camera_forward\n            )\n        else:\n            # For orthographic projection with adjusted FOV\n            view_width = camera_distance * h_tan * 2\n            view_height = camera_distance * v_tan * 2\n            \n            # Calculate offset from center\n            offset_x = ndc_x * (view_width / 2)\n            offset_y = ndc_y * (view_height / 2)\n            \n            # Ray starts at camera plane, offset by x,y\n            ray_start = (\n                camera_position + \n                camera_right * offset_x + \n                camera_up * offset_y\n            )\n            \n            # Direction is parallel to forward\n            ray_dir_world = camera_forward\n        \n        # Create Rhino ray for intersection\n        ray_origin = Rhino.Geometry.Point3d(\n            camera_position[0], camera_position[1], camera_position[2]\n        )\n        ray_dir = Rhino.Geometry.Vector3d(\n            ray_dir_world[0], ray_dir_world[1], ray_dir_world[2]\n        )\n        \n        # Use a much longer ray for testing - KEY FIX #1\n        ray_dir.Unitize()  # Ensure unit vector\n        ray = Rhino.Geometry.Ray3d(ray_origin, ray_dir)\n        \n        # Create a line going through the mesh for intersection - KEY FIX #2\n        # Sometimes mesh.ray doesn't work well, but Line.MeshIntersection does\n        line_end = ray_origin + ray_dir * max_ray_length\n        line = Rhino.Geometry.Line(ray_origin, line_end)\n        \n        # Try multiple intersection methods - KEY FIX #3\n        intersection_found = False\n        hit_point = None\n        \n        # Method 1: MeshRay\n        intersection_param = Rhino.Geometry.Intersect.Intersection.MeshRay(mesh, ray)\n        if intersection_param >= 0:\n            intersection_found = True\n            hit_point = ray_origin + ray_dir * intersection_param\n        \n        # Method 2: Line-Mesh intersection if ray casting failed\n        if not intersection_found:\n            intersections = Rhino.Geometry.Intersect.Intersection.MeshLine(mesh, line)\n            if intersections and len(intersections) > 0:\n                intersection_found = True\n                # Use the closest intersection point\n                closest_t = float('inf')\n                for intersection in intersections:\n                    if intersection.ParameterA < closest_t:\n                        closest_t = intersection.ParameterA\n                        hit_point = line.PointAt(closest_t)\n        \n        if intersection_found:  # Hit\n            hits += 1\n            \n            # Convert to numpy array for storage\n            hit_point_array = np.array([hit_point.X, hit_point.Y, hit_point.Z])\n            points_3d.append(hit_point_array)\n            \n            # Draw debug rays for sample points\n            if debug_rays:\n                rs.CurrentLayer(debug_layer)\n                line = rs.AddLine(\n                    [ray_origin.X, ray_origin.Y, ray_origin.Z],\n                    [hit_point.X, hit_point.Y, hit_point.Z]\n                )\n                rs.ObjectColor(line, rs.CreateColor(0, 255, 0))  # Green for hits\n        else:\n            misses += 1\n            \n            # Draw debug rays for misses\n            if debug_rays:\n                rs.CurrentLayer(debug_layer)\n                # Draw a limited length ray for misses\n                end_point = ray_origin + ray_dir * max_ray_length\n                line = rs.AddLine(\n                    [ray_origin.X, ray_origin.Y, ray_origin.Z],\n                    [end_point.X, end_point.Y, end_point.Z]\n                )\n                rs.ObjectColor(line, rs.CreateColor(255, 0, 0))  # Red for misses\n    \n    # Restore previous layer\n    rs.CurrentLayer(previous_layer)\n    \n    print(f\"Ray statistics: {hits} hits, {misses} misses\")\n    \n    # If no hits, draw camera and target points for debugging\n    if hits == 0 and debug_rays:\n        rs.CurrentLayer(debug_layer)\n        # Draw camera position\n        cam_point = rs.AddPoint(camera_position[0], camera_position[1], camera_position[2])\n        rs.ObjectColor(cam_point, rs.CreateColor(255, 255, 0))  # Yellow for camera\n        \n        # Draw target position\n        target_point = rs.AddPoint(camera_target[0], camera_target[1], camera_target[2])\n        rs.ObjectColor(target_point, rs.CreateColor(0, 255, 255))  # Cyan for target\n        \n        # Draw a line from camera to target\n        cam_line = rs.AddLine(\n            [camera_position[0], camera_position[1], camera_position[2]],\n            [camera_target[0], camera_target[1], camera_target[2]]\n        )\n        rs.ObjectColor(cam_line, rs.CreateColor(255, 0, 255))  # Magenta for camera line\n    \n    return np.array(points_3d)\n\n\ndef process_all_views(output_folder=\"C:\\\\Screenshots\", mesh_guid=None, \n                     debug_rays=False, fov_adjustment=1.0, max_sample_points=60):\n    \"\"\"Process all views in the camera data\"\"\"\n    # Load camera data\n    camera_data_path = os.path.join(output_folder, \"camera_data.json\")\n    if not os.path.exists(camera_data_path):\n        print(f\"Camera data not found at {camera_data_path}\")\n        return\n        \n    with open(camera_data_path, 'r') as json_file:\n        camera_data = json.load(json_file)\n    \n    # Process each view\n    results = {}\n    for view_info in camera_data[\"views\"]:\n        target_filename = view_info[\"filename\"]\n        \n        # Create a layer for visualization\n        base_layer = f\"RayCast_{target_filename.replace('.jpeg', '')}\"\n        if not rs.IsLayer(base_layer):\n            rs.AddLayer(base_layer)\n            rs.LayerColor(base_layer, rs.CreateColor(255, 0, 0))  # Red for visibility\n        else:\n            # Clear existing objects from this layer\n            rs.CurrentLayer(base_layer)\n            objects = rs.ObjectsByLayer(base_layer)\n            if objects:\n                rs.DeleteObjects(objects)\n        \n        # Get mask filename\n        mask_filename = target_filename.replace(\".jpeg\", \"_combined_mask.png\")\n        mask_filepath = os.path.join(output_folder, mask_filename)\n        \n        # Check if mask exists\n        if not os.path.exists(mask_filepath):\n            print(f\"Mask not found: {mask_filepath}, trying alternatives...\")\n            \n            # Try alternative mask names\n            alt_mask = target_filename.replace(\".jpeg\", \"_mask.png\")\n            alt_mask_path = os.path.join(output_folder, alt_mask)\n            if os.path.exists(alt_mask_path):\n                mask_filepath = alt_mask_path\n                print(f\"Using alternative mask: {alt_mask}\")\n            else:\n                print(f\"No suitable mask found for {target_filename}, skipping this view\")\n                continue\n        \n        # Get depth data path\n        depth_filepath = os.path.join(output_folder, view_info[\"depth_npy_filename\"])\n        if not os.path.exists(depth_filepath):\n            print(f\"Depth data not found: {depth_filepath}, checking alternatives...\")\n            \n            # Try to find alternative depth data\n            alt_depth = target_filename.replace(\".jpeg\", \"_depth.npy\")\n            alt_depth_path = os.path.join(output_folder, alt_depth)\n            if os.path.exists(alt_depth_path):\n                depth_filepath = alt_depth_path\n                print(f\"Using alternative depth data: {alt_depth}\")\n            else:\n                print(f\"No depth data found for {target_filename}, continuing without depth\")\n        \n        print(f\"\\n--- Processing view: {target_filename} ---\")\n        \n        # Back-project points using ray casting with limited sample points\n        points_3d = back_project_using_rays(\n            mask_filepath, depth_filepath, view_info, mesh_guid, \n            debug_rays, fov_adjustment, max_sample_points, \n            ray_length_multiplier=10.0  # Use longer rays to ensure they hit the mesh\n        )\n        \n        if len(points_3d) == 0:\n            print(f\"No valid points were back-projected for view {target_filename}\")\n            continue\n        \n        # Add points to Rhino\n        previous_layer = rs.CurrentLayer()\n        rs.CurrentLayer(base_layer)\n        \n        points_objs = []\n        for point in points_3d:\n            point_obj = rs.AddPoint([point[0], point[1], point[2]])\n            points_objs.append(point_obj)\n        \n        # Create a group for the points\n        if points_objs:\n            group_name = f\"BackProjected_{target_filename.replace('.jpeg', '')}\"\n            rs.AddGroup(group_name)\n            rs.AddObjectsToGroup(points_objs, group_name)\n        \n        # Restore previous layer\n        rs.CurrentLayer(previous_layer)\n        \n        print(f\"Created {len(points_3d)} points for view {target_filename}\")\n        \n        # Store the results\n        results[target_filename] = points_3d\n    \n    return results\n\n\ndef apply_colors_to_mesh(mesh_guid, points_3d, color=None):\n    \"\"\"Apply a color to mesh faces that are closest to the back-projected points\"\"\"\n    if color is None:\n        color = rs.CreateColor(255, 0, 0)  # Default to red\n    \n    # Get the mesh\n    mesh = rs.coercemesh(mesh_guid)\n    if not mesh:\n        print(f\"Could not coerce {mesh_guid} to a mesh.\")\n        return None\n    \n    # Dictionary to store face indices and their assigned labels\n    face_labels = {}\n    \n    # For each 3D point\n    for i, point in enumerate(points_3d):\n        # Find closest point on mesh\n        result = rs.MeshClosestPoint(mesh_guid, [point[0], point[1], point[2]])\n        \n        if result:\n            closest_point, face_index = result\n            \n            # If we found a valid face\n            if face_index >= 0:\n                # Calculate distance\n                distance = rs.Distance(point, closest_point)\n                \n                # If this face doesn't have a label yet, or we're closer than previous point\n                if face_index not in face_labels:\n                    face_labels[face_index] = (255, distance)\n                else:\n                    if distance < face_labels[face_index][1]:\n                        face_labels[face_index] = (255, distance)\n    \n    # Clean up the dictionary to only contain the label, not the distance\n    labeled_mesh = {face: label_data[0] for face, label_data in face_labels.items()}\n    \n    # Apply colors to the mesh\n    try:\n        # Create a new colored mesh\n        colored_mesh = Rhino.Geometry.Mesh()\n        colored_mesh.CopyFrom(mesh)\n        \n        # Enable vertex colors\n        colored_mesh.VertexColors.CreateMonotoneMesh(Color.LightGray)\n        \n        # Default gray for unlabeled faces\n        default_color = Color.LightGray\n        \n        # Apply colors by face\n        for face_idx in range(colored_mesh.Faces.Count):\n            # Determine color for this face\n            if face_idx in labeled_mesh:\n                # Use the specified color\n                face_color = Color.FromArgb(color.R, color.G, color.B)\n            else:\n                face_color = default_color\n            \n            # Get the face\n            face = colored_mesh.Faces[face_idx]\n            \n            # Set vertex colors for this face\n            colored_mesh.VertexColors[face.A] = face_color\n            colored_mesh.VertexColors[face.B] = face_color\n            colored_mesh.VertexColors[face.C] = face_color\n            if face.IsQuad:\n                colored_mesh.VertexColors[face.D] = face_color\n        \n        # Add the colored mesh to the document\n        new_mesh_id = sc.doc.Objects.AddMesh(colored_mesh)\n        \n        if new_mesh_id != System.Guid.Empty:\n            print(f\"Successfully created colored mesh with ID: {new_mesh_id}\")\n            sc.doc.Views.Redraw()\n        else:\n            print(\"Failed to add colored mesh to document\")\n        \n    except Exception as e:\n        print(f\"Error applying colors to mesh: {str(e)}\")\n    \n    return labeled_mesh\n\n\n# Main execution\nif __name__ == \"__main__\":\n    # Fixed output folder path\n    output_folder = \"C:\\\\Screenshots\"\n    \n    # Verify folder exists\n    if not os.path.exists(output_folder):\n        print(f\"Warning: Output folder {output_folder} does not exist!\")\n        output_folder = rs.GetString(\"Path to data folder with masks and depth data\", output_folder)\n        if not os.path.exists(output_folder):\n            print(\"Invalid folder path\")\n            exit()\n    \n    # Ask user to select a mesh for ray casting\n    mesh_guid = rs.GetObject(\"Select mesh for ray casting\", 32)  # 32 is mesh filter\n    if not mesh_guid:\n        print(\"Error: No mesh selected. Ray casting requires a mesh.\")\n        exit()\n    \n    # Ask for FOV adjustment factor\n    fov_adjustment = rs.GetReal(\"FOV adjustment factor (1.0=original, 0.5=narrower, 2.0=wider)\", 1.0, 0.1, 5.0)\n    if fov_adjustment is None:\n        fov_adjustment = 1.0  # Use original FOV by default\n    \n    # Ask if user wants to debug with visual rays (warning: this can create a lot of geometry)\n    debug_rays = rs.MessageBox(\"Draw debug rays to visualize sample rays?\\nWarning: This can create a lot of geometry with multiple views.\", 4) == 6  # 6 is Yes\n    \n    # Ask for number of sample points\n    max_sample_points = rs.GetInteger(\"Maximum number of sample points per view\", 100, 10, 1000)\n    if max_sample_points is None:\n        max_sample_points = 100  # Default\n    \n    # Process all views\n    print(f\"Processing all views with maximum {max_sample_points} sample points per view\")\n    results = process_all_views(\n        output_folder=output_folder,\n        mesh_guid=mesh_guid,\n        debug_rays=debug_rays,\n        fov_adjustment=fov_adjustment,\n        max_sample_points=max_sample_points\n    )\n    \n    # Ask if user wants to color the mesh using points from all views\n    if results and len(results) > 0:\n        if rs.MessageBox(\"Apply colors to mesh based on back-projected points from all views?\", 4) == 6:\n            # Combine all points from all views\n            all_points = np.vstack([points for points in results.values()])\n            apply_colors_to_mesh(mesh_guid, all_points)",
  "language": "python",
  "imports": [
    "Rhino",
    "Rhino.Geometry",
    "rhinoscriptsyntax",
    "scriptcontext"
  ],
  "has_docstring": false
}