{
  "source_url": "https://github.com/OHBA-analysis/osl-ephys/blob/d4bf635399a4556a8e8dd6d8c2007aa475f731db/osl_ephys/source_recon/batch.py",
  "repo": "OHBA-analysis/osl-ephys",
  "repo_stars": 63,
  "repo_description": "Electrophysiological Data Analysis Toolbox",
  "license": "NOASSERTION",
  "filepath": "osl_ephys/source_recon/batch.py",
  "instruction": "Batch processing for source reconstruction.",
  "code": "\"\"\"Batch processing for source reconstruction.\n\n\"\"\"\n\n# Authors: Chetan Gohil <chetan.gohil@psych.ox.ac.uk>\n#          Mats van Es <mats.vanes@psych.ox.ac.uk>           \n\nimport os\nimport sys\nimport traceback\nimport pprint\nimport inspect\n\nfrom copy import deepcopy\nfrom time import localtime, strftime\nfrom functools import partial\nfrom dask.distributed import Variable, Queue\n\nimport numpy as np\nimport yaml\nimport mne\n\nfrom . import rhino, wrappers, freesurfer_utils\nfrom ..report import src_report\nfrom ..utils import logger as osl_logger\nfrom ..utils import validate_outdir, find_run_id, parallel\nfrom ..utils.misc import set_random_seed\n\nimport logging\nlogger = logging.getLogger(__name__)\n\n\ndef load_config(config):\n    \"\"\"Load config.\n\n    Parameters\n    ----------\n    config : str or dict\n        Path to yaml file or str to convert to dict or a dict.\n\n    Returns\n    -------\n    config : dict\n        Source reconstruction config.\n    \"\"\"\n    if type(config) not in [str, dict]:\n        raise ValueError(\"config must be a str or dict, got {}.\".format(type(config)))\n\n    if isinstance(config, str):\n        try:\n            # See if we have a filepath\n            with open(config, \"r\") as f:\n                config = yaml.load(f, Loader=yaml.FullLoader)\n        except (UnicodeDecodeError, FileNotFoundError, OSError):\n            # We have a str\n            config = yaml.load(config, Loader=yaml.FullLoader)\n\n    # Validation\n    if \"source_recon\" not in config:\n        raise ValueError(\"source_recon must be included in the config.\")\n\n    return config\n\n\ndef find_func(method, extra_funcs):\n    \"\"\"Find a source reconstruction function.\n\n    Parameters\n    ----------\n    method : str\n        Function name.\n    extra_funcs : list of functions\n        Custom functions.\n\n    Returns\n    -------\n    func : function\n        Function to use.\n    \"\"\"\n    func = None\n\n    # Look in custom functions\n    if extra_funcs is not None:\n        func_ind = [idx if (f.__name__ == method) else -1 for idx, f in enumerate(extra_funcs) ]\n        if np.max(func_ind) > -1:\n            func = extra_funcs[np.argmax(func_ind)]\n\n    # Look in osl_ephys.source_recon.wrappers\n    if func is None and hasattr(wrappers, method):\n        func = getattr(wrappers, method)\n\n    return func\n\n\ndef run_src_chain(\n    config,\n    outdir,\n    subject,\n    preproc_file=None,\n    smri_file=None,\n    epoch_file=None,\n    surface_extraction_method='fsl',\n    logsdir=None,\n    reportdir=None,\n    gen_report=True,\n    verbose=\"INFO\",\n    mneverbose=\"WARNING\",\n    extra_funcs=None,\n    random_seed='auto',\n):\n    \"\"\"Source reconstruction.\n\n    Parameters\n    ----------\n    config : str or dict\n        Source reconstruction config.\n    outdir : str\n        Source reconstruction directory.\n    subject : str\n        Subject name.\n    surface_extraction_method : str\n        Can be 'fsl' or 'freesurfer'.\n    preproc_file : str\n        Preprocessed fif file.\n    smri_file : str\n        Structural MRI file.\n    epoch_file : str\n        Epoched fif file.\n    logsdir : str\n        Directory to save log files to.\n    reportdir : str\n        Directory to save report files to.\n    gen_report : bool\n        Should we generate a report?\n    verbose : str\n        Level of verbose.\n    mneverbose : str\n        Level of MNE verbose.\n    extra_funcs : list of functions\n        Custom functions.\n    random_seed : 'auto' (default), int or None\n        Random seed to set. If 'auto', a random seed will be generated. Random seeds are set for both Python and NumPy.\n        If None, no random seed is set.\n\n    Returns\n    -------\n    flag : bool\n        Flag indicating whether source reconstruction was successful.\n    \"\"\"\n    if surface_extraction_method == 'fsl':\n        rhino.fsl_utils.check_fsl()\n    elif surface_extraction_method == 'freesurfer':\n        freesurfer_utils.check_freesurfer()\n\n    # Directories\n    outdir = validate_outdir(outdir)\n    logsdir = validate_outdir(logsdir or outdir / \"logs\")\n    reportdir = validate_outdir(reportdir or outdir / \"src_report\")\n\n    # Use the subject ID for the run ID\n    run_id = subject\n\n    # Generate log filename\n    name_base = \"{run_id}_{ftype}.{fext}\"\n    logbase = os.path.join(logsdir, name_base)\n    logfile = logbase.format(run_id=run_id, ftype=\"src\", fext=\"log\")\n    mne.utils._logging.set_log_file(logfile)\n\n    # Finish setting up loggers\n    osl_logger.set_up(prefix=run_id, log_file=logfile, level=verbose, startup=False)\n    mne.set_log_level(mneverbose)\n    logger = logging.getLogger(__name__)\n    now = strftime(\"%Y-%m-%d %H:%M:%S\", localtime())\n    logger.info(\"{0} : Starting osl-ephys Processing\".format(now))\n    logger.info(\"input : {0}\".format(outdir / subject))\n\n    # Set random seed\n    if random_seed == 'auto':\n        set_random_seed()\n    elif random_seed is None:\n        pass\n    else:\n        set_random_seed(random_seed)\n    \n    # Load config\n    if not isinstance(config, dict):\n        config = load_config(config)\n\n    # Check what files are in the output directory\n    preproc_filename = f\"{outdir}/{subject}/{subject}_preproc-raw.fif\"\n    epoch_filename = f\"{outdir}/{subject}/{subject}_epo.fif\"\n    if os.path.exists(preproc_filename) and os.path.exists(epoch_filename):\n        if preproc_file is None and epoch_file is None:\n            raise ValueError(\n                \"Both preproc and epoch fif files found. \"\n                \"Please pass preproc_file=True or epoch_file=True.\"\n            )\n    elif os.path.exists(preproc_filename):\n        preproc_file = preproc_filename\n    elif os.path.exists(epoch_filename):\n        epoch_file = epoch_filename\n\n    # Validation\n    doing_coreg = (\n        any([\"compute_surfaces\" in method for method in config[\"source_recon\"]]) or\n        any([\"coregister\" in method for method in config[\"source_recon\"]])\n    )\n    if doing_coreg and smri_file is None:\n        raise ValueError(\"smri_file must be passed if we're doing coregistration.\")\n\n    # MAIN BLOCK - Run source reconstruction and catch any exceptions\n    try:\n        for stage in deepcopy(config[\"source_recon\"]):\n            method, userargs = next(iter(stage.items()))\n            func = find_func(method, extra_funcs=extra_funcs)\n            if func is None:\n                avail_funcs = inspect.getmembers(wrappers, inspect.isfunction)\n                avail_names = [name for name, _ in avail_funcs]\n                if method not in avail_names:\n                    raise NotImplementedError(\n                        f\"{method} not available.\\n\"\n                        + \"Please pass via extra_funcs \"\n                        + f\"or use available functions: {avail_names}.\"\n                    )\n            def wrapped_func(**kwargs):\n                sig = inspect.signature(func)\n                args = [param for param in sig.parameters.keys() if param != 'kwargs']\n                defaults = [param.default for param in sig.parameters.values() if param.default is not inspect.Parameter.empty]\n                args_with_defaults = args[-len(defaults):] if defaults else []\n                kwargs_to_pass = {}\n                for a in args:\n                    if a in kwargs:\n                        kwargs_to_pass[a] = kwargs[a]\n                    elif a not in args_with_defaults:\n                        raise ValueError(f\"{a} needs to be passed to {func.__name__}\")\n                return func(**kwargs_to_pass)\n            wrapped_func(\n                outdir=outdir,\n                subject=subject,\n                surface_extraction_method=surface_extraction_method,\n                preproc_file=preproc_file,\n                smri_file=smri_file,\n                epoch_file=epoch_file,\n                reportdir=reportdir,\n                logsdir=logsdir,\n                **userargs,\n            )\n\n    except Exception as e:\n        logger.critical(\"*********************************\")\n        logger.critical(\"* SOURCE RECONSTRUCTION FAILED! *\")\n        logger.critical(\"*********************************\")\n\n        ex_type, ex_value, ex_traceback = sys.exc_info()\n        logger.error(\"{0} : {1}\".format(method, func))\n        logger.error(ex_type)\n        logger.error(ex_value)\n        logger.error(traceback.print_tb(ex_traceback))\n\n        with open(logfile.replace(\".log\", \".error.log\"), \"w\") as f:\n            f.write(\"osl-ephys SOURCE RECON failed at: {0}\".format(now))\n            f.write(\"\\n\")\n            f.write('Processing failed during stage : \"{0}\"'.format(method))\n            f.write(str(ex_type))\n            f.write(\"\\n\")\n            f.write(str(ex_value))\n            f.write(\"\\n\")\n            traceback.print_tb(ex_traceback, file=f)\n\n        return False\n\n    if gen_report:\n        # Generate data and individual HTML data for the report\n        src_report.gen_html_data(config, outdir, subject, reportdir, extra_funcs=extra_funcs)\n\n        # Generate individual subject HTML report\n        src_report.gen_html_page(reportdir)\n\n    return True\n\n\ndef run_src_batch(\n    config,\n    outdir,\n    subjects,\n    preproc_files=None,\n    smri_files=None,\n    epoch_files=None,\n    surface_extraction_method='fsl',\n    logsdir=None,\n    reportdir=None,\n    gen_report=True,\n    verbose=\"INFO\",\n    mneverbose=\"WARNING\",\n    extra_funcs=None,\n    dask_client=False,\n    random_seed='auto',\n):\n    \"\"\"Batch source reconstruction.\n\n    Parameters\n    ----------\n    config : str or dict\n        Source reconstruction config.\n    outdir : str\n        Source reconstruction directory.\n    subjects : list of str\n        Subject names.\n    surface_extraction_method : str\n        Can be 'fsl' or 'freesurfer'.\n    preproc_files : list of str\n        Preprocessed fif files.\n    smri_files : list of str or str\n        Structural MRI files. Can be 'standard' to use MNI152_T1_2mm.nii\n        for the structural.\n    epoch_files : list of str\n        Epoched fif file.\n    logsdir : str\n        Directory to save log files to.\n    reportdir : str\n        Directory to save report files to.\n    gen_report : bool\n        Should we generate a report?\n    verbose : str\n        Level of verbose.\n    mneverbose : str\n        Level of MNE verbose.\n    extra_funcs : list of functions\n        Custom functions.\n    dask_client : bool\n        Are we using a dask client?\n    random_seed : 'auto' (default), int or None\n        Random seed to set. If 'auto', a random seed will be generated. Random seeds are set for both Python and NumPy.\n        If None, no random seed is set.\n\n    Returns\n    -------\n    flags : list of bool\n        Flags indicating whether coregistration was successful.\n    \"\"\"\n    if surface_extraction_method == 'fsl':\n        rhino.fsl_utils.check_fsl()\n    elif surface_extraction_method == 'freesurfer':\n        freesurfer_utils.check_freesurfer()\n\n    # Directories\n    outdir = validate_outdir(outdir)\n    logsdir = validate_outdir(logsdir or outdir / \"logs\")\n    reportdir = validate_outdir(reportdir or outdir / \"src_report\")\n\n    # Initialise Loggers\n    mne.set_log_level(mneverbose)\n    logfile = os.path.join(logsdir, 'batch_src.log')\n    osl_logger.set_up(log_file=logfile, level=verbose, startup=False)\n    logger.info('Starting osl-ephys Batch Source Reconstruction')\n\n    # Set random seed\n    if random_seed == 'auto':\n        random_seed = set_random_seed()\n    elif random_seed is None:\n        pass\n    else:\n        set_random_seed(random_seed)\n    \n    # Load config\n    config = load_config(config)\n    config_str = pprint.PrettyPrinter().pformat(config)\n    logger.info('Running config\\n {0}'.format(config_str))\n\n    # Number of files (subjects) to process\n    n_subjects = len(subjects)\n\n    # Validation\n    if preproc_files is not None and epoch_files is not None:\n        raise ValueError(\"Please pass either preproc_file or epoch_files, not both.\")\n\n    if preproc_files and epoch_files:\n        raise ValueError(\n            \"Cannot pass both preproc_files=True and epoch_files=True. \"\n            \"Please only pass one of these.\"\n        )\n\n    if isinstance(preproc_files, list):\n        n_files = len(preproc_files)\n        if n_subjects != n_files:\n            raise ValueError(f\"Got {n_subjects} subjects and {n_files} preproc_files.\")\n\n    elif isinstance(epoch_files, list):\n        n_files = len(epoch_files)\n        if n_subjects != n_files:\n            raise ValueError(f\"Got {n_subjects} subjects and {n_files} epoch_files.\")\n\n    else:\n        # Check what files are in the output directory\n        preproc_files_list = []\n        epoch_files_list = []\n        for subject in subjects:\n            preproc_file = f\"{outdir}/{subject}/{subject}_preproc-raw.fif\"\n            epoch_file = f\"{outdir}/{subject}/{subject}_epo.fif\"\n            if os.path.exists(preproc_file) and os.path.exists(epoch_file):\n                if preproc_files is None and epoch_files is None:\n                    raise ValueError(\n                        \"Both preproc and epoch fif files found. \"\n                        \"Please pass preproc_files=True or epoch_files=True.\"\n                    )\n            elif os.path.exists(preproc_file):\n                preproc_files_list.append(preproc_file)\n            elif os.path.exists(epoch_file):\n                epoch_files_list.append(epoch_file)\n        if len(preproc_files_list) > 0:\n            preproc_files = preproc_files_list\n        elif len(epoch_files_list) > 0:\n            epoch_files = epoch_files_list\n\n    doing_coreg = (\n        any([\"compute_surfaces\" in method for method in config[\"source_recon\"]]) or\n        any([\"coregister\" in method for method in config[\"source_recon\"]])\n    )\n\n    if doing_coreg and smri_files is None:\n        raise ValueError(\"smri_files must be passed if we are coregistering.\")\n    elif smri_files is None or isinstance(smri_files, str):\n        smri_files = [smri_files] * n_subjects\n\n    if preproc_files is None:\n        preproc_files = [None] * n_subjects\n\n    if epoch_files is None:\n        epoch_files = [None] * n_subjects\n\n    # Create partial function with fixed options\n    pool_func = partial(\n        run_src_chain,\n        surface_extraction_method=surface_extraction_method,\n        logsdir=logsdir,\n        reportdir=reportdir,\n        gen_report=gen_report,\n        verbose=verbose,\n        mneverbose=mneverbose,\n        extra_funcs=extra_funcs,\n        random_seed=random_seed,\n    )\n\n    # Loop through input files to generate arguments for run_coreg_chain\n    args = []\n    for subject, preproc_file, smri_file, epoch_file, in zip(subjects, preproc_files, smri_files, epoch_files):\n        args.append((config, outdir, subject, preproc_file, smri_file, epoch_file))\n\n    # Actually run the processes\n    if dask_client:\n        flags = parallel.dask_parallel_bag(pool_func, args)\n    else:\n        flags = [pool_func(*aa) for aa in args]\n\n    osl_logger.set_up(log_file=logfile, level=verbose, startup=False)\n    logger.info(\"Processed {0}/{1} files successfully\".format(int(np.sum(flags)), len(flags)))\n\n    if gen_report and int(np.sum(flags)) > 0:\n        # Generate individual subject HTML report\n        src_report.gen_html_page(reportdir)\n\n        # Generate a summary report\n        if src_report.gen_html_summary(reportdir):\n            logger.info(\"******************************\" + \"*\" * len(str(reportdir)))\n            logger.info(f\"* REMEMBER TO CHECK REPORT: {reportdir} *\")\n            logger.info(\"******************************\" + \"*\" * len(str(reportdir)))\n\n    return flags",
  "language": "python",
  "imports": [],
  "has_docstring": true
}