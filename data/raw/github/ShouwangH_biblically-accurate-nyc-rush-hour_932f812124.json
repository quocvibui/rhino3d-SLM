{
  "source_url": "https://github.com/ShouwangH/biblically-accurate-nyc-rush-hour/blob/3b6f9138e5f88f40b0559ce2e200416a2a18dd82/scripts/export-nyc3d.py",
  "repo": "ShouwangH/biblically-accurate-nyc-rush-hour",
  "repo_stars": 0,
  "repo_description": null,
  "license": "unknown",
  "filepath": "scripts/export-nyc3d.py",
  "instruction": "Export NYC 3D Model layers to glTF for web visualization.",
  "code": "#!/usr/bin/env python3\n\"\"\"\nExport NYC 3D Model layers to glTF for web visualization.\n\nReads .3dm files from data/nyc_3d_model/ and exports selected layers\nto public/assets/nyc3d/ as Draco-compressed glTF files.\n\nUsage:\n    python scripts/export-nyc3d.py [--version v1|v2] [--dry-run]\n\nRequires:\n    pip install rhino3dm trimesh numpy\n\nPer CLAUDE.md:\n- Isolated from current assets (outputs to nyc3d/ subdirectory)\n- Feature flag controls whether new assets are used\n- Graceful fallback to legacy assets if loading fails\n\"\"\"\n\nimport argparse\nimport json\nimport os\nimport sys\nimport zipfile\nfrom pathlib import Path\nfrom typing import Dict, List, Set, Tuple, Any\nfrom dataclasses import dataclass, field\nimport time\n\n# =============================================================================\n# Configuration\n# =============================================================================\n\n# Layer name mappings (NYC 3D Model uses inconsistent naming across CDs)\n# NOTE: Use \"Surface\" layers for 3D geometry (Brep), non-Surface are just polylines\nLAYER_ALIASES = {\n    # Buildings - use Surface layers (have Brep geometry)\n    'Facade Surface': 'buildings',\n    'RoofTop Surface': 'buildings',\n    # Roadbed - polylines only, will be converted to flat surfaces\n    'Roadbed': 'roadbed',\n    # Water - polylines only, will be converted to flat surfaces\n    'Water': 'water',\n    'Shoreline': 'water',\n    # Parks - polylines only, will be converted to flat surfaces\n    'Park': 'parks',\n    'Parks': 'parks',\n    'Open Space': 'parks',\n    # Infrastructure\n    'Bridges & Tunnels': 'infrastructure',\n    'Bridge_Tunnel_Overpass': 'infrastructure',\n    # Landmarks - mixed Brep and polylines\n    'Statue_of_Liberty': 'landmarks',\n    # V2 layers\n    'Sidewalk': 'sidewalks',\n    'Sidewalks': 'sidewalks',\n    'Subway Entrances': 'subway_entrances',\n    'Rail Line': 'rail',\n}\n\n# Export versions - which categories to include\n# NOTE: Sidewalks are NOT exported - they're implied via texture tricks:\n#   - Curb band (casing) on roadbeds\n#   - Building plinth (ring around footprints)\n#   - Intersection pads\nEXPORT_VERSIONS = {\n    'v1': {'buildings', 'roadbed', 'water', 'parks', 'infrastructure', 'landmarks'},\n    'v2': {'buildings', 'roadbed', 'water', 'parks', 'infrastructure', 'landmarks',\n           'subway_entrances', 'rail'},  # No sidewalks - implied via texture\n}\n\n# Triangle budget per category (for decimation)\n# NOTE: Buildings decimation causes open faces - keep original geometry\nTRIANGLE_BUDGETS = {\n    'buildings': 2_000_000,   # Keep original to avoid open faces from decimation\n    'roadbed': 200_000,       # Increased budget for better quality\n    'water': 50_000,\n    'parks': 200_000,         # Increased budget for better quality\n    'infrastructure': 50_000,\n    'landmarks': 100_000,     # Increased for complex geometry like Statue of Liberty\n    'sidewalks': 50_000,\n    'subway_entrances': 10_000,\n    'rail': 20_000,\n}\n\n# Sample density for Brep surface fallback (when no pre-computed mesh)\n# Higher = more triangles for curved surfaces, 2 = minimum (4 corners only)\nSAMPLE_DENSITY = {\n    'buildings': 2,        # Buildings have pre-computed meshes\n    'landmarks': 12,       # High for curved geometry like Statue of Liberty\n    'infrastructure': 4,   # Medium for bridges\n    'default': 2,          # Minimum for flat surfaces\n}\n\n# Coordinate system: convert from NYC State Plane (feet) to local meters\n# Origin: Battery Park (40.7033° N, -74.0170° W)\nORIGIN_LAT = 40.7033\nORIGIN_LNG = -74.0170\n\n# NYC State Plane Long Island (EPSG:2263) approximate conversion\n# These are rough values - actual conversion would need pyproj\nSTATE_PLANE_ORIGIN_X = 979536  # feet (exact X at Battery Park via pyproj)\nSTATE_PLANE_ORIGIN_Y = 195508  # feet (exact Y at Battery Park via pyproj)\nFEET_TO_METERS = 0.3048\nMM_TO_FEET = 0.00328084  # For files stored in mm (like MN06)\n\n# Coordinate unit detection thresholds\n# If X > 100,000,000, assume millimeters (MN06 uses ~300M mm values)\nMM_THRESHOLD = 100_000_000\n\n\n@dataclass\nclass ExportStats:\n    \"\"\"Track export statistics.\"\"\"\n    objects_processed: int = 0\n    vertices_total: int = 0\n    faces_total: int = 0\n    layers_found: Dict[str, int] = field(default_factory=dict)\n    files_exported: List[str] = field(default_factory=list)\n    errors: List[str] = field(default_factory=list)\n\n\n# =============================================================================\n# Rhino Geometry Extraction\n# =============================================================================\n\ndef extract_3dm_files(data_dir: Path, include_brooklyn: bool = False) -> List[Path]:\n    \"\"\"Find and extract .3dm files from zip archives.\n\n    Args:\n        data_dir: Directory containing NYC 3D Model zip files\n        include_brooklyn: If True, also process Brooklyn files (for bridges)\n\n    By default only processes MN (Manhattan) files. Set include_brooklyn=True\n    to also get Brooklyn Bridge, Manhattan Bridge, etc.\n    \"\"\"\n    extracted = []\n\n    for zip_path in sorted(data_dir.glob('*.zip')):\n        stem_lower = zip_path.stem.lower()\n        # Process Manhattan files (MN01-MN06)\n        is_manhattan = stem_lower.startswith('nyc_3dmodel_mn')\n        # Process Brooklyn files (BK01-BK17) for bridges\n        is_brooklyn = stem_lower.startswith('nyc_3dmodel_bk') and include_brooklyn\n\n        if not is_manhattan and not is_brooklyn:\n            print(f\"  Skipping {zip_path.name}\")\n            continue\n\n        print(f\"  Extracting {zip_path.name}...\")\n        extract_dir = Path('/tmp') / zip_path.stem.upper()\n        extract_dir.mkdir(exist_ok=True)\n\n        with zipfile.ZipFile(zip_path, 'r') as z:\n            z.extractall(extract_dir)\n\n        for dm_file in extract_dir.glob('*.3dm'):\n            extracted.append(dm_file)\n\n    return extracted\n\n\ndef get_layer_category(layer_name: str, target_categories: Set[str]) -> str | None:\n    \"\"\"Map layer name to export category, or None if not in target.\"\"\"\n    category = LAYER_ALIASES.get(layer_name)\n    if category and category in target_categories:\n        return category\n    return None\n\n\ndef extract_mesh_from_brep(brep, transform_func, rhino3dm, sample_density: int = 2) -> Tuple[List, List] | None:\n    \"\"\"Extract mesh vertices and faces from a Brep object.\n\n    First tries to get pre-computed render mesh. If not available (like MN06),\n    falls back to sampling the surface to create a mesh grid.\n\n    Args:\n        brep: Rhino Brep object\n        transform_func: Coordinate transformation function\n        rhino3dm: The rhino3dm module\n        sample_density: Grid density for surface sampling (default 2 = 4 corners only,\n                       use 8-16 for curved geometry like landmarks)\n    \"\"\"\n    try:\n        # Get faces from Brep\n        brep_faces = brep.Faces\n        if not brep_faces or len(brep_faces) == 0:\n            return None\n\n        all_vertices = []\n        all_faces = []\n        vertex_offset = 0\n\n        for i in range(len(brep_faces)):\n            brep_face = brep_faces[i]\n\n            # Try to get pre-computed mesh first\n            mesh = brep_face.GetMesh(rhino3dm.MeshType.Any)\n\n            if mesh is not None:\n                # Extract vertices from pre-computed mesh\n                for j in range(len(mesh.Vertices)):\n                    v = mesh.Vertices[j]\n                    x, y, z = transform_func(v.X, v.Y, v.Z)\n                    all_vertices.append([x, y, z])\n\n                # Extract faces\n                for j in range(len(mesh.Faces)):\n                    mf = mesh.Faces[j]\n                    if mf[3] == mf[2]:  # Triangle\n                        all_faces.append([\n                            mf[0] + vertex_offset,\n                            mf[1] + vertex_offset,\n                            mf[2] + vertex_offset\n                        ])\n                    else:  # Quad - split into two triangles\n                        all_faces.append([\n                            mf[0] + vertex_offset,\n                            mf[1] + vertex_offset,\n                            mf[2] + vertex_offset\n                        ])\n                        all_faces.append([\n                            mf[0] + vertex_offset,\n                            mf[2] + vertex_offset,\n                            mf[3] + vertex_offset\n                        ])\n\n                vertex_offset += len(mesh.Vertices)\n            else:\n                # No pre-computed mesh - sample the underlying surface with a grid\n                surface = brep_face.UnderlyingSurface()\n                if surface is None:\n                    continue\n\n                try:\n                    u_domain = surface.Domain(0)\n                    v_domain = surface.Domain(1)\n\n                    # Sample surface in a grid pattern\n                    # sample_density determines grid resolution (2=2x2=4 pts, 8=8x8=64 pts)\n                    n = sample_density\n                    grid_verts = []\n\n                    for vi in range(n):\n                        for ui in range(n):\n                            # Use slightly inset fractions to avoid edge issues\n                            u_frac = 0.01 + 0.98 * ui / (n - 1) if n > 1 else 0.5\n                            v_frac = 0.01 + 0.98 * vi / (n - 1) if n > 1 else 0.5\n\n                            u = u_domain.T0 + u_frac * (u_domain.T1 - u_domain.T0)\n                            v = v_domain.T0 + v_frac * (v_domain.T1 - v_domain.T0)\n                            pt = surface.PointAt(u, v)\n                            x, y, z = transform_func(pt.X, pt.Y, pt.Z)\n                            grid_verts.append([x, y, z])\n\n                    all_vertices.extend(grid_verts)\n\n                    # Create triangles from grid (quads split into 2 triangles each)\n                    for vi in range(n - 1):\n                        for ui in range(n - 1):\n                            # Indices in the grid\n                            idx00 = vertex_offset + vi * n + ui\n                            idx10 = vertex_offset + vi * n + (ui + 1)\n                            idx01 = vertex_offset + (vi + 1) * n + ui\n                            idx11 = vertex_offset + (vi + 1) * n + (ui + 1)\n\n                            # Two triangles per quad\n                            all_faces.append([idx00, idx10, idx11])\n                            all_faces.append([idx00, idx11, idx01])\n\n                    vertex_offset += len(grid_verts)\n\n                except Exception:\n                    continue\n\n        if all_vertices and all_faces:\n            return all_vertices, all_faces\n        return None\n    except Exception as e:\n        return None\n\n\ndef extract_mesh_from_mesh(mesh_obj, transform_func) -> Tuple[List, List] | None:\n    \"\"\"Extract vertices and faces from a Mesh object.\"\"\"\n    try:\n        vertices = []\n        faces = []\n\n        for i in range(len(mesh_obj.Vertices)):\n            v = mesh_obj.Vertices[i]\n            x, y, z = transform_func(v.X, v.Y, v.Z)\n            vertices.append([x, y, z])\n\n        for i in range(len(mesh_obj.Faces)):\n            face = mesh_obj.Faces[i]\n            if face[3] == face[2]:  # Triangle\n                faces.append([face[0], face[1], face[2]])\n            else:  # Quad\n                faces.append([face[0], face[1], face[2]])\n                faces.append([face[0], face[2], face[3]])\n\n        if vertices and faces:\n            return vertices, faces\n        return None\n    except Exception as e:\n        return None\n\n\ndef extract_mesh_from_extrusion(extrusion, transform_func, rhino3dm) -> Tuple[List, List] | None:\n    \"\"\"Extract mesh from an Extrusion object.\"\"\"\n    try:\n        mesh = extrusion.GetMesh(rhino3dm.MeshType.Any)\n        if mesh is None:\n            return None\n        return extract_mesh_from_mesh(mesh, transform_func)\n    except Exception as e:\n        return None\n\n\ndef extract_polygon_from_polyline(polyline_curve, transform_func) -> list | None:\n    \"\"\"Extract 2D polygon coordinates from a polyline (for later union).\n\n    Returns list of [x, z] coordinates (in local meters) or None if invalid.\n    \"\"\"\n    try:\n        polyline = polyline_curve.TryGetPolyline()\n        if polyline is None or len(polyline) < 3:\n            return None\n\n        coords = []\n        for i in range(len(polyline) - 1):\n            pt = polyline[i]\n            x, y, z = transform_func(pt.X, pt.Y, pt.Z)\n            coords.append([x, z])  # Use x, z for 2D polygon\n\n        if len(coords) < 3:\n            return None\n        return coords\n    except Exception:\n        return None\n\n\n# Track triangulation failures for debugging\n_triangulation_stats = {'success': 0, 'empty': 0, 'invalid': 0, 'tiny': 0, 'error': 0}\n\ndef extract_flat_surface_from_polyline(polyline_curve, transform_func, y_level: float = 0.0) -> Tuple[List, List] | None:\n    \"\"\"Convert a closed polyline to a flat triangulated surface.\n\n    Used for roadbed, water, parks which are stored as 2D polylines in the 3D model.\n    Uses earcut triangulation for proper handling of concave polygons.\n    \"\"\"\n    global _triangulation_stats\n    try:\n        import numpy as np\n\n        # Get polyline points\n        polyline = polyline_curve.TryGetPolyline()\n        if polyline is None or len(polyline) < 3:\n            _triangulation_stats['empty'] += 1\n            return None\n\n        # Extract points and transform\n        points_2d = []\n        points_3d = []\n        for i in range(len(polyline) - 1):  # Skip last point if closed (duplicate)\n            pt = polyline[i]\n            x, y, z = transform_func(pt.X, pt.Y, pt.Z)\n            points_2d.append([x, z])  # Use x, z for triangulation (flat on ground)\n            points_3d.append([x, y_level, z])  # Set y to ground level\n\n        if len(points_3d) < 3:\n            _triangulation_stats['empty'] += 1\n            return None\n\n        # Use earcut triangulation for robust handling of concave polygons\n        try:\n            import mapbox_earcut as earcut\n            from shapely.geometry import Polygon as ShapelyPolygon, MultiPolygon\n\n            # Create shapely polygon to validate and clean\n            poly = ShapelyPolygon(points_2d)\n            if not poly.is_valid:\n                poly = poly.buffer(0)  # Fix self-intersections\n                # buffer(0) can return MultiPolygon - handle that\n                if isinstance(poly, MultiPolygon):\n                    # Use largest polygon\n                    poly = max(poly.geoms, key=lambda g: g.area)\n\n            if poly.is_empty:\n                _triangulation_stats['invalid'] += 1\n                return None\n\n            if poly.area < 0.1:  # Skip tiny polygons (< 0.1 m²)\n                _triangulation_stats['tiny'] += 1\n                return None\n\n            # Get the exterior ring coordinates\n            coords = list(poly.exterior.coords)[:-1]  # Remove duplicate last point\n            if len(coords) < 3:\n                _triangulation_stats['invalid'] += 1\n                return None\n\n            # Prepare 2D array for earcut (shape: N x 2)\n            coords_array = np.array(coords, dtype=np.float64)\n\n            # Triangulate with earcut\n            # Note: ring_end_indices must point to end of each ring\n            ring_end = np.array([len(coords)], dtype=np.uint32)\n            indices = earcut.triangulate_float64(coords_array, ring_end)\n\n            if len(indices) == 0:\n                # Fallback to fan triangulation\n                n = len(coords)\n                faces = [[0, i, i + 1] for i in range(1, n - 1)]\n                points_3d = [[x, y_level, z] for x, z in coords]\n                _triangulation_stats['success'] += 1\n                return points_3d, faces\n\n            # Convert indices to faces\n            faces = []\n            for i in range(0, len(indices), 3):\n                faces.append([int(indices[i]), int(indices[i+1]), int(indices[i+2])])\n\n            # Convert to 3D vertices at y_level\n            points_3d = [[x, y_level, z] for x, z in coords]\n            _triangulation_stats['success'] += 1\n            return points_3d, faces\n\n        except Exception as ex:\n            # Fallback to simple fan triangulation\n            n = len(points_3d)\n            faces = [[0, i, i + 1] for i in range(1, n - 1)]\n            _triangulation_stats['success'] += 1\n            return points_3d, faces\n\n    except Exception as e:\n        _triangulation_stats['error'] += 1\n        return None\n\ndef print_triangulation_stats():\n    \"\"\"Print triangulation statistics.\"\"\"\n    global _triangulation_stats\n    print(f\"    Triangulation stats: {_triangulation_stats}\")\n    _triangulation_stats = {'success': 0, 'empty': 0, 'invalid': 0, 'tiny': 0, 'error': 0}\n\n\ndef transform_coords(x: float, y: float, z: float) -> Tuple[float, float, float]:\n    \"\"\"Transform from NYC State Plane to local coordinate system (meters).\n\n    Handles both feet (MN01-MN05) and millimeters (MN06) input units.\n    \"\"\"\n    # Detect unit system: if X > 100M, assume millimeters\n    if x > MM_THRESHOLD:\n        # Convert from mm to feet first\n        x = x * MM_TO_FEET\n        y = y * MM_TO_FEET\n        z = z * MM_TO_FEET\n\n    # Convert from State Plane feet to local meters\n    local_x = (x - STATE_PLANE_ORIGIN_X) * FEET_TO_METERS\n    local_z = -(y - STATE_PLANE_ORIGIN_Y) * FEET_TO_METERS  # Flip Y to Z, negate for north\n    local_y = z * FEET_TO_METERS  # Z becomes Y (up)\n\n    return local_x, local_y, local_z\n\n\n# =============================================================================\n# Main Export Logic\n# =============================================================================\n\n# Categories that should be flat surfaces (polylines -> flat mesh)\nFLAT_CATEGORIES = {'roadbed', 'water', 'parks', 'infrastructure'}\n\n# Categories that should use polygon union (merges overlapping shapes)\n# NOTE: Roadbed removed - union destroys street boundaries/curbs\n# NOTE: Parks union disabled for debugging - may be causing gaps\nUNION_CATEGORIES = set()  # Disabled: {'parks'}\n\n# Y-levels for flat surfaces (meters)\n# Buildings sit at Y=0 (sidewalk level), roads are below curb height\nFLAT_Y_LEVELS = {\n    'roadbed': -0.15,  # Roads are ~15cm below curb/sidewalk level\n    'water': -1.0,     # Water well below ground\n    'parks': 0.01,     # Parks at sidewalk level (tiny offset to avoid z-fighting)\n    'infrastructure': 15.0,  # Bridges elevated ~15m above ground (typical clearance)\n}\n\n\ndef process_3dm_file(\n    dm_path: Path,\n    target_categories: Set[str],\n    category_meshes: Dict[str, List],\n    stats: ExportStats,\n    rhino3dm_module,\n    category_polygons: Dict[str, List] = None\n):\n    \"\"\"Process a single .3dm file and extract geometry by category.\n\n    For flat categories (roadbed, parks, water), collects raw 2D polygons\n    in category_polygons for later union and triangulation.\n    \"\"\"\n    print(f\"\\n  Processing {dm_path.name}...\")\n    model = rhino3dm_module.File3dm.Read(str(dm_path))\n\n    # Build layer index -> category mapping\n    layer_map = {}\n    for i, layer in enumerate(model.Layers):\n        category = get_layer_category(layer.Name, target_categories)\n        if category:\n            layer_map[i] = (category, layer.Name)\n            stats.layers_found[layer.Name] = stats.layers_found.get(layer.Name, 0)\n\n    # Process objects\n    processed_in_file = 0\n    polygons_in_file = 0\n    for obj in model.Objects:\n        layer_idx = obj.Attributes.LayerIndex\n        if layer_idx not in layer_map:\n            continue\n\n        category, layer_name = layer_map[layer_idx]\n        stats.layers_found[layer_name] = stats.layers_found.get(layer_name, 0) + 1\n\n        geometry = obj.Geometry\n        mesh_data = None\n\n        # Get object type\n        obj_type = type(geometry).__name__\n\n        if obj_type == 'Brep':\n            # Use higher sample density for curved geometry (landmarks, infrastructure)\n            density = SAMPLE_DENSITY.get(category, SAMPLE_DENSITY['default'])\n            mesh_data = extract_mesh_from_brep(geometry, transform_coords, rhino3dm_module, density)\n        elif obj_type == 'Mesh':\n            mesh_data = extract_mesh_from_mesh(geometry, transform_coords)\n        elif obj_type == 'Extrusion':\n            mesh_data = extract_mesh_from_extrusion(geometry, transform_coords, rhino3dm_module)\n        elif obj_type == 'PolylineCurve' and category in FLAT_CATEGORIES:\n            # For union categories: collect raw polygon for later union\n            # For other flat categories: triangulate individually to preserve boundaries\n            if category in UNION_CATEGORIES and category_polygons is not None and category in category_polygons:\n                poly_coords = extract_polygon_from_polyline(geometry, transform_coords)\n                if poly_coords and len(poly_coords) >= 3:\n                    category_polygons[category].append(poly_coords)\n                    polygons_in_file += 1\n                    stats.objects_processed += 1\n                continue  # Skip individual triangulation\n            else:\n                # Fallback: triangulate individually\n                y_level = FLAT_Y_LEVELS.get(category, 0.0)\n                mesh_data = extract_flat_surface_from_polyline(geometry, transform_coords, y_level)\n\n        if mesh_data:\n            vertices, faces = mesh_data\n            if len(vertices) > 0 and len(faces) > 0:\n                category_meshes[category].append({\n                    'vertices': vertices,\n                    'faces': faces\n                })\n                stats.objects_processed += 1\n                stats.vertices_total += len(vertices)\n                stats.faces_total += len(faces)\n                processed_in_file += 1\n\n    if polygons_in_file > 0:\n        print(f\"    Extracted {processed_in_file} meshes, {polygons_in_file} polygons\")\n    else:\n        print(f\"    Extracted {processed_in_file} meshes\")\n    print_triangulation_stats()\n\n\ndef merge_meshes(mesh_list: List[Dict]) -> Tuple[List, List]:\n    \"\"\"Merge multiple meshes into one.\"\"\"\n    all_vertices = []\n    all_faces = []\n    vertex_offset = 0\n\n    for mesh in mesh_list:\n        all_vertices.extend(mesh['vertices'])\n        for face in mesh['faces']:\n            all_faces.append([f + vertex_offset for f in face])\n        vertex_offset += len(mesh['vertices'])\n\n    return all_vertices, all_faces\n\n\ndef union_and_triangulate_polygons(polygons_2d: List[List], y_level: float) -> Tuple[List, List]:\n    \"\"\"Union multiple 2D polygons and triangulate the result.\n\n    This creates a single cohesive mesh instead of triangle soup from\n    individually triangulated polygons.\n\n    Args:\n        polygons_2d: List of polygon coordinates [[x,z], [x,z], ...]\n        y_level: Y coordinate (height) for the flat surface\n\n    Returns:\n        (vertices, faces) tuple for the unified mesh\n    \"\"\"\n    import numpy as np\n    import mapbox_earcut as earcut\n    from shapely.geometry import Polygon as ShapelyPolygon, MultiPolygon\n    from shapely.ops import unary_union\n\n    print(f\"    Unioning {len(polygons_2d):,} polygons...\")\n\n    # Convert to Shapely polygons, filtering invalid ones\n    shapely_polys = []\n    for coords in polygons_2d:\n        try:\n            if len(coords) < 3:\n                continue\n            poly = ShapelyPolygon(coords)\n            if not poly.is_valid:\n                poly = poly.buffer(0)  # Fix self-intersections\n            if poly.is_valid and not poly.is_empty and poly.area > 0.1:\n                shapely_polys.append(poly)\n        except Exception:\n            continue\n\n    if not shapely_polys:\n        return [], []\n\n    print(f\"    Valid polygons: {len(shapely_polys):,}\")\n\n    # Union all polygons into one (or multipolygon)\n    try:\n        unified = unary_union(shapely_polys)\n    except Exception as e:\n        print(f\"    Union failed: {e}\")\n        return [], []\n\n    # Handle result (could be Polygon or MultiPolygon)\n    if unified.is_empty:\n        return [], []\n\n    all_vertices = []\n    all_faces = []\n    vertex_offset = 0\n\n    # Process each polygon (or the single unified polygon)\n    geoms = [unified] if unified.geom_type == 'Polygon' else list(unified.geoms)\n\n    print(f\"    Result: {len(geoms)} polygon(s)\")\n\n    for geom in geoms:\n        if geom.is_empty or geom.area < 1.0:  # Skip tiny fragments\n            continue\n\n        try:\n            # Get exterior ring\n            exterior_coords = list(geom.exterior.coords)[:-1]\n            if len(exterior_coords) < 3:\n                continue\n\n            # Collect all rings: exterior first, then holes\n            all_coords = list(exterior_coords)\n            ring_ends = [len(exterior_coords)]\n\n            # Add interior rings (holes) - these create holes in triangulation\n            for interior in geom.interiors:\n                hole_coords = list(interior.coords)[:-1]\n                if len(hole_coords) >= 3:\n                    all_coords.extend(hole_coords)\n                    ring_ends.append(len(all_coords))\n\n            # Triangulate with earcut (handles holes via ring_end_indices)\n            coords_array = np.array(all_coords, dtype=np.float64)\n            ring_end_indices = np.array(ring_ends, dtype=np.uint32)\n            indices = earcut.triangulate_float64(coords_array, ring_end_indices)\n\n            if len(indices) == 0:\n                continue\n\n            # Add vertices (convert 2D [x,z] to 3D [x, y_level, z])\n            for x, z in all_coords:\n                all_vertices.append([x, y_level, z])\n\n            # Add faces with offset\n            for i in range(0, len(indices), 3):\n                all_faces.append([\n                    int(indices[i]) + vertex_offset,\n                    int(indices[i+1]) + vertex_offset,\n                    int(indices[i+2]) + vertex_offset\n                ])\n\n            vertex_offset += len(all_coords)\n\n        except Exception as e:\n            continue\n\n    print(f\"    Unified mesh: {len(all_vertices):,} vertices, {len(all_faces):,} faces\")\n    return all_vertices, all_faces\n\n\ndef decimate_mesh(vertices: List, faces: List, target_faces: int) -> Tuple[List, List]:\n    \"\"\"Decimate mesh to target face count using available methods.\"\"\"\n    import numpy as np\n    import trimesh\n\n    if len(faces) <= target_faces:\n        return vertices, faces\n\n    mesh = trimesh.Trimesh(\n        vertices=np.array(vertices),\n        faces=np.array(faces),\n        process=False  # Don't process - we'll handle manually\n    )\n\n    original_faces = len(mesh.faces)\n    print(f\"    Attempting decimation: {original_faces:,} -> {target_faces:,} faces\")\n\n    # Try method 1: simplify_quadric_decimation\n    # Note: API expects target_reduction as ratio (0-1), not face count\n    try:\n        target_reduction = 1.0 - (target_faces / original_faces)\n        target_reduction = max(0.01, min(0.99, target_reduction))  # Clamp to valid range\n        print(f\"    Method 1 (quadric): target_reduction={target_reduction:.2%}\")\n        decimated = mesh.simplify_quadric_decimation(target_reduction)\n        if decimated is not None and len(decimated.faces) > 0:\n            print(f\"    Method 1 (quadric): success -> {len(decimated.faces):,} faces\")\n            return decimated.vertices.tolist(), decimated.faces.tolist()\n    except Exception as e:\n        print(f\"    Method 1 (quadric) failed: {str(e)[:60]}\")\n\n    # Try method 2: vertex clustering (always works, may be less quality)\n    try:\n        # Calculate voxel size to achieve target reduction\n        bounds = mesh.bounds\n        volume = np.prod(bounds[1] - bounds[0])\n        # Rough estimate: double voxel size halves face count\n        reduction_ratio = original_faces / target_faces\n        voxel_size = (volume / original_faces) ** (1/3) * (reduction_ratio ** 0.5)\n\n        # Clamp to reasonable range (0.5m to 10m)\n        voxel_size = max(0.5, min(10.0, voxel_size))\n        print(f\"    Method 2 (voxel clustering): voxel_size={voxel_size:.2f}m\")\n\n        # Quantize vertices to grid\n        quantized_verts = np.round(mesh.vertices / voxel_size) * voxel_size\n\n        # Find unique vertices\n        unique_verts, inverse_indices = np.unique(\n            quantized_verts, axis=0, return_inverse=True\n        )\n\n        # Remap faces\n        new_faces = inverse_indices[mesh.faces]\n\n        # Remove degenerate faces (where vertices collapsed to same point)\n        valid_mask = (new_faces[:, 0] != new_faces[:, 1]) & \\\n                     (new_faces[:, 1] != new_faces[:, 2]) & \\\n                     (new_faces[:, 0] != new_faces[:, 2])\n        new_faces = new_faces[valid_mask]\n\n        # Create new mesh with processing to clean up\n        decimated = trimesh.Trimesh(vertices=unique_verts, faces=new_faces, process=True)\n\n        print(f\"    Method 2 result: {len(decimated.faces):,} faces\")\n        return decimated.vertices.tolist(), decimated.faces.tolist()\n\n    except Exception as e:\n        print(f\"    Method 2 (voxel) failed: {str(e)[:50]}\")\n\n    # Fallback: return original\n    print(f\"    Warning: all decimation methods failed, using original mesh\")\n    return vertices, faces\n\n\ndef export_to_gltf(\n    vertices: List,\n    faces: List,\n    output_path: Path,\n    category: str,\n    merge_coplanar: bool = False\n):\n    \"\"\"Export mesh to glTF format with cleaning for flat surfaces.\"\"\"\n    import numpy as np\n    import trimesh\n\n    mesh = trimesh.Trimesh(\n        vertices=np.array(vertices),\n        faces=np.array(faces),\n        process=True  # Let trimesh clean up automatically\n    )\n\n    # For flat categories (roadbed, parks, water), clean up the mesh more aggressively\n    if category in FLAT_CATEGORIES:\n        initial_faces = len(mesh.faces)\n\n        # 1. Merge vertices within a small tolerance (0.5 meters for flat surfaces)\n        mesh.merge_vertices(merge_tex=True, merge_norm=True)\n\n        # 2. Remove degenerate faces (zero-area or near-zero-area)\n        mask = mesh.nondegenerate_faces()\n        if mask is not None and len(mask) > 0:\n            mesh.update_faces(mask)\n\n        # 3. Remove faces with very small area (thin slivers)\n        # NOTE: Lowered threshold - 0.01 m² was removing too many valid triangles\n        areas = mesh.area_faces\n        min_area = 0.0001  # 0.0001 m² = 1 cm² minimum (was 0.01 = 10cm x 10cm)\n        small_count = np.sum(areas < min_area)\n        valid_area_mask = areas >= min_area\n        mesh.update_faces(valid_area_mask)\n        if small_count > 0:\n            print(f\"    Removed {small_count:,} tiny triangles (< {min_area} m²)\")\n\n        # 3b. Remove oversized triangles (triangulation bugs from degenerate polygons)\n        # Max reasonable road polygon is ~70m x 70m = 5000 m²\n        areas = mesh.area_faces\n        max_area = 5000  # m²\n        oversized_count = np.sum(areas > max_area)\n        if oversized_count > 0:\n            valid_size_mask = areas <= max_area\n            mesh.update_faces(valid_size_mask)\n            print(f\"    Removed {oversized_count:,} oversized triangles (> {max_area} m²)\")\n\n        # 4. Remove duplicate faces (exact duplicates and reversed duplicates)\n        # This helps when polygons share edges/overlap\n        before_merge = len(mesh.faces)\n        mesh.merge_vertices()  # Ensure vertices are merged first\n\n        # Get unique faces (considering both orientations)\n        faces_sorted = np.sort(mesh.faces, axis=1)\n        _, unique_idx = np.unique(faces_sorted, axis=0, return_index=True)\n        dup_count = len(mesh.faces) - len(unique_idx)\n        mesh.update_faces(unique_idx)\n        if dup_count > 0:\n            print(f\"    Removed {dup_count:,} duplicate faces\")\n\n        # 5. Remove unreferenced vertices\n        mesh.remove_unreferenced_vertices()\n\n        # 6. Fix face normals - ensure they point UP for flat ground surfaces\n        # This prevents black faces when viewed from above\n        face_normals = mesh.face_normals\n        down_facing_mask = face_normals[:, 1] < 0  # Y < 0 means pointing down\n        down_count = np.sum(down_facing_mask)\n        if down_count > 0:\n            # Flip winding order for down-facing faces (reverses normal direction)\n            down_faces = np.where(down_facing_mask)[0]\n            mesh.faces[down_faces] = mesh.faces[down_faces][:, ::-1]\n            # Clear cached normals so they get recomputed\n            if hasattr(mesh, '_cache'):\n                mesh._cache.clear()\n            print(f\"    Fixed {down_count:,} down-facing normals\")\n\n        removed = initial_faces - len(mesh.faces)\n        print(f\"    After cleaning: {len(mesh.faces):,} faces (removed {removed:,})\")\n\n    # Remove vertex colors before export to let Three.js material control color\n    # trimesh adds default gray vertex colors which override material in Three.js\n    # Setting visual to None and recreating mesh ensures no vertex colors\n    vertices = mesh.vertices.copy()\n    faces = mesh.faces.copy()\n    face_normals = mesh.face_normals.copy() if mesh.face_normals is not None else None\n\n    # Create a fresh mesh without visual data\n    clean_mesh = trimesh.Trimesh(\n        vertices=vertices,\n        faces=faces,\n        face_normals=face_normals,\n        process=False  # Don't process, keep as-is\n    )\n    # Explicitly set no visual to prevent default vertex colors\n    clean_mesh.visual = None\n\n    # Export as GLB (binary glTF)\n    clean_mesh.export(str(output_path), file_type='glb')\n\n    file_size = output_path.stat().st_size / (1024 * 1024)\n    print(f\"    Exported {category}: {len(mesh.faces):,} faces, {file_size:.1f} MB\")\n\n\ndef generate_manifest(\n    output_dir: Path,\n    stats: ExportStats,\n    version: str\n) -> Dict:\n    \"\"\"Generate manifest.json with asset metadata.\"\"\"\n    manifest = {\n        'version': version,\n        'generated': time.strftime('%Y-%m-%dT%H:%M:%SZ'),\n        'source': 'NYC DCP 3D Building Model (2018)',\n        'assets': {},\n        'stats': {\n            'objects_processed': stats.objects_processed,\n            'total_vertices': stats.vertices_total,\n            'total_faces': stats.faces_total,\n            'layers_processed': stats.layers_found\n        }\n    }\n\n    for file_path in stats.files_exported:\n        name = Path(file_path).stem\n        size = Path(file_path).stat().st_size\n        manifest['assets'][name] = {\n            'file': Path(file_path).name,\n            'size_bytes': size\n        }\n\n    manifest_path = output_dir / 'manifest.json'\n    with open(manifest_path, 'w') as f:\n        json.dump(manifest, f, indent=2)\n\n    return manifest\n\n\ndef main():\n    parser = argparse.ArgumentParser(description='Export NYC 3D Model to glTF')\n    parser.add_argument('--version', choices=['v1', 'v2'], default='v1',\n                       help='Export version (v1=essential, v2=extended)')\n    parser.add_argument('--dry-run', action='store_true',\n                       help='Analyze without exporting')\n    parser.add_argument('--category', type=str,\n                       help='Export only specific category')\n    parser.add_argument('--include-brooklyn', action='store_true',\n                       help='Include Brooklyn files (for bridges)')\n    args = parser.parse_args()\n\n    # Paths\n    script_dir = Path(__file__).parent\n    project_root = script_dir.parent\n    data_dir = project_root / 'data' / 'nyc_3d_model'\n    output_dir = project_root / 'public' / 'assets' / 'nyc3d'\n\n    # Validate\n    if not data_dir.exists():\n        print(f\"Error: Data directory not found: {data_dir}\")\n        print(\"Please ensure NYC 3D Model zips are in data/nyc_3d_model/\")\n        sys.exit(1)\n\n    # Determine target categories\n    target_categories = EXPORT_VERSIONS[args.version]\n    if args.category:\n        if args.category not in target_categories:\n            print(f\"Error: Category '{args.category}' not in {args.version}\")\n            sys.exit(1)\n        target_categories = {args.category}\n\n    print(f\"NYC 3D Model Export ({args.version})\")\n    print(f\"=\" * 50)\n    print(f\"Target categories: {', '.join(sorted(target_categories))}\")\n    print(f\"Output directory: {output_dir}\")\n\n    # Extract .3dm files\n    print(f\"\\n1. Extracting .3dm files...\")\n    include_brooklyn = args.include_brooklyn or (args.category == 'infrastructure')\n    dm_files = extract_3dm_files(data_dir, include_brooklyn=include_brooklyn)\n    print(f\"   Found {len(dm_files)} .3dm files\")\n\n    if not dm_files:\n        print(\"Error: No .3dm files found\")\n        sys.exit(1)\n\n    # Import rhino3dm (after extraction to fail fast if not installed)\n    try:\n        import rhino3dm\n        import trimesh\n        import numpy as np\n    except ImportError as e:\n        print(f\"\\nError: Missing dependency: {e}\")\n        print(\"Install with: pip install rhino3dm trimesh numpy\")\n        sys.exit(1)\n\n    # Initialize category meshes and raw polygons\n    category_meshes: Dict[str, List] = {cat: [] for cat in target_categories}\n    category_polygons: Dict[str, List] = {cat: [] for cat in target_categories if cat in UNION_CATEGORIES}\n    stats = ExportStats()\n\n    # Process each file\n    print(f\"\\n2. Processing geometry...\")\n    for dm_path in dm_files:\n        process_3dm_file(dm_path, target_categories, category_meshes, stats, rhino3dm, category_polygons)\n\n    # Summary\n    print(f\"\\n3. Summary:\")\n    print(f\"   Objects processed: {stats.objects_processed:,}\")\n    print(f\"   Total vertices: {stats.vertices_total:,}\")\n    print(f\"   Total faces: {stats.faces_total:,}\")\n    print(f\"\\n   Layers found:\")\n    for layer, count in sorted(stats.layers_found.items(), key=lambda x: -x[1]):\n        print(f\"     {layer}: {count:,}\")\n\n    if args.dry_run:\n        print(\"\\n[Dry run - no files exported]\")\n        return\n\n    # Create output directory\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Merge, decimate, and export each category\n    print(f\"\\n4. Exporting assets...\")\n    for category in sorted(target_categories):\n        print(f\"\\n  Processing {category}...\")\n\n        # For union categories, use polygon union if we have raw polygons\n        if category in UNION_CATEGORIES and category in category_polygons and category_polygons[category]:\n            polygons = category_polygons[category]\n            y_level = FLAT_Y_LEVELS.get(category, 0.0)\n\n            # Union and triangulate\n            vertices, faces = union_and_triangulate_polygons(polygons, y_level)\n\n            if not vertices or not faces:\n                print(f\"    {category}: polygon union failed, falling back to mesh merge\")\n                # Fall back to old method\n                meshes = category_meshes[category]\n                if not meshes:\n                    print(f\"    {category}: no geometry found, skipping\")\n                    continue\n                vertices, faces = merge_meshes(meshes)\n        else:\n            # Non-flat categories: use traditional mesh merge\n            meshes = category_meshes[category]\n            if not meshes:\n                print(f\"    {category}: no geometry found, skipping\")\n                continue\n\n            print(f\"    Merging {len(meshes)} meshes...\")\n            vertices, faces = merge_meshes(meshes)\n            print(f\"    Merged: {len(vertices):,} vertices, {len(faces):,} faces\")\n\n        # Decimate if needed\n        target = TRIANGLE_BUDGETS.get(category, 50_000)\n        if len(faces) > target:\n            print(f\"    Decimating to {target:,} faces...\")\n            vertices, faces = decimate_mesh(vertices, faces, target)\n\n        # Export\n        output_path = output_dir / f'{category}.glb'\n        export_to_gltf(vertices, faces, output_path, category)\n        stats.files_exported.append(str(output_path))\n\n    # Generate manifest\n    print(f\"\\n5. Generating manifest...\")\n    manifest = generate_manifest(output_dir, stats, args.version)\n    print(f\"   Written to {output_dir / 'manifest.json'}\")\n\n    print(f\"\\n✓ Export complete!\")\n    print(f\"  Files: {len(stats.files_exported)}\")\n    print(f\"  Location: {output_dir}\")\n\n\nif __name__ == '__main__':\n    main()\n",
  "language": "python",
  "imports": [
    "rhino3dm"
  ],
  "has_docstring": true
}