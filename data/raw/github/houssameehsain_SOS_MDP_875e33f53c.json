{
  "source_url": "https://github.com/houssameehsain/SOS_MDP/blob/9c874d89094f8c3ce6d06aec133679907c4d2245/RhGh_envs/BR_DRL_Rhcompute.py",
  "repo": "houssameehsain/SOS_MDP",
  "repo_stars": 2,
  "repo_description": "Deep reinforcement learning to solve self-organized settlement growth modeled as Markov decision processes. ",
  "license": "MIT",
  "filepath": "RhGh_envs/BR_DRL_Rhcompute.py",
  "instruction": "Br drl rhcompute",
  "code": "import os\nimport numpy as np\nfrom math import floor\nimport cv2\n\nimport rhino3dm\nimport requests\nimport base64\nimport json\nimport compute_rhino3d.Util\n\nimport rhinoinside\nrhinoinside.load()\nimport System\nimport Rhino \n\nimport gym\nfrom stable_baselines3 import A2C, PPO, DQN\nfrom stable_baselines3.common.monitor import Monitor\nfrom stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder\nfrom stable_baselines3.common.env_checker import check_env\nimport wandb\nfrom wandb.integration.sb3 import WandbCallback\n\n\nclass BeadyRing(gym.Env):\n    metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": 10}\n\n    def __init__(self, run):\n        # establish rh compute connection\n        compute_rhino3d.Util.url = 'http://localhost:8081/'\n        compute_rhino3d.Util.authToken = ''\n\n        self.post_url = compute_rhino3d.Util.url + 'grasshopper'\n\n        ## Read grasshopper .ghx file ########################### Replace this path\n        gh_data = open('D:/RLinGUD/BeadyRing/BR_localObs_Rhcompute_env.ghx', mode='r', encoding='utf-8-sig').read()\n        data_bytes = gh_data.encode('utf-8')\n        encoded = base64.b64encode(data_bytes)\n        self.algo = encoded.decode('utf-8')\n\n        self._obs_size = 3 # make sure these match .ghx file env\n        self._max_row_len = 36\n        self.pad = int(floor(self._obs_size/2))\n\n        self.action_space = gym.spaces.Discrete(2)\n        self.observation_space = gym.spaces.Box(low=0, high=255, \n                                                shape=(1, self._obs_size, self._obs_size), \n                                                dtype=np.uint8) \n\n        self.res = None\n        self.viewer = None\n        self.isopen = True\n\n        self.run = run\n        self.path = f\"D:/RLinGUD/BeadyRing/images/{self.run.id}/\"\n        if not os.path.exists(self.path):\n            os.makedirs(self.path)\n\n        self.eps = 0\n        self.stp = 0\n        self.cumul_rwd = 0\n\n    def step(self, action):\n        assert self.action_space.contains(action), \"Invalid action\"\n        assert self.res is not None, \"Call reset before using step method.\"\n\n        observation, reward, done, info = self.Rhcompute_connect(action=action)\n\n        self.stp += 1\n        self.cumul_rwd += reward\n        if done:\n            self.eps += 1\n\n        return observation, reward, done, info\n\n    def reset(self):\n        # initial observation\n        observation = self.Rhcompute_connect(reset=True)\n        # reset counters\n        self.stp = 0\n        self.cumul_rwd = 0\n\n        return observation\n\n    def render(self, mode=\"rgb_array\"):\n        assert mode in [\"human\", \"rgb_array\"], \"Invalid mode, must be either 'human' or 'rgb_array'\"\n\n        screen_width = 500\n        screen_height = 500\n\n        # get display array\n        frame = []\n        for output in self.res['values']:\n            paramName = output['ParamName']\n            InnerTree = output['InnerTree']\n            if paramName == 'RH_OUT:state':\n                for _, InnerVals in InnerTree.items():\n                    row = []\n                    for val in InnerVals:\n                        data = json.loads(val['data'])\n                        row.append(data)\n                    frame.append(row)\n        frame = np.asarray(frame)\n        frame = frame[self.pad:self._max_row_len+self.pad, self.pad:self._max_row_len+self.pad]\n        frame = frame.reshape(self._max_row_len, self._max_row_len)\n        frame = cv2.resize(frame, (screen_width, screen_height), interpolation = cv2.INTER_AREA)\n        screen = np.zeros((screen_width, screen_height, 3), dtype=np.uint8)\n        screen[:,:,0] = frame.astype(np.uint8)\n        screen[:,:,1] = frame.astype(np.uint8)\n        screen[:,:,2] = frame.astype(np.uint8)\n\n        img = cv2.copyMakeBorder(screen, 30, 5, 5, 5, cv2.BORDER_CONSTANT, None, value = [255, 255, 255])\n        img = cv2.putText(img, f\"eps {self.eps} | step {self.stp} | return {self.cumul_rwd}\", \n                        (5,20), cv2.FONT_HERSHEY_COMPLEX, 0.6, (0, 0, 0, 255), 1)\n        # save to project dir\n        cv2.imwrite(self.path + f\"eps-{self.eps}_step-{self.stp}.png\", img)\n\n        if mode == \"human\":\n            cv2.imshow('Beady Ring', img)\n            cv2.waitKey(0)\n\n        return img if mode == \"rgb_array\" else self.isopen\n\n    def Rhcompute_connect(self, action=None, reset=False):\n        if action is not None:\n            gh_json = {\n                'algo': self.algo,\n                'pointer': None,\n                'values': [\n                    {\n                        'ParamName': 'RH_IN:action',\n                        'InnerTree': {\n                            '{ 0; }': [{'type': 'System.Int32', 'data': str(int(action))}]\n                        }\n                    }\n                ]\n            }\n        elif reset:\n            gh_json = {\n                'algo': self.algo,\n                'pointer': None,\n                'values': [\n                    {\n                        'ParamName': 'RH_IN:reset',\n                        'InnerTree': {\n                            '{ 0; }': [{'type': 'System.Boolean', 'data': reset}]\n                        }\n                    }\n                ]\n            }\n        else:\n            raise ValueError('Either action or reset must be provided')\n\n        # Send\n        response = requests.post(self.post_url, json=gh_json)\n\n        # Receive\n        res = response.content.decode('utf-8')\n        self.res = json.loads(res)\n\n        observation = None\n        reward = None\n        done = False\n        info = {}\n\n        for output in self.res['values']:\n            paramName = output['ParamName']\n            InnerTree = output['InnerTree']\n            if paramName == 'RH_OUT:observation':\n                observation = []\n                for _, InnerVals in InnerTree.items():\n                    obs = []\n                    for val in InnerVals:\n                        data = json.loads(val['data'])\n                        obs.append(data)\n                    observation.append(obs)\n                observation = np.asarray(observation).reshape((1, self._obs_size, self._obs_size))\n            elif paramName == 'RH_OUT:reward':\n                for _, InnerVals in InnerTree.items():\n                    for val in InnerVals:\n                        data = json.loads(val['data'])\n                        reward = data\n            elif paramName == 'RH_OUT:done':\n                for _, InnerVals in InnerTree.items():\n                    for val in InnerVals:\n                        data = json.loads(val['data'])\n                        done = data\n            elif paramName == 'RH_OUT:info':\n                for _, InnerVals in InnerTree.items():\n                    for val in InnerVals:\n                        data = json.loads(val['data'])\n                        info = data\n\n        if observation is None:\n            raise ValueError('observation cannot be None')\n\n        if reset:\n            return observation\n        else:\n            return observation, reward, done, info\n    \n    def close(self):\n        self.isopen = False\n\n\nif __name__ == '__main__':\n    # Log in to W&B account\n    print('Wandb login ...')\n    wandb.login(key='') # place wandb key here!\n\n    config = {\n        \"rl_alg\": \"PPO\",\n        \"policy_type\": \"MlpPolicy\",\n        \"total_timesteps\": 5000000\n    }\n\n    run = wandb.init(\n        entity='', #Replace with your wandb entity & project\n        project=\"BeadyRing_DRL\",\n        config=config,\n        sync_tensorboard=True, # auto-upload sb3's tensorboard metrics\n        monitor_gym=True,\n        name=f'{config[\"rl_alg\"]}_{config[\"policy_type\"]}'\n    )\n\n    def make_env():\n        env = BeadyRing(run)\n        # check_env(env) # check if the env follows the gym interface\n        env = Monitor(env)  # record stats\n        return env\n\n    env = DummyVecEnv([make_env])\n    env = VecVideoRecorder(env, f\"videos/{run.id}\", \n                           record_video_trigger=lambda x: x % 12960 == 0, \n                           video_length=1296, name_prefix='BR_RL-train') # replace grid_len to match env\n\n    model = PPO(config[\"policy_type\"], env, verbose=1, tensorboard_log=f\"runs/{run.id}\", device='cuda')\n    model.learn(total_timesteps=config[\"total_timesteps\"],\n                callback=WandbCallback(\n                    gradient_save_freq=100,\n                    model_save_path=f'models/{run.id}', \n                    model_save_freq=100,\n                    verbose=2\n                    )    \n                )\n\n    # cum_rwd = 0\n    # obs = env.reset()\n    # for i in range(300):\n    #     action, _states = model.predict(obs, deterministic=True)\n    #     obs, reward, done, info = env.step(action)\n    #     cum_rwd += reward\n    #     if done:\n    #         obs = env.reset()\n    #         print(\"Return = \", cum_rwd)\n    #         cum_rwd = 0\n    env.close()\n\n    run.finish()\n\n",
  "language": "python",
  "imports": [
    "Rhino",
    "rhino3dm"
  ],
  "has_docstring": false
}