{
  "source_url": "https://github.com/VisionXLab/CastDet/blob/fc59d806d799c3de0bd820891704874767b32f66/mmrotate/models/dense_heads/rhino_align_head.py",
  "repo": "VisionXLab/CastDet",
  "repo_stars": 72,
  "repo_description": "[ECCV'24/IJCV'26] Code repo for \"Toward Open Vocabulary Aerial Object Detection with CLIP-Activated Student-Teacher Learning\" ",
  "license": "Apache-2.0",
  "filepath": "mmrotate/models/dense_heads/rhino_align_head.py",
  "instruction": "Copyright (c) SI Analytics. All rights reserved. Licensed under the CC BY-NC 4.0 License. See LICENSE file in the project root for full license information. Copyright (c) OpenMMLab. All rights reserved. Licensed under the Apache License, Version 2.0. See LICENSE file in the mmrotate repository for full license information.",
  "code": "# Copyright (c) SI Analytics. All rights reserved.\n# Licensed under the CC BY-NC 4.0 License. See LICENSE file in the project root for full license information.\n#\n# Copyright (c) OpenMMLab. All rights reserved.\n# Licensed under the Apache License, Version 2.0. See LICENSE file in the mmrotate repository for full license information.\nimport torch\nfrom mmdet.models.utils import multi_apply\nfrom mmdet.utils import InstanceList, OptInstanceList, reduce_mean\nfrom torch import Tensor\nfrom typing import Dict, List, Tuple\n\nfrom mmrotate.registry import MODELS\nfrom .rhino_phc_head import RHINOPositiveHungarianClassificationHead\n\n\n@MODELS.register_module()\nclass RHINOAlignHead(RHINOPositiveHungarianClassificationHead):\n    \"\"\"RHINOAlignHead.\n\n    The align head matches gt using only the query generated from the encoder.\n    \"\"\"\n\n    def loss_by_feat(\n        self,\n        all_layers_cls_scores: Tensor,\n        all_layers_bbox_preds: Tensor,\n        enc_cls_scores: Tensor,\n        enc_bbox_preds: Tensor,\n        batch_gt_instances: InstanceList,\n        batch_img_metas: List[dict],\n        dn_meta: Dict[str, int],\n        batch_gt_instances_ignore: OptInstanceList = None\n    ) -> Dict[str, Tensor]:\n        \"\"\"Loss function.\n\n        Args:\n            all_layers_cls_scores (Tensor): Classification scores of all\n                decoder layers, has shape (num_decoder_layers, bs,\n                num_queries_total, cls_out_channels), where\n                `num_queries_total` is the sum of `num_denoising_queries`\n                and `num_matching_queries`.\n            all_layers_bbox_preds (Tensor): Regression outputs of all decoder\n                layers. Each is a 5D-tensor with normalized coordinate format\n                (cx, cy, w, h, a) and has shape (num_decoder_layers, bs,\n                num_queries_total, 5).\n            enc_cls_scores (Tensor): The score of each point on encode\n                feature map, has shape (bs, num_feat_points, cls_out_channels).\n            enc_bbox_preds (Tensor): The proposal generate from the encode\n                feature map, has shape (bs, num_feat_points, 5) with the last\n                dimension arranged as (cx, cy, w, h, a).\n            batch_gt_instances (list[:obj:`InstanceData`]): Batch of\n                gt_instance. It usually includes ``bboxes`` and ``labels``\n                attributes.\n            batch_img_metas (list[dict]): Meta information of each image, e.g.,\n                image size, scaling factor, etc.\n            dn_meta (Dict[str, int]): The dictionary saves information about\n                group collation, including 'num_denoising_queries' and\n                'num_denoising_groups'. It will be used for split outputs of\n                denoising and matching parts and loss calculation.\n            batch_gt_instances_ignore (list[:obj:`InstanceData`], optional):\n                Batch of gt_instances_ignore. It includes ``bboxes`` attribute\n                data that is ignored during training and testing.\n                Defaults to None.\n\n        Returns:\n            dict[str, Tensor]: A dictionary of loss components.\n        \"\"\"\n        # extract denoising and matching part of outputs\n        (all_layers_matching_cls_scores, all_layers_matching_bbox_preds,\n         all_layers_denoising_cls_scores, all_layers_denoising_bbox_preds) = \\\n            self.split_outputs(\n                all_layers_cls_scores, all_layers_bbox_preds, dn_meta)\n\n        # detr head\n        assert batch_gt_instances_ignore is None, \\\n            f'{self.__class__.__name__} only supports ' \\\n            'for batch_gt_instances_ignore setting to None.'\n        num_imgs = all_layers_cls_scores.size(1)\n        if enc_cls_scores is not None:\n            num_queries = all_layers_matching_cls_scores.size(2)\n            assert num_queries == enc_cls_scores.size(1)\n            cls_scores_list = [enc_cls_scores[i] for i in range(num_imgs)]\n            bbox_preds_list = [enc_bbox_preds[i] for i in range(num_imgs)]\n        else:\n            cls_scores_list = [\n                all_layers_matching_cls_scores[0][i] for i in range(num_imgs)\n            ]\n            bbox_preds_list = [\n                all_layers_matching_bbox_preds[0][i] for i in range(num_imgs)\n            ]\n\n        cls_reg_targets = self.get_targets(cls_scores_list, bbox_preds_list,\n                                           batch_gt_instances, batch_img_metas)\n\n        losses_cls, losses_bbox, losses_iou = multi_apply(\n            self.loss_by_feat_single,\n            all_layers_matching_cls_scores,\n            all_layers_matching_bbox_preds,\n            cls_reg_targets=cls_reg_targets,\n            batch_gt_instances=batch_gt_instances,\n            batch_img_metas=batch_img_metas)\n\n        loss_dict = dict()\n        # loss from the last decoder layer\n        loss_dict['loss_cls'] = losses_cls[-1]\n        loss_dict['loss_bbox'] = losses_bbox[-1]\n        loss_dict['loss_iou'] = losses_iou[-1]\n        # loss from other decoder layers\n        num_dec_layer = 0\n        for loss_cls_i, loss_bbox_i, loss_iou_i in \\\n                zip(losses_cls[:-1], losses_bbox[:-1], losses_iou[:-1]):\n            loss_dict[f'd{num_dec_layer}.loss_cls'] = loss_cls_i\n            loss_dict[f'd{num_dec_layer}.loss_bbox'] = loss_bbox_i\n            loss_dict[f'd{num_dec_layer}.loss_iou'] = loss_iou_i\n            num_dec_layer += 1\n\n        # loss of proposal generated from encode feature map.\n        if enc_cls_scores is not None:\n            # NOTE The enc_loss calculation of the DINO is\n            # different from that of Deformable DETR.\n            enc_loss_cls, enc_losses_bbox, enc_losses_iou = \\\n                self.loss_by_feat_single(\n                    enc_cls_scores, enc_bbox_preds,\n                    cls_reg_targets=cls_reg_targets,\n                    batch_gt_instances=batch_gt_instances,\n                    batch_img_metas=batch_img_metas)\n            loss_dict['enc_loss_cls'] = enc_loss_cls\n            loss_dict['enc_loss_bbox'] = enc_losses_bbox\n            loss_dict['enc_loss_iou'] = enc_losses_iou\n\n        if all_layers_denoising_cls_scores is not None:\n            # calculate denoising loss from all decoder layers\n            dn_losses_cls, dn_losses_bbox, dn_losses_iou = self.loss_dn(\n                all_layers_denoising_cls_scores,\n                all_layers_denoising_bbox_preds,\n                all_layers_matching_cls_scores,\n                all_layers_matching_bbox_preds,\n                batch_gt_instances=batch_gt_instances,\n                batch_img_metas=batch_img_metas,\n                dn_meta=dn_meta)\n            # collate denoising loss\n            loss_dict['dn_loss_cls'] = dn_losses_cls[-1]\n            loss_dict['dn_loss_bbox'] = dn_losses_bbox[-1]\n            loss_dict['dn_loss_iou'] = dn_losses_iou[-1]\n            for num_dec_layer, (loss_cls_i, loss_bbox_i, loss_iou_i) in \\\n                    enumerate(zip(dn_losses_cls[:-1], dn_losses_bbox[:-1],\n                                  dn_losses_iou[:-1])):\n                loss_dict[f'd{num_dec_layer}.dn_loss_cls'] = loss_cls_i\n                loss_dict[f'd{num_dec_layer}.dn_loss_bbox'] = loss_bbox_i\n                loss_dict[f'd{num_dec_layer}.dn_loss_iou'] = loss_iou_i\n        return loss_dict\n\n    def loss_by_feat_single(self, cls_scores: Tensor, bbox_preds: Tensor,\n                            cls_reg_targets: Tuple,\n                            batch_gt_instances: InstanceList,\n                            batch_img_metas: List[dict]) -> Tuple[Tensor]:\n        \"\"\"Loss function for outputs from a single decoder layer of a single\n        feature level.\n\n        The only difference from the parent method is the normalization factor\n        which has 5 dimension for rotated boxes.\n\n        Args:\n            cls_scores (Tensor): Box score logits from a single decoder layer\n                for all images, has shape (bs, num_queries, cls_out_channels).\n            bbox_preds (Tensor): Sigmoid outputs from a single decoder layer\n                for all images, with normalized coordinate (cx, cy, w, h, rad)\n                and shape (bs, num_queries, 5).\n            batch_gt_instances (list[:obj:`InstanceData`]): Batch of\n                gt_instance. It usually includes ``bboxes`` and ``labels``\n                attributes.\n            batch_img_metas (list[dict]): Meta information of each image, e.g.,\n                image size, scaling factor, etc.\n\n        Returns:\n            Tuple[Tensor]: A tuple including `loss_cls`, `loss_box` and\n            `loss_iou`.\n        \"\"\"\n        (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list,\n         num_total_pos, num_total_neg) = cls_reg_targets\n        labels = torch.cat(labels_list, 0)\n        label_weights = torch.cat(label_weights_list, 0)\n        bbox_targets = torch.cat(bbox_targets_list, 0)\n        bbox_weights = torch.cat(bbox_weights_list, 0)\n\n        # classification loss\n        cls_scores = cls_scores.reshape(-1, self.cls_out_channels)\n        # construct weighted avg_factor to match with the official DETR repo\n        cls_avg_factor = num_total_pos * 1.0 + \\\n            num_total_neg * self.bg_cls_weight\n        if self.sync_cls_avg_factor:\n            cls_avg_factor = reduce_mean(\n                cls_scores.new_tensor([cls_avg_factor]))\n        cls_avg_factor = max(cls_avg_factor, 1)\n\n        loss_cls = self.loss_cls(\n            cls_scores, labels, label_weights, avg_factor=cls_avg_factor)\n\n        # Compute the average number of gt boxes across all gpus, for\n        # normalization purposes\n        num_total_pos = loss_cls.new_tensor([num_total_pos])\n        num_total_pos = torch.clamp(reduce_mean(num_total_pos), min=1).item()\n\n        # construct factors used for rescale bboxes\n        factors = []\n        for img_meta, bbox_pred in zip(batch_img_metas, bbox_preds):\n            img_h, img_w, = img_meta['img_shape']\n            # angle_factor is newly added.\n            factor = bbox_pred.new_tensor(\n                [img_w, img_h, img_w, img_h,\n                 self.angle_factor]).unsqueeze(0).repeat(bbox_pred.size(0), 1)\n            factors.append(factor)\n        factors = torch.cat(factors, 0)\n\n        # DETR regress the relative position of boxes (cxcywhr) in the image,\n        # thus the learning target is normalized by the image size. So here\n        # we need to re-scale them for calculating IoU loss\n        bbox_preds = bbox_preds.reshape(-1, 5)\n        bboxes = bbox_preds * factors\n        bboxes_gt = bbox_targets * factors\n\n        # regression IoU loss, defaultly GIoU loss\n        loss_iou = self.loss_iou(\n            bboxes, bboxes_gt, bbox_weights, avg_factor=num_total_pos)\n\n        # regression L1 loss\n        loss_bbox = self.loss_bbox(\n            bbox_preds, bbox_targets, bbox_weights, avg_factor=num_total_pos)\n        return loss_cls, loss_bbox, loss_iou\n",
  "language": "python",
  "imports": [],
  "has_docstring": true
}