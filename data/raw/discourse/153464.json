{
  "source_url": "https://discourse.mcneel.com/t/how-to-use-gpu-in-my-c-dll-called-from-python-script/153464",
  "topic_id": 153464,
  "title": "How to use GPU in my C++ DLL called from Python script",
  "question": "@dale\n \n@stevebaer\n\n\nCurrently I am using Python with calls to a C++ DLL to accelerate execution. But even taking advantage of the 24 cores running at 5 GHz in my CPU, the parallel-programmed C++ code is still quite slow, taking over 20 sec for 1 operation (finding intersecting faces in a mesh). If I could run this operation on my NVIDIA 3080 ti GPU which has over 10,000 CUDA cores running around 1.5 GHz, then I may get close to a 100X speedup (10,000/24 * 1.5 GHz/5 GHz).\n\n\nWhat is the best way to add support for CUDA code to my C++ DLL in Microsoft Visual Studio? I used a guide written by \n@dale\n to include Rhino support in the DLL so I think I need to start from here in order to add support for CUDA code. On the other hand, I see that NVIDIA recommends Nsight Visual Studio Edition Version 2022.4 to bring GPU computing into Microsoft Visual Studio.\n\n\nIf I start with Nsight, what are the steps to add Rhino support? Also what version of Nsight should be used, 2022.4 or an older version? Last time I tried to use Microsoft Visual Studio 2022 in Rhino, it had some errors as Rhino was not yet fixed up to support it. Thus I am concerned about which version of Nsight should be used.\n\n\nMy Python script calls the C++ DLL to accelerate slow operations. This has been working fine for the last several years, providing over 100X speedup for some operations.\n\n\nNow I want to take the next step and exploit my GPU for further speedup.\n\n\nRegards,\n\nTerry.",
  "code_blocks": [
    {
      "code": "#include \"cuda_runtime.h\"\n#include \"device_launch_parameters.h\"\n\n#include <stdio.h>\n\ncudaError_t addWithCuda(int *c, const int *a, const int *b, unsigned int size);\n\n__global__ void addKernel(int *c, const int *a, const int *b)\n{\n    int i = threadIdx.x;\n    c[i] = a[i] + b[i];\n}\n\nint main()\n{\n    const int arraySize = 5;\n    const int a[arraySize] = { 1, 2, 3, 4, 5 };\n    const int b[arraySize] = { 10, 20, 30, 40, 50 };\n    int c[arraySize] = { 0 };\n\n    // Add vectors in parallel.\n    cudaError_t cudaStatus = addWithCuda(c, a, b, arraySize);\n    if (cudaStatus != cudaSuccess) {\n        fprintf(stderr, \"addWithCuda failed!\");\n        return 1;\n    }\n\n    printf(\"{1,2,3,4,5} + {10,20,30,40,50} = {%d,%d,%d,%d,%d}\\n\",\n        c[0], c[1], c[2], c[3], c[4]);\n    \n    // cudaDeviceReset must be called before exiting in order for profiling and\n    // tracing tools such as Nsight and Visual Profiler to show complete traces.\n    cudaStatus = cudaDeviceReset();\n    if (cudaStatus != cudaSuccess) {\n        fprintf(stderr, \"cudaDeviceReset failed!\");\n        return 1;\n    }\n\n    return 0;\n}\n// Helper function for using CUDA to add vectors in parallel.\ncudaError_t addWithCuda(int *c, const int *a, const int *b, unsigned int size)\n{\n    int *dev_a = 0;\n    int *dev_b = 0;\n    int *dev_c = 0;\n    cudaError_t cudaStatus;\n\n    // Choose which GPU to run on, change this on a multi-GPU system.\n    cudaStatus = cudaSetDevice(0);\n    if (cudaStatus != cudaSuccess) {\n        fprintf(stderr, \"cudaSetDevice failed!  Do you have a CUDA-capable GPU installed?\");\n        goto Error;\n    }\n\n    // Allocate GPU buffers for three vectors (two input, one output)    .\n    cudaStatus = cudaMalloc((void**)&dev_c, size * sizeof(int));\n    if (cudaStatus != cudaSuccess) {\n        fprintf(stderr, \"cudaMalloc failed!\");\n        goto Error;\n    }\n\n    cudaStatus = cudaMalloc((void**)&dev_a, size * sizeof(int));\n    if (cudaStatus != cudaSuccess) {\n        fprintf(stderr, \"cudaMalloc failed!\");\n        goto Error;\n    }\n\n    cudaStatus = cudaMalloc((void**)&dev_b, size * sizeof(int));\n    if (cudaStatus != cudaSuccess) {\n        fprintf(stderr, \"cudaMalloc failed!\");\n        goto Error;\n    }\n\n    // Copy input vectors a and b from host memory to GPU buffers.\n    cudaStatus = cudaMemcpy(dev_a, a, size * sizeof(int), cudaMemcpyHostToDevice);\n    if (cudaStatus != cudaSuccess) {\n        fprintf(stderr, \"cudaMemcpy failed!\");\n        goto Error;\n    }\n\n    cudaStatus = cudaMemcpy(dev_b, b, size * sizeof(int), cudaMemcpyHostToDevice);\n    if (cudaStatus != cudaSuccess) {\n        fprintf(stderr, \"cudaMemcpy failed!\");\n        goto Error;\n    }\n\n    // Launch a kernel on the GPU with one thread for each element.\n    addKernel<<<1, size>>>(dev_c, dev_a, dev_b);\n\n    // Check for any errors launching the kernel\n    cudaStatus = cudaGetLastError();\n    if (cudaStatus != cudaSuccess) {\n        fprintf(stderr, \"addKernel launch failed: %s\\n\", cudaGetErrorString(cudaStatus));\n        goto Error;\n    }\n    \n    // cudaDeviceSynchronize waits for the kernel to finish, and returns\n    // any errors encountered during the launch.\n    cudaStatus = cudaDeviceSynchronize();\n    if (cudaStatus != cudaSuccess) {\n        fprintf(stderr, \"cudaDeviceSynchronize returned error code %d after launching addKernel!\\n\", cudaStatus);\n        goto Error;\n    }\n\n    // Copy output vector from GPU buffer to host memory.\n    cudaStatus = cudaMemcpy(c, dev_c, size * sizeof(int), cudaMemcpyDeviceToHost);\n    if (cudaStatus != cudaSuccess) {\n        fprintf(stderr, \"cudaMemcpy failed!\");\n        goto Error;\n    }\n\nError:\n    cudaFree(dev_c);\n    cudaFree(dev_a);\n    cudaFree(dev_b);\n    \n    return cudaStatus;\n}",
      "language": "csharp",
      "author": "Terry_Chappell",
      "post_number": 8,
      "is_solution": false
    },
    {
      "code": "#include <stdio.h>\n\n#include \"cuda_runtime.h\"\n#include \"device_launch_parameters.h\"\n\n#include \"cudaSuperProjector.h\"\n\n__global__ void addKernel(float* c, const float* a, const float* b) {\n    int i = threadIdx.x;\n    c[i] = a[i] + b[i];\n}\n\n// Helper function for using CUDA to add vectors in parallel.\ncudaError_t addWithCuda(float* c, const float* a, const float* b, unsigned int size) {\n    float* dev_a = 0;\n    float* dev_b = 0;\n    float* dev_c = 0;\n    cudaError_t cudaStatus;\n\n    // Choose which GPU to run on, change this on a multi-GPU system.\n    cudaStatus = cudaSetDevice(0);\n\n    // Allocate GPU buffers for three vectors (two input, one output)    .\n    cudaStatus = cudaMalloc((void**)&dev_c, size * sizeof(float));\n    cudaStatus = cudaMalloc((void**)&dev_a, size * sizeof(float));\n    cudaStatus = cudaMalloc((void**)&dev_b, size * sizeof(float));\n\n    // Copy input vectors from host memory to GPU buffers.\n    cudaStatus = cudaMemcpy(dev_a, a, size * sizeof(float), cudaMemcpyHostToDevice);\n    cudaStatus = cudaMemcpy(dev_b, b, size * sizeof(float), cudaMemcpyHostToDevice);\n\n    // Launch a kernel on the GPU with one thread for each element.\n    addKernel <<<1, size >>> (dev_c, dev_a, dev_b);\n    // Check for any errors launching the kernel\n    cudaStatus = cudaGetLastError();\n    // cudaDeviceSynchronize waits for the kernel to finish, and returns\n    // any errors encountered during the launch.\n    cudaStatus = cudaDeviceSynchronize();\n\n    // Copy output vector from GPU buffer to host memory.\n    cudaStatus = cudaMemcpy(c, dev_c, size * sizeof(float), cudaMemcpyDeviceToHost);\n    return cudaStatus;\n}\n\nvoid cudaSuperProjector::sumVectors(float* c, float* a, float* b, int N) {\n    cudaError_t cudaStatus = addWithCuda(c, a, b, N);\n    if (cudaStatus != cudaSuccess) {\n        fprintf(stderr, \"cudaSuperProjector::sumVectors failed!\");\n    }\n}",
      "language": "csharp",
      "author": "Terry_Chappell",
      "post_number": 9,
      "is_solution": false
    }
  ],
  "tags": [
    "geometry",
    "mesh",
    "python"
  ],
  "has_accepted_answer": false,
  "category_id": 3,
  "posts_count": 14,
  "views": 2325
}