{
  "source_url": "https://github.com/caitlynmeeks/Noodlings/blob/171e8daadb0bb6c2db59956581ec23bd179cbca2/applications/cmush/agent_cognition.py",
  "repo": "caitlynmeeks/Noodlings",
  "repo_stars": 2,
  "repo_description": "Multi-timescale affective agents with theatrical control - 97K parameter architecture exploring functional correlates of consciousness",
  "license": "MIT",
  "filepath": "applications/cmush/agent_cognition.py",
  "instruction": "Agent Cognition Loop Mixin - Continuous cognition and intuition",
  "code": "# ‚ñÑ‚ñÑ‚ñÑ    ‚ñÑ‚ñÑ‚ñÑ   ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ     ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ   ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ   ‚ñÑ‚ñÑ‚ñÑ      ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ ‚ñÑ‚ñÑ‚ñÑ    ‚ñÑ‚ñÑ‚ñÑ  ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ\n# ‚ñà‚ñà‚ñà‚ñà‚ñÑ  ‚ñà‚ñà‚ñà ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÑ ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÑ ‚ñà‚ñà‚ñà‚ñÄ‚ñÄ‚ñà‚ñà‚ñÑ ‚ñà‚ñà‚ñà       ‚ñà‚ñà‚ñà  ‚ñà‚ñà‚ñà‚ñà‚ñÑ  ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñÄ‚ñÄ‚ñÄ‚ñÄ‚ñÄ\n# ‚ñà‚ñà‚ñà‚ñÄ‚ñà‚ñà‚ñÑ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà  ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà       ‚ñà‚ñà‚ñà  ‚ñà‚ñà‚ñà‚ñÄ‚ñà‚ñà‚ñÑ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà\n# ‚ñà‚ñà‚ñà  ‚ñÄ‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà  ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà       ‚ñà‚ñà‚ñà  ‚ñà‚ñà‚ñà  ‚ñÄ‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà  ‚ñà‚ñà‚ñà‚ñÄ\n# ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà  ‚ñÄ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÄ   ‚ñÄ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÄ  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÄ  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñÑ‚ñà‚ñà‚ñà‚ñÑ ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà ‚ñÄ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÄ\n#\n#  ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ   ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ   ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ    ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ\n# ‚ñà‚ñà‚ñà‚ñÄ‚ñÄ‚ñÄ‚ñÄ‚ñÄ ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÑ ‚ñà‚ñà‚ñà‚ñÄ‚ñÄ‚ñà‚ñà‚ñà‚ñÑ ‚ñà‚ñà‚ñà‚ñÄ‚ñÄ‚ñÄ‚ñÄ‚ñÄ\n# ‚ñà‚ñà‚ñà      ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñÑ‚ñÑ‚ñà‚ñà‚ñà‚ñÄ ‚ñà‚ñà‚ñà‚ñÑ‚ñÑ\n# ‚ñà‚ñà‚ñà      ‚ñà‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñÄ‚ñÄ‚ñà‚ñà‚ñÑ  ‚ñà‚ñà‚ñà\n# ‚ñÄ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ‚ñÄ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÄ  ‚ñà‚ñà‚ñà  ‚ñÄ‚ñà‚ñà‚ñà ‚ñÄ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#\n#   Agent Cognition Loop\n#\n#   This is the \"thinking engine\" that runs inside every Noodling.\n#   It's the continuous background process that:\n#\n#   - Generates intuitions and insights without external input\n#   - Manages the cognition cycle (perception -> thought -> response)\n#   - Decides when to speak up vs stay quiet\n#   - Tracks how many LLM calls are happening\n#\n#   Think of it as the stream of consciousness that keeps running\n#   even when nobody is talking to the Noodling. It's why they\n#   might suddenly say \"I just realized something!\" out of nowhere.\n#\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n# MODULE:   applications.cmush.agent_cognition\n# PURPOSE:  Continuous cognition loop for Noodlings\n# LAYER:    Backend / Agent\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#\n# KEY CLASSES:\n#   CognitionLoopMixin    Continuous thought process mixin\n#\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n# SPDX-License-Identifier: MIT\n# Subject to the Noodling Ethical Covenant (NEC)\n# (C) 2026 Caitlyn Meeks\n# Noodling Technologies, LLC\n# https://noodlings.ai\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\"\"\"\nAgent Cognition Loop Mixin - Continuous cognition and intuition\n\nExtracted from agent_bridge.py for maintainability.\nContains cognition loop methods (~450 lines):\n- _generate_intuition: Deep intuition/insight generation via LLM\n- _complete_cognition_cycle: Cycle state management\n- _increment_llm_counter / _decrement_llm_counter: LLM call tracking\n- start_cognition / stop_cognition: Loop control\n- _continuous_cognition_loop: Main autonomous cognition loop\n\nThis is a mixin class - CMUSHNoodlingAgent inherits from it.\n\"\"\"\n\nfrom typing import Dict, Optional, List\nimport time\nimport asyncio\nimport logging\n\n# Scene Protocol integration (optional - graceful fallback if not available)\ntry:\n    from scene_protocol_integration import (\n        SCENE_PROTOCOL_AVAILABLE,\n        prepare_facet_context,\n        finalize_facet_context,\n    )\nexcept ImportError:\n    SCENE_PROTOCOL_AVAILABLE = False\n    prepare_facet_context = None\n    finalize_facet_context = None\n\nlogger = logging.getLogger(__name__)\n\n\nclass CognitionLoopMixin:\n    \"\"\"\n    Mixin class providing cognition loop methods for CMUSHNoodlingAgent.\n\n    This mixin expects the following attributes on self:\n    - agent_id, agent_name: Agent identity\n    - config: Agent configuration dict\n    - llm: LLM interface\n    - consciousness: Noodling charm instance\n    - Various cycle tracking attributes (cycle_in_progress, etc.)\n    - pending_perceptions: Queue of events to process\n\n    And methods from other mixins:\n    - perceive_event() from PerceptionMixin\n    - _generate_rumination() from ResponseGenerationMixin\n    \"\"\"\n\n    async def _generate_intuition(\n        self,\n        event: Dict,\n        world_state: Optional[Dict] = None,\n        recent_context: Optional[List[Dict]] = None\n    ) -> Optional[str]:\n        \"\"\"\n        Generate contextual intuition using fast LLM analysis.\n\n        The Intuition Receiver acts like a radio tuned to contextual signals,\n        providing natural awareness of:\n        - Message routing (who is being addressed)\n        - Spatial relationships (who is where)\n        - Prop tracking (who has what)\n        - Recent actions (what just happened)\n\n        This creates integrated contextual awareness rather than external scaffolding.\n\n        Args:\n            event: Current event being perceived\n            world_state: Optional world state dictionary with room/agent/object info\n            recent_context: Recent conversation context (last 3-5 messages)\n\n        Returns:\n            Intuitive awareness string, or None if intuition disabled/failed\n        \"\"\"\n        # Check if intuition receiver is enabled\n        intuition_config = self.config.get('intuition_receiver', {})\n        if not intuition_config.get('enabled', True):\n            return None\n\n        try:\n            # Extract event details\n            event_type = event.get('type', 'say')\n            speaker_id = event.get('user', '')\n            message_text = event.get('text', '')\n            room_id = event.get('room', '')\n\n            # Build context for intuition analysis\n            context_info = []\n\n            # 1. WHO IS SPEAKING\n            speaker_name = speaker_id.replace('agent_', '').replace('user_', '').title()\n            context_info.append(f\"Speaker: {speaker_name} ({speaker_id})\")\n\n            # 2. RECENT CONVERSATION FLOW\n            if recent_context:\n                recent_speakers = []\n                for entry in recent_context[-3:]:\n                    speaker = entry.get('user', '').replace('agent_', '').replace('user_', '').title()\n                    text_snippet = entry.get('text', '')[:50]\n                    recent_speakers.append(f\"{speaker}: {text_snippet}\")\n                context_info.append(f\"Recent conversation:\\n\" + \"\\n\".join(recent_speakers))\n\n            # 3. CURRENT MESSAGE\n            context_info.append(f\"Current message: '{message_text}'\")\n\n            # 4. ONGOING GAMES / EXPECTATIONS\n            # Detect if there's an active game or thing people are waiting for\n            if recent_context:\n                # Look for secret word games, memory games, etc.\n                game_mentions = []\n                for entry in recent_context[-10:]:  # Last 10 messages\n                    text_lower = entry.get('text', '').lower()\n                    if 'secret word' in text_lower or 'magic word' in text_lower:\n                        game_mentions.append(\"There's a secret word game active\")\n                    if 'memory game' in text_lower:\n                        game_mentions.append(\"There's a memory game happening\")\n                    if 'waiting for' in text_lower or 'ready for' in text_lower:\n                        game_mentions.append(\"People are waiting for something to happen\")\n\n                if game_mentions:\n                    context_info.append(f\"Active game/expectation: {', '.join(set(game_mentions))}\")\n\n            # 4. WORLD STATE (if available)\n            if world_state:\n                # Room occupants with species/metadata\n                room = world_state.get('rooms', {}).get(room_id, {})\n                all_occupants = room.get('occupants', [])\n\n                # Filter out invisible users (admin stealth mode)\n                occupants = []\n                for occ_id in all_occupants:\n                    if occ_id.startswith('user_'):\n                        user_data = world_state.get('users', {}).get(occ_id, {})\n                        if user_data.get('invisible', False):\n                            continue  # Skip invisible admin users\n                    occupants.append(occ_id)\n\n                if occupants:\n                    occupant_details = []\n                    for occ_id in occupants:\n                        occ_name = occ_id.replace('agent_', '').replace('user_', '').title()\n\n                        # Get agent metadata if available\n                        if occ_id.startswith('agent_'):\n                            agent_data = world_state.get('agents', {}).get(occ_id, {})\n                            config = agent_data.get('config', {})\n                            species = config.get('species', 'noodling')\n                            age = config.get('age', 'unknown')\n                            pronoun = config.get('pronoun', 'they')\n\n                            # Infer pronoun from common character names if not specified\n                            if pronoun == 'they':\n                                name_lower = occ_name.lower()\n                                if name_lower in ['phi', 'callie', 'desobelle']:\n                                    pronoun = 'she'\n                                elif name_lower in ['toad', 'mr. toad', 'phido']:\n                                    pronoun = 'he'\n                                elif name_lower in ['servnak']:\n                                    pronoun = 'they'  # SERVNAK is non-binary robot\n\n                            # Build descriptive string with useful metadata\n                            details = f\"{occ_name} ({species}, {age}, {pronoun})\"\n                            occupant_details.append(details)\n                        else:\n                            # Get user metadata\n                            user_data = world_state.get('users', {}).get(occ_id, {})\n                            species = user_data.get('species', 'human')\n                            age = user_data.get('age', 'unknown')\n                            pronoun = user_data.get('pronoun', 'they')\n                            details = f\"{occ_name} ({species}, {age}, {pronoun})\"\n                            occupant_details.append(details)\n\n                    context_info.append(f\"Present in room: {', '.join(occupant_details)}\")\n\n                # Objects in room\n                objects = room.get('objects', [])\n                if objects:\n                    object_list = []\n                    for obj_id in objects[:5]:  # Limit to 5 objects\n                        obj = world_state.get('objects', {}).get(obj_id, {})\n                        obj_name = obj.get('name', obj_id)\n                        object_list.append(obj_name)\n                    context_info.append(f\"Objects nearby: {', '.join(object_list)}\")\n\n                # Agent inventories (who has what)\n                agents_with_items = []\n                for agent_id in [occ for occ in occupants if occ.startswith('agent_')]:\n                    agent_data = world_state.get('agents', {}).get(agent_id, {})\n                    inventory = agent_data.get('inventory', [])\n                    if inventory:\n                        agent_name = agent_id.replace('agent_', '').title()\n                        items = []\n                        for item_id in inventory[:3]:  # Limit to 3 items per agent\n                            obj = world_state.get('objects', {}).get(item_id, {})\n                            items.append(obj.get('name', item_id))\n                        agents_with_items.append(f\"{agent_name} has: {', '.join(items)}\")\n                if agents_with_items:\n                    context_info.append(\"Possessions:\\n\" + \"\\n\".join(agents_with_items))\n\n            # Build intuition prompt\n            context_text = \"\\n\\n\".join(context_info)\n\n            my_name = self.agent_name\n\n            # Check if agent's name is in the message for better routing\n            my_name_normalized = my_name.lower().replace('_', ' ').replace('fire', '').strip()\n            message_lower = message_text.lower()\n            name_mentioned = any(name_part in message_lower for name_part in my_name_normalized.split() if len(name_part) > 2)\n\n            prompt = f\"\"\"You are {my_name}'s intuitive awareness - you report ONLY factual observations from the context provided.\n\nAGENT NAME: {my_name}\nMESSAGE TEXT: \"{message_text}\"\n\nCONTEXT:\n{context_text}\n\nGenerate brief factual awareness (1-2 sentences max) that reports:\n\n1. WHO is being addressed - CHECK THE MESSAGE TEXT ABOVE:\n   - If message contains \"{my_name}\" or \"red\" or \"you\": \"This is directed at ME\"\n   - If message names someone else specifically: \"This is for [name], not me\"\n   - If message is general/broadcast: \"This is addressed to everyone present\"\n\n2. FACTUAL OBSERVATIONS from context:\n   - Who is present (from room occupants list)\n   - Who has what objects (from possessions list)\n   - NO embellishments, NO invented details\n\nCRITICAL:\n- Check if MY NAME appears in the message text above!\n- Only report information that exists in the CONTEXT\n- NO invented details like \"fingers gripping tables\", \"warm pulses\", \"blinks\", or emotional states\n- Keep it brief: 1-2 sentences maximum\n\nExamples:\n- \"This is directed at ME - Caity said my name.\"\n- \"This is for Toad, not me.\"\n- \"This is addressed to everyone.\"\n\nGenerate factual awareness:\"\"\"\n\n            # ALWAYS use fast model for intuition - don't use agent's model override\n            # Intuition needs to be fast and reliable, not character-specific\n            intuition_model = intuition_config.get('model', 'SMALL')\n            timeout = intuition_config.get('timeout', 5)\n\n            # Track this operation\n            tracker = get_tracker()\n            with tracker.track_operation(\n                self.agent_id,\n                \"intuition_generation\",\n                {\"event_type\": event_type, \"speaker\": speaker_id}\n            ):\n                # Generate intuition using fast LLM\n                intuition = await self.llm.generate(\n                    prompt=prompt,\n                    system_prompt=f\"You are {my_name}'s intuitive contextual awareness. Be brief and natural.\",\n                    model=intuition_model,\n                    temperature=0.3,  # Low temperature for consistent analysis\n                    max_tokens=150\n                )\n\n                # Handle dict responses (some LLM clients return {text: ...})\n                if isinstance(intuition, dict):\n                    intuition = intuition.get('text', intuition.get('content', ''))\n                intuition = str(intuition)\n\n                logger.info(f\"[{self.agent_id}] Intuition generated: {intuition[:100]}...\")\n                return intuition.strip()\n\n        except Exception as e:\n            logger.warning(f\"[{self.agent_id}] Intuition generation failed: {e}\")\n            return None\n\n\n    def _complete_cognition_cycle(self):\n        \"\"\"Mark current cognition cycle as complete and fire onCycleEnd event.\"\"\"\n        if not self.cycle_in_progress:\n            logger.debug(f\"[{self.agent_id}] _complete_cognition_cycle called but no cycle in progress\")\n            return\n\n        cycle_uuid = getattr(self, 'current_cycle_uuid', 'unknown')[:8]\n        self.cycle_in_progress = False\n        duration_ms = (time.time() - self.current_cycle_timestamp) * 1000\n\n        logger.info(f\"[{self.agent_id}] Cycle {cycle_uuid} COMPLETED: \"\n                    f\"duration={duration_ms:.1f}ms, pending_llm_calls={self.pending_llm_calls}\")\n\n        # Process queued perceptions (if any)\n        if hasattr(self, 'pending_perceptions') and self.pending_perceptions:\n            queued = self.pending_perceptions.pop(0)  # FIFO\n            logger.info(f\"[{self.agent_id}]  Processing queued perception ({len(self.pending_perceptions)} remaining)\")\n            # Re-trigger perception asynchronously\n            import asyncio\n            asyncio.create_task(self.perceive_event(queued))\n\n        # Fire onCycleEnd event (for NoodleScript)\n        # TODO: Implement event system integration when NoodleScript ready\n\n    def _increment_llm_counter(self):\n        \"\"\"Increment pending LLM call counter.\"\"\"\n        self.pending_llm_calls += 1\n        logger.debug(f\"[{self.agent_id}] LLM started [cycle={self.current_cycle_uuid[:8]}, pending={self.pending_llm_calls}]\")\n\n    def _decrement_llm_counter(self):\n        \"\"\"Decrement pending LLM call counter and check for cycle completion.\"\"\"\n        self.pending_llm_calls -= 1\n        logger.debug(f\"[{self.agent_id}] LLM completed [cycle={self.current_cycle_uuid[:8]}, pending={self.pending_llm_calls}]\")\n\n        # Check if all LLM calls complete\n        if self.pending_llm_calls <= 0:\n            self._complete_cognition_cycle()\n\n\n    async def start_cognition(self):\n        \"\"\"Start continuous affect-driven cognition loop.\"\"\"\n        if not self.cognition_enabled:\n            return\n\n        if self.cognition_task and not self.cognition_task.done():\n            logger.warning(f\"Cognition already running for {self.agent_id}\")\n            return\n\n        self.cognition_task = asyncio.create_task(self._continuous_cognition_loop())\n        logger.info(f\"Started continuous cognition for {self.agent_id}\")\n\n    async def stop_cognition(self):\n        \"\"\"Stop continuous cognition loop.\"\"\"\n        if self.cognition_task:\n            self.cognition_task.cancel()\n            try:\n                await self.cognition_task\n            except asyncio.CancelledError:\n                pass\n            logger.info(f\"Stopped continuous cognition for {self.agent_id}\")\n\n    async def _continuous_cognition_loop(self):\n        \"\"\"\n        Continuous affect-driven cognition loop.\n\n        NO TIMERS. Pure dynamics.\n        Facets decide when they execute based on salience.\n        Speech emerges when affect crosses thresholds.\n        \"\"\"\n        logger.info(f\"[{self.agent_name}] Continuous cognition loop started\")\n\n        while True:\n            try:\n                # Check if cognition is paused\n                if self.cognition_paused:\n                    await asyncio.sleep(self.cognition_check_interval)\n                    continue\n\n                # Skip if facet executor not available\n                if not self.facet_executor:\n                    await asyncio.sleep(self.cognition_check_interval)\n                    continue\n\n                # CHECK CYCLE LOCK: Skip if reactive cycle in progress\n                if getattr(self, 'cycle_in_progress', False):\n                    # Reactive cycle is running - wait for it to complete\n                    await asyncio.sleep(self.cognition_check_interval)\n                    continue\n\n                # LOCK: Start autonomous cycle\n                self.cycle_in_progress = True\n                self.cycle_type = 'autonomous'\n\n                # Execute facets with NO external input (pure rumination)\n                # INCOMING receives empty string - this is autonomous thought\n\n                # Build execution context (same as reactive path)\n                from noodlestudio.core.scripted_facet import ScriptContext\n\n                exec_context = ScriptContext(\n                    cycle=self.current_cycle_uuid,\n                    timestamp=time.time(),\n                    agent_id=self.agent_id,\n                    agent_name=self.agent_name,\n                    agent_species=self.species\n                )\n\n                # Get current affect\n                affect_raw = self.get_current_affect()\n\n                # Inject agent state\n                exec_context._agent_state = {\n                    'affect': affect_raw,\n                    'identity': self.identity_prompt,\n                    'species': self.species,\n                    'personality_traits': getattr(self, 'personality_traits', {})\n                }\n\n                # Inject latent memories for insight emergence\n                exec_context._latent_memories = self.latent_memories\n\n                # Execute facets (track for NoodleStudio visualization)\n                self.current_facet = \"INCOMING\"\n                self.current_phase = \"INCOMING\"\n                self.current_assembly = getattr(self.facet_assembly, 'name', 'Facet Assembly')\n                self.current_model_label = \"\"\n                self.current_model_name = \"\"\n                self.current_llm_status = \"\"\n                self.pending_llm_calls = 1\n                try:\n                    # Pass agent reference for real-time facet tracking\n                    exec_vars = vars(exec_context)\n                    exec_vars['_agent_ref'] = self\n\n                    # Scene Protocol: inject WorldAPI with perception slice\n                    if SCENE_PROTOCOL_AVAILABLE and prepare_facet_context:\n                        exec_vars = prepare_facet_context(self.agent_id, exec_vars)\n\n                    result = await self.facet_executor.execute(\n                        assembly=self.facet_assembly,\n                        incoming_data=\"\",  # No external stimulus\n                        context=exec_vars\n                    )\n\n                    # Scene Protocol: process WorldAPI pending commands\n                    if SCENE_PROTOCOL_AVAILABLE and finalize_facet_context:\n                        scene_commands = finalize_facet_context(self.agent_id)\n                        if scene_commands:\n                            logger.debug(f\"[{self.agent_id}] Autonomous scene commands: {list(scene_commands.keys())}\")\n                finally:\n                    self.current_facet = \"\"\n                    self.current_phase = \"IDLE\"\n                    self.current_assembly = \"\"\n                    self.current_model_label = \"\"\n                    self.current_model_name = \"\"\n                    self.current_llm_status = \"\"\n                    self.pending_llm_calls = 0\n\n                # Check if facets produced speech output\n                response = result.response\n\n                # DEBUG: Log what response we got\n                logger.info(f\"[{self.agent_name}] üîç Autonomous cycle response: '{response[:100] if response else 'None'}'\")\n\n                # AFFECT-DRIVEN SPEECH COOLDOWN\n                # Get current affect state\n                phenomenal = affect_raw.get('phenomenal_state', [0.0] * 5)\n                valence = float(phenomenal[0]) if len(phenomenal) > 0 else 0.0\n                arousal = float(phenomenal[1]) if len(phenomenal) > 1 else 0.5\n                dominance = float(phenomenal[2]) if len(phenomenal) > 2 else 0.5\n                fear = float(phenomenal[3]) if len(phenomenal) > 3 else 0.0\n                sorrow = float(phenomenal[4]) if len(phenomenal) > 4 else 0.0\n                boredom = 1.0 - arousal  # Inverse of arousal\n\n                # Modulation factors (same logic as reactive path):\n                # - High arousal (>0.7) = 0.3x cooldown (GOTTA SPEAK NOW!)\n                # - Low arousal (<0.3) = 2.0x cooldown (meh... whatever)\n                # - High dominance (>0.7) = 0.5x cooldown (I'M IN CHARGE!)\n                # - High boredom (>0.7) = 3.0x cooldown (zzz not worth it)\n                arousal_factor = 0.3 if arousal > 0.7 else (2.0 if arousal < 0.3 else 1.0)\n                dominance_factor = 0.5 if dominance > 0.7 else 1.0\n                boredom_factor = 3.0 if boredom > 0.7 else 1.0\n\n                # Combine factors (multiply for compounding effects)\n                cooldown_multiplier = arousal_factor * dominance_factor * boredom_factor\n                effective_cooldown = self.min_speech_interval * cooldown_multiplier\n\n                logger.info(f\"[{self.agent_name}] üéöÔ∏è AUTONOMOUS COOLDOWN: base={self.min_speech_interval:.1f}s, \"\n                           f\"arousal={arousal:.2f}(x{arousal_factor:.1f}), \"\n                           f\"dominance={dominance:.2f}(x{dominance_factor:.1f}), \"\n                           f\"boredom={boredom:.2f}(x{boredom_factor:.1f}), \"\n                           f\"effective={effective_cooldown:.1f}s\")\n\n                # Check speech cooldown (affect-modulated!)\n                time_since_speech = time.time() - self.last_speech_time\n                can_speak = time_since_speech >= effective_cooldown\n\n                if response and response != \"[No output]\" and response != \"[SUPPRESS]\" and can_speak:\n                    # Broadcast autonomous speech to room\n                    await self._broadcast_autonomous_speech(response)\n                    self.last_speech_time = time.time()\n                    logger.info(f\"[{self.agent_name}] Autonomous speech: {response[:60]}...\")\n\n                # UNLOCK: Autonomous cycle complete\n                self.cycle_in_progress = False\n                self.cycle_type = None\n\n                # Process queued perceptions (if any) - same as reactive cycles!\n                # This ensures messages queued during autonomous cycles get handled\n                self._complete_cognition_cycle()\n\n                # Sleep briefly before next check\n                # This is just polling frequency, NOT thinking frequency\n                # Actual thinking driven by affect dynamics\n                await asyncio.sleep(self.cognition_check_interval)\n\n            except asyncio.CancelledError:\n                logger.info(f\"[{self.agent_name}] Continuous cognition loop cancelled\")\n                break\n            except Exception as e:\n                logger.error(f\"[{self.agent_name}] Error in continuous cognition: {e}\", exc_info=True)\n                await asyncio.sleep(5)  # Wait before retrying after error\n\n# ‚ô° ÔΩû ‚ô° ÔΩû ‚ô° ÔΩû ‚ô° ÔΩû ‚ô° ÔΩû ‚ô° ÔΩû ‚ô° ÔΩû ‚ô° ÔΩû ‚ô°\n# ‡™ú‚ÅÄ‚û¥ ‚ô° Made with love. Use with love.\n# Caitlyn Meeks 2026\n",
  "language": "python",
  "imports": [],
  "has_docstring": true
}