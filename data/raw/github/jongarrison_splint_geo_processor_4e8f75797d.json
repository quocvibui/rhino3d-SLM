{
  "source_url": "https://github.com/jongarrison/splint_geo_processor/blob/b4a410b94aa8ddd9dad7ee8d35d452964254ef41/generators/src/splintcommon.py",
  "repo": "jongarrison/splint_geo_processor",
  "repo_stars": 0,
  "repo_description": null,
  "license": "unknown",
  "filepath": "generators/src/splintcommon.py",
  "instruction": "Splintcommon",
  "code": "import os\nimport json\nfrom pathlib import Path\nimport traceback\nimport rhinoscriptsyntax as rs\nimport shutil\nfrom Rhino.Geometry import Brep\nimport Rhino.Geometry as rg\nimport scriptcontext as sc\n\nsplint_home_dir = Path(\"~/SplintFactoryFiles/\").expanduser()\nPath(splint_home_dir).mkdir(parents=True, exist_ok=True)\nsplint_inbox_dir = os.path.join(splint_home_dir, \"inbox\")\nPath(splint_inbox_dir).mkdir(parents=True, exist_ok=True)\nsplint_outbox_dir = os.path.join(splint_home_dir, \"outbox\")\nPath(splint_outbox_dir).mkdir(parents=True, exist_ok=True)\nsplint_archive_dir = os.path.join(splint_home_dir, \"archive\")\nPath(splint_archive_dir).mkdir(parents=True, exist_ok=True)\n\ndef inclusionTest():\n    log(\"You got it!\")\n\ndef get_output_mesh_filename(jobname, extension):\n    return os.path.join(splint_outbox_dir, f\"{jobname}.{extension}\") #extension is probably obj\n\ndef get_inbox_job_filepath(jobname):\n    return os.path.join(splint_inbox_dir, f\"{jobname}.json\")\n\ndef get_outbox_job_confirmation_filepath(jobname):\n    return os.path.join(splint_outbox_dir, f\"{jobname}.json\")\n\ndef get_log_filepath():\n    return os.path.join(splint_outbox_dir, \"log.txt\")\n\ndef get_generator_filepath():\n    return Path(__file__).parent.parent.resolve()\n\ndef log(message):\n    print(f\"log:{message}\")\n    with open(get_log_filepath(), \"a\", encoding='utf-8') as f:\n        f.write(f\"{message}\\n\")\n\ndef log_clear(message=\"\"):\n    print(f\"log_clear:{message}\")\n    with open(get_log_filepath(), \"a\", encoding='utf-8') as f:\n        f.write(f\"\\n\\n============================================\")\n        f.write(f\"{message}\\n\")\n\nlog(f\"Splint Home Dirs verified: {splint_home_dir=}\")\n\ndef confirm_job_is_processed_and_exit(jobname, is_success, message):\n    job_path = Path(get_inbox_job_filepath(jobname))\n\n    conf_path = Path(get_outbox_job_confirmation_filepath(jobname))\n    try:\n        #Remove the job queue file that started all of this work\n        # if job_path.exists():\n        #     shutil.move(job_path, splint_archive_dir) # This is now handled by the splint_geo_processor\n        #     # job_path.unlink()\n        #     if job_path.exists():\n        #         log(f\"FAILED TO Archive JOB FILE. previous message:{message}: {job_path}\")\n\n        log(f\"RESULT: {'SUCCESS' if is_success else 'FAILURE'} {message=} {jobname=}\")\n\n    except Exception as e:\n        conf_data = {\"result\": \"FAILURE\", \"phase\": \"during confirmation\", \"exception\": f\"{traceback.format_exc()}\", \"message\": message}\n        with open(conf_path, \"w\", encoding='utf-8') as f:\n            json.dump(conf_data, f, indent=2)        \n\ndef load_oldest_json_job_file(directory, algorithm_name):\n    \"\"\"\n    List all JSON files in the specified directory and process the oldest one.\n    \n    Args:\n        directory_path (str): Path to the directory containing JSON files\n    \n    Returns:\n        dict: Contents of the oldest JSON file, or None if no files found\n    \"\"\"\n    # Check if directory exists\n    directory = Path(directory)\n\n    if not directory.exists():\n        log(f\"Directory {directory} does not exist.\")\n        return None\n    \n    # Find all JSON files in the directory\n    json_files = list(directory.glob(f\"{algorithm_name}*.json\"))\n    \n    log(f\"{json_files=}\")\n\n    if not json_files:\n        log(f\"No JSON files found in {directory}\")\n        return None\n    \n    log(f\"Found {len(json_files)} JSON file(s):\")\n    for file in json_files:\n        log(f\"  - {file.name}\")\n    \n    # Find the oldest file by creation time\n    oldest_file = min(json_files, key=lambda f: f.stat().st_ctime)\n    \n    log(f\"\\nProcessing oldest file: {oldest_file.name}\")\n    \n    try:\n        # Read and parse the JSON file\n        with open(oldest_file, 'r', encoding='utf-8') as file:\n            raw_data = file.read()\n            data = json.loads(raw_data)\n        \n        data[\"jobname\"] = oldest_file.stem #name\n        log(\"Successfully loaded JSON data.\")\n        return data, raw_data\n    \n    except json.JSONDecodeError as e:\n        log(f\"Error parsing JSON file {oldest_file.name}: {e}\")\n        return None\n    except Exception as e:\n        log(f\"Error reading file {oldest_file.name}: {e}\")\n        return None\n\ndef extract_server_params_data(json_data):\n    result_data = None\n\n    if json_data is not None:\n        log(\"\\nJSON data loaded successfully:\")\n        log(f\"Data type: {type(json_data)}\")\n        \n        for key in list(json_data.keys()):\n            log(f\"INPUT: {key}: {json_data[key]}\")\n\n        result_data = json.loads(json_data[\"params\"]) if \"params\" in json_data else None\n\n        if result_data is None:\n            log(\"No 'params' key found in JSON data.\")\n        else:\n            result_data[\"jobname\"] = json_data[\"jobname\"]\n            result_data[\"objectID\"] = json_data.get(\"metadata\", {}).get(\"objectID\", \"NA\")\n\n            for key in list(result_data.keys()):\n                log(f\"RESULT: {key}: {result_data[key]}\")\n            return result_data\n    raise Exception(\"No data found to extract correctly\")\n\ndef get_next_geo_job(algorithm_name):\n    try:\n        log(f\"get_next_geo_job {algorithm_name=}\")\n        # Process the oldest JSON file\n        json_data, raw_data = load_oldest_json_job_file(splint_inbox_dir, algorithm_name)\n\n        return extract_server_params_data(json_data), raw_data\n    except Exception as e:\n        log(f\"Exception in get_next_geo_job: {traceback.format_exc()}\")\n        return None\n\ndef load_dev_data(geo_algorithm_name):\n    dev_data_path = Path.joinpath(get_generator_filepath(), f\"{geo_algorithm_name}.json\")\n    raw_data = None\n\n    log(f\"Loading dev data from {dev_data_path}\")\n    if not dev_data_path.exists():\n        log(f\"Dev data file does not exist: {dev_data_path}\")\n        raise Exception(f\"Dev data file does not exist: {dev_data_path}\")\n\n    try:\n        with open(dev_data_path, 'r', encoding='utf-8') as file:\n            raw_data = file.read()\n            data = json.loads(raw_data)\n            data[\"jobname\"] = dev_data_path.stem #name\n\n            log(f\"Loaded dev data from {dev_data_path}\")\n            return extract_server_params_data(data), raw_data\n    except json.JSONDecodeError as e:\n        log(f\"Error parsing dev data JSON file {dev_data_path}: {e}\")\n        raise e\n    except Exception as e:\n        log(f\"Error reading dev data file {dev_data_path}: {e}\")\n        raise e\n\n\n\ndef load_job_data(is_production_mode, geo_algorithm_name):\n    jobname = \"LocalDev\"\n    objectID = \"NONE\"\n    job_data = None\n    root_filename_out = None\n\n\n    output_dir_out = splint_outbox_dir\n\n    if not is_production_mode:\n        log(\"DEV: Looking for available job data...\")\n        job_data, raw_data = load_dev_data(geo_algorithm_name)\n        log(f\"DEV: Received dev job_data={job_data}\")\n\n        root_filename_out = f\"DEV_{geo_algorithm_name}_{objectID}\"\n        jobname = job_data[\"jobname\"]\n        objectID = \"LOCL\"\n    else:\n        # We start hunting for files to see if work needs to be done.\n        log(\"PROD: Looking for available job data...\")\n        job_data, raw_data = get_next_geo_job(geo_algorithm_name)\n        log(f\"PROD:\\n{job_data=}\")\n\n        if job_data is None:\n            log(\"No Prod Job Data Available\")\n            raise Exception(f\"PROD: Failed to locate job data for {geo_algorithm_name}\")\n        else:\n    \n            jobname = job_data[\"jobname\"]\n            root_filename_out = jobname\n\n    return job_data, objectID, root_filename_out, output_dir_out, raw_data\n\ndef checkGeometryExists(geo):\n    \"\"\"\n    Check if in various forms of geometry input, there is valid geometry present.\n    geo could be a list or a single geometry item.\n    could be a brep or something else\n    \"\"\"\n    print(f\"{geo=} type={type(geo)}\")\n\n    isExisting = False\n\n    if geo is None:\n        print(\"geo is None (root)\")\n        isExisting = False\n    elif type(geo) is Brep:\n        print(\"geo is Brep. Yay.\")\n        isExisting = True\n    elif hasattr(geo, '__getitem__'):\n        if len(geo) == 0 or geo[0] is None:\n            print(f\"geo[0] is None len={len(geo)}\")\n            isExisting = False\n        else:\n            isExisting = len(geo) == 1\n            print(f\"geo is list of {len(geo)} len. {isExisting=}\")\n    else:\n        print(\"fall through no recognized type\")\n\n    doesExist = isExisting\n    doesntExist = not isExisting\n    print(f\"{doesExist=} {doesntExist=}\") #Sure this looks funny, but it's useful for debugging and keeping component code minimal\n    return doesExist\n\ndef trim_solid_robust(brep_to_trim, cutting_brep, tolerance=None):\n    \"\"\"\n    Robust brep trimming with fallbacks for tangent/overlapping surfaces.\n    Returns open brep with naked edges (like GH TrimSolid component).\n    \"\"\"\n    brep_to_trim = rs.coercebrep(brep_to_trim)\n    cutting_brep = rs.coercebrep(cutting_brep)\n    log(f\"trim_solid_robust brep_to_trim={type(brep_to_trim)} cutting_brep={type(cutting_brep)} tolerance={tolerance}\")\n    if tolerance is None:\n        log(f\"{sc.doc.ModelAbsoluteTolerance=}\")\n        tolerance = sc.doc.ModelAbsoluteTolerance\n    \n    # Method 1: Try direct Split\n    log(\"Method 1: Try direct Split\")\n    try:\n        pieces = brep_to_trim.Split(cutting_brep, tolerance)\n        log(f\"trim_solid_robust 1 {pieces=}\")\n        if pieces and len(pieces) > 0:\n            # Return the piece(s) - you may need logic to pick the right one\n            return pieces\n    except Exception as e:\n        log(f\"Exception in trim_solid_robust Method 1: {traceback.format_exc()}\")\n    \n    # Method 2: Try with slightly larger tolerance\n    log(\"Method 2: Try with slightly larger tolerance\")\n    try:\n        pieces = brep_to_trim.Split(cutting_brep, tolerance * 10)\n        #Can we find the piece with significant overlap with brep_to_trim?\n\n        log(f\"trim_solid_robust 2 {pieces=}\")\n        if pieces and len(pieces) > 0:\n            return pieces\n    except Exception as e:\n        log(f\"Exception in trim_solid_robust Method 2: {traceback.format_exc()}\")\n    \n    # Method 3: Try intersecting first, then trim\n    # (more robust for tangent surfaces)\n    log(\"Method 3: Try intersecting first, then trim\")\n    try:\n        # Get intersection curves\n        intersection = rg.Intersect.Intersection.BrepBrep(\n            brep_to_trim, cutting_brep, tolerance\n        )\n        log(f\"trim_solid_robust 3 {intersection=}\")\n\n        if intersection[1]:  # if curves exist\n            # Use curves to split\n            # This is more complex - may need curve-based splitting\n            pass\n    except Exception as e:\n        log(f\"Exception in trim_solid_robust Method 3: {traceback.format_exc()}\")\n\n    \n    # Return None or original brep if all methods fail\n    return None",
  "language": "python",
  "imports": [
    "Rhino",
    "Rhino.Geometry",
    "rhinoscriptsyntax",
    "scriptcontext"
  ],
  "has_docstring": false
}