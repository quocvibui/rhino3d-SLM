{
  "source_url": "https://github.com/degun-osint/RhinoScraper/blob/ef1bb1d5fc75bd25b1d7d4d22ed964cf16d1888b/core/analyzer.py",
  "repo": "degun-osint/RhinoScraper",
  "repo_stars": 5,
  "repo_description": null,
  "license": "unknown",
  "filepath": "core/analyzer.py",
  "instruction": "Analyzer",
  "code": "from typing import Dict, Any, List, Optional, Set\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom urllib.parse import urlparse, urljoin\nimport asyncio\nfrom bs4 import BeautifulSoup\nimport aiohttp\nfrom config.settings import Settings\n\nfrom core.session import SessionManager\nfrom core.cache import RhinoCache\nfrom extractors import (\n    ContentExtractor,\n    SecurityExtractor,\n    SocialExtractor,\n    DomainExtractor,\n    EmailExtractor,\n    PhoneExtractor,\n    TechnologyExtractor,\n    SensitiveFileExtractor\n)\n\n\n@dataclass\nclass AnalysisResult:\n    \"\"\"Structure de données pour les résultats d'analyse\"\"\"\n    url: str\n    status_code: int\n    analyzed_at: str\n    content: Dict[str, Any]\n    security: Dict[str, Any]\n    social: Dict[str, Any]\n    domain: Dict[str, Any]\n    emails: List[Dict[str, str]]\n    phones: List[str]\n    technologies: List[str]\n    sensitive_files: List[Dict[str, Any]]\n    internal_links: Dict[str, Any]\n\n\nclass SiteAnalyzer:\n    def __init__(self, session_manager: SessionManager, cache: RhinoCache):\n        self.session_manager = session_manager\n        self.cache = cache\n        self.settings = Settings.get_instance()\n        self.analyzed_urls: Set[str] = set()\n\n    async def analyze(self, url: str, depth: int = 0) -> Optional[AnalysisResult]:\n        \"\"\"Analyse complète d'une URL avec tous les extracteurs\"\"\"\n        try:\n            if url in self.analyzed_urls or depth > self.settings.MAX_DEPTH:\n                return None\n\n            if cached_result := self.cache.get(url):\n                return cached_result\n\n            print(f\"Analyzing URL: {url} at depth {depth}\")  # Debug\n            self.analyzed_urls.add(url)\n\n            session = await self.session_manager.get_session()\n            async with session.get(url, ssl=False) as response:\n                html = await response.text()\n                soup = BeautifulSoup(html, 'html.parser')\n\n                # Création des instances d'extracteurs\n                extractors = [\n                    ContentExtractor(soup, url),\n                    SecurityExtractor(soup, url),\n                    SocialExtractor(soup, url),\n                    DomainExtractor(soup, url),\n                    EmailExtractor(soup, url),\n                    PhoneExtractor(soup, url),\n                    TechnologyExtractor(soup, url),\n                    SensitiveFileExtractor(soup, url),  # Suppression du paramètre session\n                ]\n\n                # Exécution parallèle des extracteurs\n                results = await asyncio.gather(\n                    *[extractor.extract() for extractor in extractors],\n                    return_exceptions=True\n                )\n\n                # Traitement des résultats\n                combined_results = {}\n                for result in results:\n                    if isinstance(result, Exception):\n                        print(f\"Extractor error: {str(result)}\")\n                        continue\n                    if isinstance(result, dict):\n                        combined_results.update(result)\n\n                # Analyse récursive des liens internes si nécessaire\n                internal_links = {}\n                if depth < self.settings.MAX_DEPTH:\n                    found_links = self._get_internal_links(soup, url)\n                    print(f\"Found {len(found_links)} internal links\")\n                    if found_links:  # Ne procéder que s'il y a des liens à analyser\n                        internal_links = await self._analyze_internal_links(found_links, depth + 1)\n\n                # Construction du résultat final\n                analysis_result = AnalysisResult(\n                    url=url,\n                    status_code=response.status,\n                    analyzed_at=datetime.now().isoformat(),\n                    content=combined_results.get('content', {}),\n                    security=combined_results.get('security_info', {}),\n                    social=combined_results.get('social_media', {}),\n                    domain=combined_results.get('domain_info', {}),\n                    emails=combined_results.get('emails', []),\n                    phones=combined_results.get('phones', []),\n                    technologies=combined_results.get('technologies', []),\n                    sensitive_files=combined_results.get('sensitive_files', []),\n                    internal_links=internal_links\n                )\n\n                self.cache.set(url, analysis_result)\n                return analysis_result\n\n        except aiohttp.ClientError as e:\n\n            print(f\"Network error analyzing {url}: {str(e)}\")\n\n            return None\n\n        except Exception as e:\n\n            print(f\"Error analyzing {url}: {str(e)}\")\n\n            return None\n\n    def _get_internal_links(self, soup: BeautifulSoup, base_url: str) -> Set[str]:\n        \"\"\"Extrait les liens internes de la page\"\"\"\n        internal_links = set()\n        base_domain = urlparse(base_url).netloc.replace('www.', '')  # Supprime le www pour la comparaison\n\n        print(f\"Base domain: {base_domain}\")  # Debug\n\n        for a in soup.find_all('a', href=True):\n            href = a['href'].strip()\n\n            # Ignorer les liens vides ou spéciaux\n            if not href or any(href.startswith(prefix) for prefix in ('mailto:', 'tel:', 'javascript:', '#')):\n                continue\n\n            try:\n                # Convertir l'URL relative en absolue\n                full_url = urljoin(base_url, href)\n                parsed_url = urlparse(full_url)\n\n                # Normaliser le domaine pour la comparaison (enlever le www si présent)\n                url_domain = parsed_url.netloc.replace('www.', '')\n\n                # Vérifier que c'est un lien HTTP(S) et interne\n                if (parsed_url.scheme in ('http', 'https') and\n                        url_domain == base_domain and\n                        full_url not in internal_links):  # Éviter les doublons\n                    print(f\"Found internal link: {full_url}\")  # Debug\n                    internal_links.add(full_url)\n            except Exception as e:\n                print(f\"Error processing URL {href}: {str(e)}\")\n                continue\n\n        print(f\"Total internal links found: {len(internal_links)}\")  # Debug\n        return internal_links\n\n    async def _analyze_internal_links(self,\n                                      links: Set[str],\n                                      depth: int) -> Dict[str, Any]:\n        \"\"\"Analyse récursive des liens internes\"\"\"\n        print(f\"Analyzing {len(links)} links at depth {depth}\")\n\n        if depth > self.settings.MAX_DEPTH:\n            return {}\n\n        # Filtrer les liens déjà analysés\n        new_links = [link for link in links if link not in self.analyzed_urls]\n        print(f\"New links to analyze: {len(new_links)}\")  # Debug\n\n        # Trier les liens pour assurer une cohérence dans l'analyse\n        links_to_process = sorted(new_links)[:self.settings.MAX_LINKS_PER_LEVEL]\n        print(f\"Links selected for processing: {len(links_to_process)}\")  # Debug\n\n        if not links_to_process:\n            return {}\n\n        tasks = []\n        for link in links_to_process:\n            if link not in self.analyzed_urls:\n                tasks.append(self.analyze(link, depth))\n\n        results = []\n        if tasks:\n            results = await asyncio.gather(*tasks, return_exceptions=True)\n\n        # Construire le dictionnaire des résultats\n        analyzed_links = {}\n        for link, result in zip(links_to_process, results):\n            if not isinstance(result, Exception) and result is not None:\n                analyzed_links[link] = result\n                print(f\"Successfully analyzed: {link}\")  # Debug\n\n        return analyzed_links",
  "language": "python",
  "imports": [],
  "has_docstring": false
}